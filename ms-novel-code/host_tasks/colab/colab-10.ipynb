{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKe-t1pIOo2f"
      },
      "source": [
        "# Metadata\n",
        "\n",
        "**L1 Taxonomy** - Problem Solving & Algorithms\n",
        "\n",
        "**L2 Taxonomy** - Machine Learning Algorithms\n",
        "\n",
        "**Subtopic** - Scikit-learn integration for explainable k-means\n",
        "\n",
        "**Use Case** - Create a Python solution that integrates scikit-learn’s k-means clustering with an additional explainability layer. After obtaining an initial partition, implement a greedy decision tree method that refines the clusters by selecting axis-aligned cuts based on weighted depth and cost metrics. This integration offers a practical way to deliver interpretable machine learning models for domains where auditability and transparency are priorities fileciteturn0file13.\n",
        "\n",
        "**Programming Language** - Python\n",
        "\n",
        "**Target Model** - o1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGjREnmeoS7x"
      },
      "source": [
        "# Model Breaking Hints\n",
        "\n",
        "\n",
        "1) **What is the initial use case?**\n",
        "\n",
        "   The initial problem involves creating a Python solution that combines scikit-learn’s k-means clustering with an explainability layer. After obtaining clusters via k-means, it requires implementing a greedy decision tree to refine these clusters using axis-aligned cuts based on weighted depth and cost metrics, enhancing interpretability for domains prioritizing auditability and transparency.\n",
        "\n",
        "2) **Why is the initial use case easy?**\n",
        "\n",
        "   - It uses standard algorithms like k-means clustering and basic decision trees, which are well-documented and have extensive library support.\n",
        "   - The clustering and refinement steps are performed on static data, eliminating the complexity of handling dynamic or streaming inputs.\n",
        "   - The axis-aligned cuts are straightforward to implement and don't require advanced mathematical concepts.\n",
        "   - There are no additional constraints like handling high-dimensional data, complex distance metrics, or multi-objective optimization.\n",
        "\n",
        "3) **How could we make it harder?**\n",
        "\n",
        "   - **Incorporate online clustering over streaming data**: Introduce the need to handle real-time data updates efficiently using advanced data structures like dynamic kd-trees.\n",
        "   - **Handle high-dimensional data with manifold learning**: Extend the model to work with high-dimensional data while maintaining interpretability, requiring sophisticated mathematical reasoning.\n",
        "   - **Introduce multi-objective optimization**: Refine clusters using a decision tree that not only minimizes variance but also maximizes interpretability and satisfies constraints like fairness and privacy.\n",
        "   - **Adapt to non-Euclidean spaces with custom metric learning**: Require the algorithm to function in non-standard distance spaces, dealing with obscure corner cases challenging traditional clustering approaches.\n",
        "\n",
        "4) **Which parameters can we change?**\n",
        "\n",
        "   - **Data Nature**: Shift from static to streaming data to introduce the complexity of online clustering.\n",
        "   - **Data Dimensionality**: Move from low-dimensional to high-dimensional data, necessitating techniques like manifold learning or topological data analysis.\n",
        "   - **Distance Metrics**: Replace standard Euclidean metrics with custom, learned metrics to handle non-Euclidean spaces.\n",
        "   - **Refinement Method**: Upgrade from simple greedy decision trees to multi-objective decision trees that address multiple constraints simultaneously.\n",
        "   - **Constraints**: Introduce additional constraints such as fairness and privacy, which require careful consideration during the clustering and refinement processes.\n",
        "\n",
        "5) **What can be a final hard prompt?**\n",
        "\n",
        "   By combining these complexities, we arrive at a challenging problem that requires advanced knowledge in data structures, algorithms, and mathematical reasoning:\n",
        "\n",
        "   \"Develop a Python algorithm that performs online clustering of streaming, high-dimensional, non-Euclidean data by integrating dynamic kd-trees with manifold learning techniques. After initial clustering, refine the clusters using a multi-objective decision tree that minimizes intra-cluster variance, maximizes interpretability, and satisfies constraints like fairness and privacy, all while adapting to custom metric learning in non-standard distance spaces.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS0xHSaZoEJO"
      },
      "source": [
        "# Setup\n",
        "\n",
        "```requirements.txt\n",
        "scikit-learn>=1.2.0\n",
        "numpy>=1.23.0\n",
        "\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YToTLlRqOqmj"
      },
      "source": [
        "# Prompt\n",
        "\n",
        "I want to build a Python program that uses scikit learn's k means clustering to segment a dataset into meaningful groups and then enhances this segmentation using an explainability layer. The explainability layer must apply a custom greedy decision tree algorithm that selects axis aligned cuts to refine the initial k means clusters.\n",
        "\n",
        "\n",
        "**Input Format**\n",
        "\n",
        "Three parameters will be provided as inputs:\n",
        "\n",
        "  - data: A NumPy array of shape (n_samples, n_features), where all values are numeric.\n",
        "  - k: An integer specifying the number of clusters for initial k means. It must satisfy 1 <= k <= 15.\n",
        "  - max_depth: An integer indicating the maximum depth of the greedy decision tree (>= 1).\n",
        "\n",
        "- Each row of data represents a sample, and each column represents a feature.\n",
        "\n",
        "\n",
        "**Output Format**\n",
        "\n",
        "- The program must return a dictionary with the following keys:\n",
        "\n",
        "  - refined_labels: A list of integers of length n_samples, representing the refined cluster assignment for each sample.\n",
        "  - tree_structure: A list of dictionaries, each describing a node in the decision tree with the keys:\n",
        "\n",
        "    - depth (int): Depth of the node.\n",
        "    - split_feature (int): Index of the feature used for splitting, or -1 if it is a leaf.\n",
        "    - split_value (float): Value of the axis-aligned cut, or None if it is a leaf.\n",
        "    - cluster_distribution (dict): Mapping from cluster label to count of samples in this node.\n",
        "  - initial_centroids: A list of k lists, each being a centroid from the original k means clustering.\n",
        "\n",
        "\n",
        "**Examples 1:**\n",
        "\n",
        "```python\n",
        "Input:\n",
        "  data = np.array([[1.0, 2.0], [1.1, 1.9], [9.0, 8.8], [9.1, 9.0]])\n",
        "  k = 2\n",
        "  max_depth = 2\n",
        "\n",
        "Output:\n",
        "  {\n",
        "    \"refined_labels\": [0, 0, 1, 1],\n",
        "    \"tree_structure\": [\n",
        "      {\"depth\": 0, \"split_feature\": 0, \"split_value\": 5.0, \"cluster_distribution\": {0: 2, 1: 2}},\n",
        "      {\"depth\": 1, \"split_feature\": -1, \"split_value\": None, \"cluster_distribution\": {0: 2}},\n",
        "      {\"depth\": 1, \"split_feature\": -1, \"split_value\": None, \"cluster_distribution\": {1: 2}}\n",
        "    ],\n",
        "    \"initial_centroids\": [[1.05, 1.95], [9.05, 8.9]]\n",
        "  }\n",
        "```\n",
        "\n",
        "**Example 2:**\n",
        "\n",
        "```python\n",
        "Input:\n",
        "  data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "  k = 1\n",
        "  max_depth = 1\n",
        "\n",
        "Output:\n",
        "  {\n",
        "    \"refined_labels\": [0, 0, 0, 0],\n",
        "    \"tree_structure\": [],\n",
        "    \"initial_centroids\": [[0.5, 0.5]]\n",
        "  }\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q79gFg5DOtlN"
      },
      "source": [
        "# Requirements\n",
        "\n",
        "**Explicit and Implicit Points:**\n",
        "\n",
        "  - The refined_labels must reflect the updated groupings after applying the decision tree.\n",
        "  - If the decision tree reaches max_depth or no split improves separation, stop splitting.\n",
        "  - Nodes in tree_structure must appear in depth first order.\n",
        "  - If a node is a leaf, it must have split_feature: -1 and split_value: None.\n",
        "  - The cluster_distribution must include all clusters observed in the node.\n",
        "  - Use axis aligned splits only. Do not use multi dimensional splits.\n",
        "\n",
        "**Solution Expectations:**\n",
        "\n",
        "  - The model should cluster the data using k means.\n",
        "  - The explainability layer should simulate how decisions refine the clusters.\n",
        "\n",
        "**Function Signature:**\n",
        "\n",
        "```python\n",
        "def explainable_kmeans(data: np.ndarray, k: int, max_depth: int) -> dict:\n",
        "```\n",
        "\n",
        "**Classes or Structures**\n",
        "\n",
        "```python\n",
        "class TreeNode:\n",
        "    def __init__(self, depth: int):\n",
        "        self.depth: int\n",
        "        self.split_feature: int\n",
        "        self.split_value: float\n",
        "        self.cluster_distribution: Dict[int, int]\n",
        "```\n",
        "\n",
        "\n",
        "**Edge Case Behavior**\n",
        "\n",
        "- Empty dataset: Raise ValueError \"Input data is empty\".\n",
        "- All identical points: Return 1 cluster, empty tree.\n",
        "- k > unique samples: Raise ValueError \"k exceeds number of unique samples\".\n",
        "- max_depth = 0: Tree is never built; return initial clusters.\n",
        "- 1D feature space: Tree can still split based on 1 axis.\n",
        "- Floating point instability: Ensure centroids and splits use np.float64 for precision.\n",
        "- Dataset with NaNs: Raise ValueError \"Input contains NaNs\".\n",
        "- High dimensional data >=100 features: Tree should consider only top 10 most variant axes.\n",
        "- Overlapping clusters: Tree should try to separate but may retain overlap.\n",
        "- Small sample set n < k: Raise ValueError \"Not enough samples for k clusters\".\n",
        "\n",
        "**Constraints**\n",
        "\n",
        "- You must not use eval() or exec() anywhere in the code.\n",
        "- You must ensure that every feature considered for splitting has at least 2 unique values in the node.\n",
        "- All calculations for cost must be in float64 precision. Do not cast to lower precision types.\n",
        "- Use only numpy, scikit learn, and typing libraries. Do not use pandas or any visualization package.\n",
        "- You must write your own function to calculate cluster distribution within a node. Do not use collections.Counter.\n",
        "- All inputs must be validated. Invalid types or out of bound values must raise clear ValueError messages with exact reasons.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSg66GeEbDAT"
      },
      "outputs": [],
      "source": [
        "# code\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "\n",
        "\n",
        "class TreeNode:\n",
        "    \"\"\"Represents a node in the decision tree.\"\"\"\n",
        "\n",
        "    def __init__(self, depth: int):\n",
        "        self.depth = depth\n",
        "        self.split_feature = -1  # -1 for leaf nodes\n",
        "        self.split_value = None  # None for leaf nodes\n",
        "        self.cluster_distribution = {}\n",
        "        self.left_child = None\n",
        "        self.right_child = None\n",
        "        self.sample_indices = []\n",
        "\n",
        "\n",
        "def explainable_kmeans(data: np.ndarray, k: int, max_depth: int) -> dict:\n",
        "    \"\"\"\n",
        "    Performs k-means clustering with explainable decision tree refinement.\n",
        "    \"\"\"\n",
        "    # Input validation\n",
        "    _validate_inputs(data, k, max_depth)\n",
        "\n",
        "    # Handle edge case: max_depth = 0\n",
        "    if max_depth == 0:\n",
        "        kmeans_result = _perform_kmeans(data, k)\n",
        "        return {\n",
        "            \"refined_labels\": kmeans_result['labels'].tolist(),\n",
        "            \"tree_structure\": [],\n",
        "            \"initial_centroids\": kmeans_result['centroids'].tolist()\n",
        "        }\n",
        "\n",
        "    # Perform initial k-means clustering\n",
        "    kmeans_result = _perform_kmeans(data, k)\n",
        "    initial_labels = kmeans_result['labels']\n",
        "    initial_centroids = kmeans_result['centroids']\n",
        "\n",
        "    # Build decision tree for explainability\n",
        "    root_node = _build_decision_tree(data, initial_labels, max_depth)\n",
        "\n",
        "    # Extract refined labels from tree\n",
        "    refined_labels = _extract_refined_labels(data, root_node, len(data))\n",
        "\n",
        "    # Convert tree to required format\n",
        "    tree_structure = _convert_tree_to_structure(root_node)\n",
        "\n",
        "    return {\n",
        "        \"refined_labels\": refined_labels.tolist(),\n",
        "        \"tree_structure\": tree_structure,\n",
        "        \"initial_centroids\": initial_centroids.tolist()\n",
        "    }\n",
        "\n",
        "\n",
        "def _validate_inputs(data: np.ndarray, k: int, max_depth: int) -> None:\n",
        "    \"\"\"Validates all input parameters according to constraints.\"\"\"\n",
        "\n",
        "    # Check data type and format\n",
        "    if not isinstance(data, np.ndarray):\n",
        "        raise ValueError(\"Data must be a numpy array\")\n",
        "\n",
        "    if data.size == 0:\n",
        "        raise ValueError(\"Input data is empty\")\n",
        "\n",
        "    if data.ndim != 2:\n",
        "        raise ValueError(\"Data must be 2-dimensional\")\n",
        "\n",
        "    # Check for NaNs\n",
        "    if np.isnan(data).any():\n",
        "        raise ValueError(\"Input contains NaNs\")\n",
        "\n",
        "    # Validate k parameter\n",
        "    if not isinstance(k, int) or k < 1 or k > 15:\n",
        "        raise ValueError(\"k must be an integer between 1 and 15\")\n",
        "\n",
        "    # Check unique samples constraint\n",
        "    unique_samples = len(np.unique(data, axis=0))\n",
        "    if k > unique_samples:\n",
        "        raise ValueError(\"k exceeds number of unique samples\")\n",
        "\n",
        "    # Check minimum samples constraint\n",
        "    if len(data) < k:\n",
        "        raise ValueError(\"Not enough samples for k clusters\")\n",
        "\n",
        "    # Validate max_depth\n",
        "    if not isinstance(max_depth, int) or max_depth < 0:\n",
        "        raise ValueError(\"max_depth must be a non-negative integer\")\n",
        "\n",
        "\n",
        "def _perform_kmeans(data: np.ndarray, k: int) -> Dict:\n",
        "    \"\"\"Performs k-means clustering with float64 precision.\"\"\"\n",
        "\n",
        "    # Ensure float64 precision\n",
        "    data_float64 = data.astype(np.float64)\n",
        "\n",
        "    # Handle single cluster case\n",
        "    if k == 1:\n",
        "        labels = np.zeros(len(data), dtype=int)\n",
        "        centroids = np.mean(data_float64, axis=0, keepdims=True)\n",
        "        return {'labels': labels, 'centroids': centroids}\n",
        "\n",
        "    # Perform k-means clustering\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    labels = kmeans.fit_predict(data_float64)\n",
        "    centroids = kmeans.cluster_centers_\n",
        "\n",
        "    return {'labels': labels, 'centroids': centroids}\n",
        "\n",
        "\n",
        "def _build_decision_tree(data: np.ndarray, labels: np.ndarray, max_depth: int) -> TreeNode:\n",
        "    \"\"\"Builds a greedy decision tree for cluster refinement.\"\"\"\n",
        "\n",
        "    root_indices = np.arange(len(data))\n",
        "    root_node = TreeNode(depth=0)\n",
        "    root_node.sample_indices = root_indices\n",
        "    root_node.cluster_distribution = _calculate_cluster_distribution(labels[root_indices])\n",
        "\n",
        "    # Build tree recursively\n",
        "    _build_tree_recursive(root_node, data, labels, max_depth)\n",
        "\n",
        "    return root_node\n",
        "\n",
        "\n",
        "def _build_tree_recursive(node: TreeNode, data: np.ndarray, labels: np.ndarray, max_depth: int) -> None:\n",
        "    \"\"\"Recursively builds the decision tree using greedy axis-aligned splits.\"\"\"\n",
        "\n",
        "    # Stop if max depth reached\n",
        "    if node.depth >= max_depth:\n",
        "        return\n",
        "\n",
        "    # Stop if node is pure or has insufficient samples\n",
        "    if len(set(labels[node.sample_indices])) <= 1 or len(node.sample_indices) <= 1:\n",
        "        return\n",
        "\n",
        "    # Find best split\n",
        "    best_split = _find_best_split(data, labels, node.sample_indices)\n",
        "\n",
        "    if best_split is None:\n",
        "        return  # No valid split found\n",
        "\n",
        "    feature_idx, split_value, left_indices, right_indices = best_split\n",
        "\n",
        "    # Set node split information\n",
        "    node.split_feature = feature_idx\n",
        "    node.split_value = float(split_value)\n",
        "\n",
        "    # Create child nodes\n",
        "    node.left_child = TreeNode(depth=node.depth + 1)\n",
        "    node.left_child.sample_indices = left_indices\n",
        "    node.left_child.cluster_distribution = _calculate_cluster_distribution(labels[left_indices])\n",
        "\n",
        "    node.right_child = TreeNode(depth=node.depth + 1)\n",
        "    node.right_child.sample_indices = right_indices\n",
        "    node.right_child.cluster_distribution = _calculate_cluster_distribution(labels[right_indices])\n",
        "\n",
        "    # Recursively build subtrees\n",
        "    _build_tree_recursive(node.left_child, data, labels, max_depth)\n",
        "    _build_tree_recursive(node.right_child, data, labels, max_depth)\n",
        "\n",
        "\n",
        "def _find_best_split(data: np.ndarray, labels: np.ndarray, sample_indices: np.ndarray) -> Optional[Tuple]:\n",
        "    \"\"\"Finds the best axis-aligned split for the given samples.\"\"\"\n",
        "\n",
        "    node_data = data[sample_indices]\n",
        "    node_labels = labels[sample_indices]\n",
        "\n",
        "    # For high dimensional data, consider only top 10 most variant features\n",
        "    features_to_consider = _select_features_to_consider(node_data)\n",
        "\n",
        "    best_split = None\n",
        "    best_score = float('inf')\n",
        "\n",
        "    for feature_idx in features_to_consider:\n",
        "        feature_values = node_data[:, feature_idx]\n",
        "\n",
        "        # Check if feature has at least 2 unique values\n",
        "        unique_values = np.unique(feature_values)\n",
        "        if len(unique_values) < 2:\n",
        "            continue\n",
        "\n",
        "        # Try splits at midpoints between consecutive unique values\n",
        "        for i in range(len(unique_values) - 1):\n",
        "            split_value = (unique_values[i] + unique_values[i + 1]) / 2.0\n",
        "\n",
        "            # Create split\n",
        "            left_mask = feature_values <= split_value\n",
        "            right_mask = ~left_mask\n",
        "\n",
        "            # Skip if either side is empty\n",
        "            if not np.any(left_mask) or not np.any(right_mask):\n",
        "                continue\n",
        "\n",
        "            # Calculate split quality using weighted impurity\n",
        "            score = _calculate_split_score(node_labels, left_mask, right_mask)\n",
        "\n",
        "            if score < best_score:\n",
        "                best_score = score\n",
        "                left_indices = sample_indices[left_mask]\n",
        "                right_indices = sample_indices[right_mask]\n",
        "                best_split = (feature_idx, split_value, left_indices, right_indices)\n",
        "\n",
        "    return best_split\n",
        "\n",
        "\n",
        "def _select_features_to_consider(data: np.ndarray) -> List[int]:\n",
        "    \"\"\"Selects features to consider for splitting, limiting to top 10 for high-dimensional data.\"\"\"\n",
        "\n",
        "    n_features = data.shape[1]\n",
        "\n",
        "    # For high dimensional data (>=100 features), select top 10 most variant\n",
        "    if n_features >= 100:\n",
        "        variances = np.var(data, axis=0)\n",
        "        top_features = np.argsort(variances)[-10:]\n",
        "        return top_features.tolist()\n",
        "\n",
        "    return list(range(n_features))\n",
        "\n",
        "\n",
        "def _calculate_split_score(labels: np.ndarray, left_mask: np.ndarray, right_mask: np.ndarray) -> float:\n",
        "    \"\"\"Calculates weighted impurity score for a split using float64 precision.\"\"\"\n",
        "\n",
        "    total_samples = len(labels)\n",
        "    left_samples = np.sum(left_mask)\n",
        "    right_samples = np.sum(right_mask)\n",
        "\n",
        "    # Calculate weighted gini impurity\n",
        "    left_weight = left_samples / total_samples\n",
        "    right_weight = right_samples / total_samples\n",
        "\n",
        "    left_gini = _calculate_gini_impurity(labels[left_mask])\n",
        "    right_gini = _calculate_gini_impurity(labels[right_mask])\n",
        "\n",
        "    # Return weighted impurity as float64\n",
        "    weighted_impurity = np.float64(left_weight * left_gini + right_weight * right_gini)\n",
        "    return weighted_impurity\n",
        "\n",
        "\n",
        "def _calculate_gini_impurity(labels: np.ndarray) -> float:\n",
        "    \"\"\"Calculates Gini impurity for a set of labels with float64 precision.\"\"\"\n",
        "\n",
        "    if len(labels) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate cluster distribution\n",
        "    cluster_dist = _calculate_cluster_distribution(labels)\n",
        "    total_samples = len(labels)\n",
        "\n",
        "    # Calculate Gini impurity\n",
        "    gini = np.float64(1.0)\n",
        "    for count in cluster_dist.values():\n",
        "        probability = np.float64(count) / np.float64(total_samples)\n",
        "        gini -= probability * probability\n",
        "\n",
        "    return gini\n",
        "\n",
        "\n",
        "def _calculate_cluster_distribution(labels: np.ndarray) -> Dict[int, int]:\n",
        "    \"\"\"Calculates cluster distribution without using collections.Counter.\"\"\"\n",
        "\n",
        "    distribution = {}\n",
        "    for label in labels:\n",
        "        label_int = int(label)\n",
        "        if label_int in distribution:\n",
        "            distribution[label_int] += 1\n",
        "        else:\n",
        "            distribution[label_int] = 1\n",
        "\n",
        "    return distribution\n",
        "\n",
        "\n",
        "def _extract_refined_labels(data: np.ndarray, root_node: TreeNode, n_samples: int) -> np.ndarray:\n",
        "    \"\"\"Extracts refined cluster labels from the decision tree.\"\"\"\n",
        "\n",
        "    refined_labels = np.zeros(n_samples, dtype=int)\n",
        "\n",
        "    # Assign cluster labels based on tree structure\n",
        "    _assign_labels_recursive(root_node, refined_labels, 0)\n",
        "\n",
        "    return refined_labels\n",
        "\n",
        "\n",
        "def _assign_labels_recursive(node: TreeNode, labels: np.ndarray, cluster_id: int) -> int:\n",
        "    \"\"\"Recursively assigns cluster labels to samples based on tree structure.\"\"\"\n",
        "\n",
        "    if node.split_feature == -1:  # Leaf node\n",
        "        # Assign current cluster_id to all samples in this leaf\n",
        "        for idx in node.sample_indices:\n",
        "            labels[idx] = cluster_id\n",
        "        return cluster_id + 1\n",
        "    else:\n",
        "        # Process left subtree first, then right subtree\n",
        "        cluster_id = _assign_labels_recursive(node.left_child, labels, cluster_id)\n",
        "        cluster_id = _assign_labels_recursive(node.right_child, labels, cluster_id)\n",
        "        return cluster_id\n",
        "\n",
        "\n",
        "def _convert_tree_to_structure(root_node: TreeNode) -> List[Dict]:\n",
        "    \"\"\"Converts tree to required output format in depth-first order.\"\"\"\n",
        "\n",
        "    if root_node is None:\n",
        "        return []\n",
        "\n",
        "    structure = []\n",
        "    _traverse_tree_depth_first(root_node, structure)\n",
        "    return structure\n",
        "\n",
        "\n",
        "def _traverse_tree_depth_first(node: TreeNode, structure: List[Dict]) -> None:\n",
        "    \"\"\"Traverses tree in depth-first order and builds structure list.\"\"\"\n",
        "\n",
        "    # Add current node to structure\n",
        "    node_dict = {\n",
        "        \"depth\": node.depth,\n",
        "        \"split_feature\": node.split_feature,\n",
        "        \"split_value\": node.split_value,\n",
        "        \"cluster_distribution\": node.cluster_distribution\n",
        "    }\n",
        "    structure.append(node_dict)\n",
        "\n",
        "    # Traverse children in depth-first order\n",
        "    if node.left_child is not None:\n",
        "        _traverse_tree_depth_first(node.left_child, structure)\n",
        "\n",
        "    if node.right_child is not None:\n",
        "        _traverse_tree_depth_first(node.right_child, structure)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUlcq7ycbHYw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47568255-0d9d-4eae-e56d-8cfb24cfc0be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_all_clusters_present_in_distribution (__main__.TestExplainableKMeans.test_all_clusters_present_in_distribution) ... ok\n",
            "test_all_identical_points (__main__.TestExplainableKMeans.test_all_identical_points) ... ok\n",
            "test_axis_aligned_only (__main__.TestExplainableKMeans.test_axis_aligned_only) ... ok\n",
            "test_cluster_distribution_is_dict (__main__.TestExplainableKMeans.test_cluster_distribution_is_dict) ... ok\n",
            "test_cluster_label_range (__main__.TestExplainableKMeans.test_cluster_label_range) ... ok\n",
            "test_correct_centroid_precision (__main__.TestExplainableKMeans.test_correct_centroid_precision) ... ok\n",
            "test_depth_limit_enforced (__main__.TestExplainableKMeans.test_depth_limit_enforced) ... ok\n",
            "test_empty_data (__main__.TestExplainableKMeans.test_empty_data) ... ok\n",
            "test_feature_with_one_unique_value (__main__.TestExplainableKMeans.test_feature_with_one_unique_value) ... ok\n",
            "test_high_dimensional_input (__main__.TestExplainableKMeans.test_high_dimensional_input) ... ok\n",
            "test_invalid_data_type (__main__.TestExplainableKMeans.test_invalid_data_type) ... ok\n",
            "test_k_equals_1 (__main__.TestExplainableKMeans.test_k_equals_1) ... ok\n",
            "test_k_greater_than_unique (__main__.TestExplainableKMeans.test_k_greater_than_unique) ... ok\n",
            "test_max_depth_zero (__main__.TestExplainableKMeans.test_max_depth_zero) ... ok\n",
            "test_nan_input (__main__.TestExplainableKMeans.test_nan_input) ... ok\n",
            "test_no_split_when_all_same_cluster (__main__.TestExplainableKMeans.test_no_split_when_all_same_cluster) ... ok\n",
            "test_node_leaf_condition (__main__.TestExplainableKMeans.test_node_leaf_condition) ... ok\n",
            "test_node_ordering_depth_first (__main__.TestExplainableKMeans.test_node_ordering_depth_first) ... ok\n",
            "test_non_integer_k (__main__.TestExplainableKMeans.test_non_integer_k) ... ok\n",
            "test_non_integer_max_depth (__main__.TestExplainableKMeans.test_non_integer_max_depth) ... ok\n",
            "test_not_enough_samples_for_k (__main__.TestExplainableKMeans.test_not_enough_samples_for_k) ... ok\n",
            "test_only_top_10_features_considered (__main__.TestExplainableKMeans.test_only_top_10_features_considered) ... ok\n",
            "test_precision_stability (__main__.TestExplainableKMeans.test_precision_stability) ... ok\n",
            "test_random_large_sample (__main__.TestExplainableKMeans.test_random_large_sample) ... ok\n",
            "test_refined_label_length_matches_data (__main__.TestExplainableKMeans.test_refined_label_length_matches_data) ... ok\n",
            "test_refined_label_type (__main__.TestExplainableKMeans.test_refined_label_type) ... ok\n",
            "test_simple_two_clusters (__main__.TestExplainableKMeans.test_simple_two_clusters) ... ok\n",
            "test_single_cluster_no_split (__main__.TestExplainableKMeans.test_single_cluster_no_split) ... ok\n",
            "test_single_feature (__main__.TestExplainableKMeans.test_single_feature) ... ok\n",
            "test_split_feature_valid_range (__main__.TestExplainableKMeans.test_split_feature_valid_range) ... ok\n",
            "test_tree_structure_content (__main__.TestExplainableKMeans.test_tree_structure_content) ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 31 tests in 2.916s\n",
            "\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "# tests\n",
        "import unittest\n",
        "import numpy as np\n",
        "from main import explainable_kmeans\n",
        "\n",
        "class TestExplainableKMeans(unittest.TestCase):\n",
        "\n",
        "    def test_simple_two_clusters(self):\n",
        "        data = np.array([[1.0, 2.0], [1.1, 1.9], [9.0, 8.8], [9.1, 9.0]])\n",
        "        result = explainable_kmeans(data, k=2, max_depth=2)\n",
        "        self.assertEqual(len(result[\"refined_labels\"]), 4)\n",
        "        self.assertEqual(len(result[\"initial_centroids\"]), 2)\n",
        "        self.assertTrue(all(isinstance(node, dict) for node in result[\"tree_structure\"]))\n",
        "\n",
        "    def test_single_cluster_no_split(self):\n",
        "        data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "        result = explainable_kmeans(data, k=1, max_depth=1)\n",
        "        self.assertEqual(result[\"tree_structure\"], [\n",
        "            {\n",
        "        \"depth\": 0,\n",
        "        \"split_feature\": -1,\n",
        "        \"split_value\": None,\n",
        "        \"cluster_distribution\": {0: 4}\n",
        "            }\n",
        "        ])\n",
        "        self.assertEqual(result[\"refined_labels\"], [0, 0, 0, 0])\n",
        "\n",
        "    def test_empty_data(self):\n",
        "        with self.assertRaises(ValueError) as ctx:\n",
        "            explainable_kmeans(np.array([]).reshape(0, 2), k=1, max_depth=1)\n",
        "        self.assertIn(\"Input data is empty\", str(ctx.exception))\n",
        "\n",
        "    def test_all_identical_points(self):\n",
        "        data = np.array([[1.0, 1.0]] * 5)\n",
        "        k = 1\n",
        "        max_depth = 2\n",
        "        result = explainable_kmeans(data, k, max_depth)\n",
        "\n",
        "        expected_structure = [\n",
        "            {\n",
        "                \"depth\": 0,\n",
        "                \"split_feature\": -1,\n",
        "                \"split_value\": None,\n",
        "                \"cluster_distribution\": {0: 5}\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        self.assertEqual(result[\"tree_structure\"], expected_structure)\n",
        "\n",
        "    def test_k_greater_than_unique(self):\n",
        "        data = np.array([[1, 2], [1, 2], [1, 2]])\n",
        "        with self.assertRaises(ValueError) as ctx:\n",
        "            explainable_kmeans(data, k=5, max_depth=2)\n",
        "        self.assertIn(\"k exceeds number of unique samples\", str(ctx.exception))\n",
        "\n",
        "    def test_max_depth_zero(self):\n",
        "        data = np.random.rand(10, 2)\n",
        "        result = explainable_kmeans(data, k=3, max_depth=0)\n",
        "        self.assertEqual(result[\"tree_structure\"], [])\n",
        "        self.assertEqual(len(result[\"initial_centroids\"]), 3)\n",
        "\n",
        "    def test_single_feature(self):\n",
        "        data = np.array([[1], [2], [9], [10]])\n",
        "        result = explainable_kmeans(data, k=2, max_depth=2)\n",
        "        self.assertEqual(len(result[\"refined_labels\"]), 4)\n",
        "        self.assertTrue(isinstance(result[\"tree_structure\"], list))\n",
        "\n",
        "    def test_nan_input(self):\n",
        "        data = np.array([[1.0, 2.0], [np.nan, 3.0]])\n",
        "        with self.assertRaises(ValueError) as ctx:\n",
        "            explainable_kmeans(data, k=2, max_depth=1)\n",
        "        self.assertIn(\"Input contains NaNs\", str(ctx.exception))\n",
        "\n",
        "    def test_high_dimensional_input(self):\n",
        "        data = np.random.rand(50, 150)\n",
        "        result = explainable_kmeans(data, k=4, max_depth=3)\n",
        "        self.assertEqual(len(result[\"initial_centroids\"]), 4)\n",
        "\n",
        "    def test_k_equals_1(self):\n",
        "        data = np.random.rand(20, 5)\n",
        "        result = explainable_kmeans(data, k=1, max_depth=2)\n",
        "        self.assertEqual(len(set(result[\"refined_labels\"])), 1)\n",
        "\n",
        "    def test_depth_limit_enforced(self):\n",
        "        data = np.random.rand(100, 2)\n",
        "        result = explainable_kmeans(data, k=3, max_depth=1)\n",
        "        for node in result[\"tree_structure\"]:\n",
        "            self.assertLessEqual(node[\"depth\"], 1)\n",
        "\n",
        "    def test_axis_aligned_only(self):\n",
        "        data = np.random.rand(10, 3)\n",
        "        result = explainable_kmeans(data, k=2, max_depth=2)\n",
        "        for node in result[\"tree_structure\"]:\n",
        "            self.assertTrue(-1 <= node[\"split_feature\"] < 3 or node[\"split_feature\"] == -1)\n",
        "\n",
        "    def test_feature_with_one_unique_value(self):\n",
        "        data = np.array([[1, 2], [1, 3], [1, 4]])\n",
        "        result = explainable_kmeans(data, k=1, max_depth=2)\n",
        "        self.assertTrue(isinstance(result[\"refined_labels\"], list))\n",
        "\n",
        "    def test_correct_centroid_precision(self):\n",
        "        data = np.random.rand(5, 2)\n",
        "        result = explainable_kmeans(data, k=2, max_depth=1)\n",
        "        for centroid in result[\"initial_centroids\"]:\n",
        "            self.assertTrue(all(isinstance(v, float) for v in centroid))\n",
        "\n",
        "    def test_non_integer_k(self):\n",
        "        with self.assertRaises(ValueError) as ctx:\n",
        "            explainable_kmeans(np.random.rand(10, 2), k=\"3\", max_depth=2)\n",
        "        self.assertIn(\"k must be an integer\", str(ctx.exception))\n",
        "\n",
        "    def test_non_integer_max_depth(self):\n",
        "        with self.assertRaises(ValueError) as ctx:\n",
        "            explainable_kmeans(np.random.rand(10, 2), k=3, max_depth=\"2\")\n",
        "        self.assertIn(\"max_depth must be a non-negative integer\", str(ctx.exception))\n",
        "\n",
        "    def test_invalid_data_type(self):\n",
        "        with self.assertRaises(ValueError) as ctx:\n",
        "            explainable_kmeans(\"not an array\", k=3, max_depth=2)\n",
        "        self.assertIn(\"Data must be a numpy array\", str(ctx.exception))\n",
        "\n",
        "    def test_tree_structure_content(self):\n",
        "        data = np.random.rand(10, 2)\n",
        "        result = explainable_kmeans(data, k=2, max_depth=2)\n",
        "        for node in result[\"tree_structure\"]:\n",
        "            self.assertIn(\"depth\", node)\n",
        "            self.assertIn(\"split_feature\", node)\n",
        "            self.assertIn(\"split_value\", node)\n",
        "            self.assertIn(\"cluster_distribution\", node)\n",
        "\n",
        "    def test_cluster_distribution_is_dict(self):\n",
        "        data = np.random.rand(20, 3)\n",
        "        result = explainable_kmeans(data, k=3, max_depth=2)\n",
        "        for node in result[\"tree_structure\"]:\n",
        "            self.assertTrue(isinstance(node[\"cluster_distribution\"], dict))\n",
        "\n",
        "    def test_node_leaf_condition(self):\n",
        "        data = np.random.rand(10, 2)\n",
        "        result = explainable_kmeans(data, k=2, max_depth=1)\n",
        "        for node in result[\"tree_structure\"]:\n",
        "            if node[\"split_feature\"] == -1:\n",
        "                self.assertIsNone(node[\"split_value\"])\n",
        "\n",
        "    def test_cluster_label_range(self):\n",
        "        data = np.random.rand(12, 2)\n",
        "        result = explainable_kmeans(data, k=3, max_depth=2)\n",
        "        self.assertTrue(set(result[\"refined_labels\"]).issubset(set(range(3))))\n",
        "\n",
        "    def test_no_split_when_all_same_cluster(self):\n",
        "        data = np.random.rand(10, 2)\n",
        "        result = explainable_kmeans(data, k=1, max_depth=3)\n",
        "        self.assertEqual(len(set(result[\"refined_labels\"])), 1)\n",
        "\n",
        "    def test_random_large_sample(self):\n",
        "        data = np.random.rand(1000, 2)\n",
        "        result = explainable_kmeans(data, k=5, max_depth=3)\n",
        "        self.assertEqual(len(result[\"refined_labels\"]), 1000)\n",
        "\n",
        "    def test_only_top_10_features_considered(self):\n",
        "        data = np.random.rand(200, 100)\n",
        "        result = explainable_kmeans(data, k=4, max_depth=2)\n",
        "        used_features = {node[\"split_feature\"] for node in result[\"tree_structure\"] if node[\"split_feature\"] != -1}\n",
        "        self.assertLessEqual(len(used_features), 10)\n",
        "\n",
        "    def test_refined_label_length_matches_data(self):\n",
        "        data = np.random.rand(75, 5)\n",
        "        result = explainable_kmeans(data, k=4, max_depth=2)\n",
        "        self.assertEqual(len(result[\"refined_labels\"]), len(data))\n",
        "\n",
        "    def test_refined_label_type(self):\n",
        "        data = np.random.rand(20, 2)\n",
        "        result = explainable_kmeans(data, k=3, max_depth=1)\n",
        "        self.assertTrue(all(isinstance(lbl, int) for lbl in result[\"refined_labels\"]))\n",
        "\n",
        "    def test_node_ordering_depth_first(self):\n",
        "\n",
        "        data = np.array([\n",
        "            [1.0, 2.0],\n",
        "            [1.1, 2.1],\n",
        "            [9.0, 8.9],\n",
        "            [9.1, 9.0],\n",
        "            [5.0, 5.0],\n",
        "        ])\n",
        "        k = 2\n",
        "        max_depth = 2\n",
        "\n",
        "        result = explainable_kmeans(data, k, max_depth)\n",
        "        tree = result[\"tree_structure\"]\n",
        "\n",
        "        # Validate preorder depth-first traversal (parent before children)\n",
        "        stack = []\n",
        "        last_depth = -1\n",
        "        for node in tree:\n",
        "            depth = node[\"depth\"]\n",
        "            # Depth can increase by 1 (child), or decrease (backtracking), or stay same (sibling)\n",
        "            # Ensure no jumps > 1 in depth without a proper return\n",
        "            if depth > last_depth + 1:\n",
        "                self.fail(f\"Invalid depth transition from {last_depth} to {depth}\")\n",
        "            last_depth = depth\n",
        "\n",
        "        # Optional: ensure root node exists\n",
        "        self.assertEqual(tree[0][\"depth\"], 0)\n",
        "\n",
        "    def test_split_feature_valid_range(self):\n",
        "        data = np.random.rand(20, 4)\n",
        "        result = explainable_kmeans(data, k=3, max_depth=2)\n",
        "        for node in result[\"tree_structure\"]:\n",
        "            if node[\"split_feature\"] != -1:\n",
        "                self.assertTrue(0 <= node[\"split_feature\"] < 4)\n",
        "\n",
        "    def test_all_clusters_present_in_distribution(self):\n",
        "        data = np.random.rand(30, 2)\n",
        "        result = explainable_kmeans(data, k=3, max_depth=2)\n",
        "        for node in result[\"tree_structure\"]:\n",
        "            total = sum(node[\"cluster_distribution\"].values())\n",
        "            self.assertTrue(total > 0)\n",
        "\n",
        "    def test_precision_stability(self):\n",
        "        data = np.random.rand(100, 2).astype(np.float64)\n",
        "        result = explainable_kmeans(data, k=5, max_depth=2)\n",
        "        for node in result[\"tree_structure\"]:\n",
        "            self.assertTrue(isinstance(node[\"split_value\"], (float, type(None))))\n",
        "\n",
        "    def test_not_enough_samples_for_k(self):\n",
        "        data = np.random.rand(2, 2)\n",
        "        with self.assertRaises(ValueError) as ctx:\n",
        "            explainable_kmeans(data, k=5, max_depth=1)\n",
        "        self.assertIn(\"k exceeds number of unique samples\", str(ctx.exception))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main(argv=[''], exit=False, verbosity= 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxZs2ehLYWkm"
      },
      "source": [
        "# Model Breaking Proof\n",
        "\n",
        "#### Model Breaking Task URL: <https://rlhf-v3.turing.com/prompt/44ec3a3f-1a5a-4c14-be48-0d1fc51131bb?origin=https%3A%2F%2Flabeling-ms.turing.com&redirect_url=https%3A%2F%2Flabeling-ms.turing.com%2Fconversations%2F44735%2Fview>\n",
        "\n",
        "#### Model code:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "from typing import Dict, List\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "class TreeNode:\n",
        "    def __init__(self, depth: int):\n",
        "        self.depth = depth\n",
        "        self.split_feature = -1      # -1 means leaf\n",
        "        self.split_value = None      # None means leaf\n",
        "        self.cluster_distribution: Dict[int, int] = {}\n",
        "        self.majority_label = None   # Used to assign a final label when leaf\n",
        "        self.left_child = None\n",
        "        self.right_child = None\n",
        "\n",
        "def explainable_kmeans(data: np.ndarray, k: int, max_depth: int) -> dict:\n",
        "    \"\"\"\n",
        "    Clusters the data into k clusters using KMeans, then refines/segments\n",
        "    those assignments via a custom greedy (axis-aligned) decision tree.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : np.ndarray\n",
        "        2D array of shape (n_samples, n_features).\n",
        "    k : int\n",
        "        Number of clusters for the initial KMeans. Must be 1 <= k <= 15.\n",
        "    max_depth : int\n",
        "        Maximum depth of the decision tree. Must be >= 1.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    dict\n",
        "        A dictionary with keys:\n",
        "          - refined_labels: List[int] of length n_samples with the updated cluster/segment assignment.\n",
        "          - tree_structure: List[Dict], each dict describes a node in depth-first order:\n",
        "                {\n",
        "                  \"depth\": int,\n",
        "                  \"split_feature\": int,\n",
        "                  \"split_value\": float,\n",
        "                  \"cluster_distribution\": Dict[int, int]\n",
        "                }\n",
        "          - initial_centroids: List[List[float]] of shape (k, n_features).\n",
        "    \"\"\"\n",
        "\n",
        "    # ---------------------------- Input Validation ----------------------------\n",
        "\n",
        "    if not isinstance(data, np.ndarray):\n",
        "        raise ValueError(\"Input data must be a NumPy array.\")\n",
        "\n",
        "    if data.size == 0:\n",
        "        raise ValueError(\"Input data is empty\")\n",
        "\n",
        "    if np.isnan(data).any():\n",
        "        raise ValueError(\"Input contains NaNs\")\n",
        "\n",
        "    if len(data.shape) != 2:\n",
        "        raise ValueError(\"Input data must be 2-dimensional\")\n",
        "\n",
        "    n_samples, n_features = data.shape\n",
        "\n",
        "    if not isinstance(k, int) or k < 1 or k > 15:\n",
        "        raise ValueError(\"k must be an integer between 1 and 15\")\n",
        "\n",
        "    if not isinstance(max_depth, int) or max_depth < 1:\n",
        "        raise ValueError(\"max_depth must be an integer >= 1\")\n",
        "\n",
        "    # Number of unique samples check\n",
        "    unique_samples = np.unique(data, axis=0).shape[0]\n",
        "    if k > unique_samples:\n",
        "        raise ValueError(\"k exceeds number of unique samples\")\n",
        "\n",
        "    # If all points identical AND k > 1, KMeans will fail to create distinct clusters\n",
        "    # but to align with the requirement, let's allow KMeans to handle that scenario.\n",
        "    # If the user specifically wants \"All identical => return 1 cluster, empty tree\",\n",
        "    # they must pass k=1 in that scenario.\n",
        "\n",
        "    # If n_samples < k as a second protective check\n",
        "    if n_samples < k:\n",
        "        raise ValueError(\"Not enough samples for k clusters\")\n",
        "\n",
        "    # ----------------------------- KMeans Clustering -----------------------------\n",
        "\n",
        "    # Perform KMeans clustering\n",
        "    # We use a fixed random_state=0 for reproducibility.\n",
        "    # Note: n_init changed in sklearn >= 1.4; we set n_init explicitly to avoid warnings.\n",
        "    kmeans = KMeans(n_clusters=k, random_state=0, n_init='auto', max_iter=300)\n",
        "    kmeans.fit(data)\n",
        "    initial_centroids = kmeans.cluster_centers_.astype(np.float64)\n",
        "    cluster_labels = kmeans.labels_\n",
        "\n",
        "    # If k=1, there's only one cluster; no split is possible, or if all points are identical\n",
        "    # we effectively have a single cluster. Then either no tree or an empty tree is returned.\n",
        "    if k == 1 or unique_samples == 1:\n",
        "        # All refined labels are the same\n",
        "        return {\n",
        "            \"refined_labels\": [0] * n_samples,\n",
        "            \"tree_structure\": [],\n",
        "            \"initial_centroids\": initial_centroids.tolist()\n",
        "        }\n",
        "\n",
        "    # ----------------------------- Precompute Feature Set -----------------------------\n",
        "    # If high dimensional (>= 100 features), select the top 10 most variant features globally.\n",
        "    if n_features >= 100:\n",
        "        variances = np.var(data, axis=0)  # shape (n_features,)\n",
        "        # get indices of top 10 features\n",
        "        top_10_idx = np.argsort(variances)[::-1][:10]\n",
        "        candidate_features = set(top_10_idx.tolist())\n",
        "    else:\n",
        "        # consider all features\n",
        "        candidate_features = set(range(n_features))\n",
        "\n",
        "    # ----------------------- Helper Functions for Decision Tree -----------------------\n",
        "\n",
        "    def get_cluster_distribution(labels_subset: np.ndarray) -> Dict[int, int]:\n",
        "        \"\"\"\n",
        "        Build a dictionary cluster_label -> count for the given labels_subset.\n",
        "        \"\"\"\n",
        "        dist = {}\n",
        "        for lab in labels_subset:\n",
        "            if lab not in dist:\n",
        "                dist[lab] = 0\n",
        "            dist[lab] += 1\n",
        "        return dist\n",
        "\n",
        "    def gini_impurity(dist: Dict[int, int]) -> float:\n",
        "        \"\"\"\n",
        "        Compute Gini impurity given a cluster distribution dictionary.\n",
        "        Gini = 1 - sum_{all clusters} (p_i^2),\n",
        "        where p_i = dist[cluster_label] / total.\n",
        "        \"\"\"\n",
        "        total = sum(dist.values())\n",
        "        if total == 0:\n",
        "            # No samples => no impurity\n",
        "            return 0.0\n",
        "        gini = 1.0\n",
        "        for count in dist.values():\n",
        "            p = count / total\n",
        "            gini -= p ** 2\n",
        "        return gini\n",
        "\n",
        "    # Build a recursive decision tree\n",
        "    def build_tree(data_indices: np.ndarray, depth: int) -> TreeNode:\n",
        "        \"\"\"\n",
        "        Recursively build the decision tree up to max_depth, selecting\n",
        "        the best single-feature split (axis-aligned) to improve cluster purity.\n",
        "        \"\"\"\n",
        "        node = TreeNode(depth)\n",
        "        node_labels = cluster_labels[data_indices]  # cluster assignments at this node\n",
        "        dist = get_cluster_distribution(node_labels)\n",
        "        node.cluster_distribution = dist\n",
        "        node_gini = gini_impurity(dist)\n",
        "\n",
        "        # majority label in current node\n",
        "        node.majority_label = max(dist.keys(), key=lambda c: dist[c])\n",
        "\n",
        "        # Stopping conditions\n",
        "        # 1) If depth == max_depth, stop.\n",
        "        # 2) If all examples are from the same cluster (pure node).\n",
        "        # 3) If no split can reduce impurity (no improvement).\n",
        "        if depth >= max_depth:\n",
        "            return node\n",
        "        if len(dist) == 1:\n",
        "            # Already pure, no need to split\n",
        "            return node\n",
        "\n",
        "        best_gain = 0.0\n",
        "        best_feature = -1\n",
        "        best_value = None\n",
        "        best_left_indices = None\n",
        "        best_right_indices = None\n",
        "\n",
        "        # For each feature in candidate_features, try all possible splits\n",
        "        for f in candidate_features:\n",
        "            # Values of this feature for the samples in data_indices\n",
        "            feature_values = data[data_indices, f]\n",
        "\n",
        "            # If there's only 1 unique value among these samples, skip\n",
        "            unique_vals = np.unique(feature_values)\n",
        "            if len(unique_vals) < 2:\n",
        "                continue\n",
        "\n",
        "            # For an axis-aligned cut, we try midpoints between consecutive unique values\n",
        "            # Sort unique_vals and consider midpoints\n",
        "            unique_vals_sorted = np.sort(unique_vals)\n",
        "\n",
        "            # Potential cut points: midpoints between consecutive unique values\n",
        "            # e.g., mid = (u[i] + u[i+1]) / 2.0\n",
        "            for i in range(len(unique_vals_sorted) - 1):\n",
        "\n",
        "                # <Issue>: <Prompt requires all splits use np.float64 precision; 'cut' is float by default, not explicitly cast.>\n",
        "                cut = 0.5 * (unique_vals_sorted[i] + unique_vals_sorted[i+1])\n",
        "\n",
        "                left_mask = feature_values <= cut\n",
        "                right_mask = feature_values > cut\n",
        "\n",
        "                left_indices = data_indices[left_mask]\n",
        "                right_indices = data_indices[right_mask]\n",
        "\n",
        "                # If either child is empty, skip\n",
        "                if len(left_indices) == 0 or len(right_indices) == 0:\n",
        "                    continue\n",
        "\n",
        "                # Calculate gini for children\n",
        "                left_dist = get_cluster_distribution(cluster_labels[left_indices])\n",
        "                right_dist = get_cluster_distribution(cluster_labels[right_indices])\n",
        "                left_gini = gini_impurity(left_dist)\n",
        "                right_gini = gini_impurity(right_dist)\n",
        "                n_left = len(left_indices)\n",
        "                n_right = len(right_indices)\n",
        "                n_total = n_left + n_right\n",
        "                weighted_child_gini = (n_left / n_total) * left_gini + (n_right / n_total) * right_gini\n",
        "\n",
        "                gain = node_gini - weighted_child_gini\n",
        "                if gain > best_gain:\n",
        "                    best_gain = gain\n",
        "                    best_feature = f\n",
        "                    best_value = cut\n",
        "                    best_left_indices = left_indices\n",
        "                    best_right_indices = right_indices\n",
        "\n",
        "        # If we found a beneficial split, apply it\n",
        "        if best_gain > 1e-15:  # some small epsilon\n",
        "            node.split_feature = best_feature\n",
        "            node.split_value = best_value\n",
        "\n",
        "            # Build children\n",
        "            node.left_child = build_tree(best_left_indices, depth + 1)\n",
        "            node.right_child = build_tree(best_right_indices, depth + 1)\n",
        "\n",
        "        return node\n",
        "\n",
        "    # ----------------------- Build the Tree from the Whole Dataset -----------------------\n",
        "    root = build_tree(np.arange(n_samples), depth=0)\n",
        "\n",
        "    # ----------------------- Extract Tree Structure in Depth-First Order -----------------------\n",
        "    tree_structure = []\n",
        "\n",
        "    def dfs_tree_structure(node: TreeNode):\n",
        "        if node is None:\n",
        "            return\n",
        "        tree_structure.append({\n",
        "            \"depth\": node.depth,\n",
        "            \"split_feature\": node.split_feature,\n",
        "            \"split_value\": node.split_value,\n",
        "            \"cluster_distribution\": dict(sorted(node.cluster_distribution.items()))\n",
        "        })\n",
        "        if node.split_feature != -1:\n",
        "            dfs_tree_structure(node.left_child)\n",
        "            dfs_tree_structure(node.right_child)\n",
        "\n",
        "    # Collect the tree nodes if the root is non-leaf\n",
        "    if root.split_feature != -1:\n",
        "        dfs_tree_structure(root)\n",
        "    else:\n",
        "        # root is a leaf => no splits\n",
        "        # In that case, the tree_structure has just one node or is empty?\n",
        "        # The examples show that if no splits are done, we might return an empty list.\n",
        "        # But the question's examples suggest if all labels remain the same, tree_structure is empty.\n",
        "        # We'll check if the root is truly a leaf. If so, that typically means no splits.\n",
        "        # The requirement states \"tree_structure must appear in depth first order.\"\n",
        "        # If no splits are made, the user example (like k=1) uses an empty list.\n",
        "        # So we follow that convention.\n",
        "\n",
        "        # <Issue>: <Prompt requires that leaf nodes appear in tree_structure with split_feature=-1 and split_value=None. This code discards root if it's a leaf.>\n",
        "        tree_structure = []\n",
        "\n",
        "    \n",
        "    # ----------------------- Assign Refined Labels Using the Built Tree -----------------------\n",
        "\n",
        "    # We'll define a function that traverses the tree from the root\n",
        "    # for a given sample row until it reaches a leaf node, returning the node's majority_label.\n",
        "    def classify(sample: np.ndarray, node: TreeNode):\n",
        "        if node.split_feature == -1:\n",
        "            # Leaf node\n",
        "            return node.majority_label\n",
        "        else:\n",
        "            if sample[node.split_feature] <= node.split_value:\n",
        "                return classify(sample, node.left_child)\n",
        "            else:\n",
        "                return classify(sample, node.right_child)\n",
        "\n",
        "    refined_labels = []\n",
        "    for i in range(n_samples):\n",
        "        refined_labels.append(classify(data[i], root))\n",
        "\n",
        "    # ----------------------- Return the Required Dictionary -----------------------\n",
        "    return {\n",
        "        \"refined_labels\": refined_labels,\n",
        "        \"tree_structure\": tree_structure,\n",
        "        \"initial_centroids\": initial_centroids.tolist()\n",
        "    }\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}