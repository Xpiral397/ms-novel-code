{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKe-t1pIOo2f"
      },
      "source": [
        "# Metadata\n",
        "\n",
        "**L1 Taxonomy** - Networking\n",
        "\n",
        "**L2 Taxonomy** - GraphQL\n",
        "\n",
        "**Subtopic** - Optimising GraphQL Queries in Python with DataLoader\n",
        "\n",
        "**Use Case** - Design an advanced caching strategy using Python's DataLoader to efficiently manage and invalidate cache entries for GraphQL queries in a dynamic data environment.\n",
        "\n",
        "**Programming Language** - Python\n",
        "\n",
        "**Target Model** - GPT-4o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS0xHSaZoEJO"
      },
      "source": [
        "# Setup\n",
        "\n",
        "```requirements.txt\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YToTLlRqOqmj"
      },
      "source": [
        "# Prompt\n",
        "\n",
        "**Problem Description**\n",
        "You run a multi-tenant GraphQL gateway that logs every field access in real time. To diagnose performance hotspots, you must report the top-k most frequently requested fields within a sliding window of the last W seconds at each incoming request. Because the traffic volume is massive and memory is limited, your solution must use a space-efficient, approximate heavy-hitters algorithm (e.g. Count–Min Sketch combined with a stream summary) and support a sliding window eviction mechanism.\n",
        "\n",
        "**Input Format and Constraints**\n",
        "\n",
        "```\n",
        "N W k\n",
        "t_1 field_1\n",
        "t_2 field_2\n",
        "…\n",
        "t_n field_n\n",
        "```\n",
        "\n",
        "* `N` (1 ≤ N ≤ 10^6): number of logged events\n",
        "* `W` (1 ≤ W ≤ 10^5): size of the sliding window in seconds\n",
        "* `k` (1 ≤ k ≤ 100): number of top fields to report\n",
        "* Each of the next N lines has:\n",
        "\n",
        "  * `t_i` (integer, non-decreasing): the timestamp in seconds\n",
        "  * `fieldᵢ` (string of length <=  32): the GraphQL field name accessed\n",
        "\n",
        "**Expected Output Format**\n",
        "After processing each event i, output a line:\n",
        "\n",
        "```\n",
        "_1ᵢ [f_1:c_2, f_2:c_2 …, f_k:c_k]\n",
        "```\n",
        "\n",
        "where `f₁…fₖ` are the k fields with the highest *estimated* counts within the window `[t_i–W+1, t_i]`, sorted by descending count (ties broken by lex order), and `c_j` is the estimated count for `f_j`.\n",
        "\n",
        "**Example**\n",
        "\n",
        "```\n",
        "Input:\n",
        "6 5 2\n",
        "1 user\n",
        "2 post\n",
        "3 user\n",
        "6 user\n",
        "7 comment\n",
        "10 post\n",
        "\n",
        "Output:\n",
        "1 [user:1, post:0]\n",
        "2 [user:1, post:1]\n",
        "3 [user:2, post:1]\n",
        "6 [user:2, post:1]\n",
        "7 [user:2, comment:1]\n",
        "10 [post:1, user:1]\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q79gFg5DOtlN"
      },
      "source": [
        "\n",
        "\n",
        "# Requirements\n",
        "\n",
        "1. **Explicit & Implicit Points**\n",
        "\n",
        "   * Use a Count–Min Sketch (CMS) or similar sub-linear sketch to track approximate frequencies.\n",
        "   * Maintain exact timestamps in a sliding‐window buffer to evict contributions older than W seconds.\n",
        "   * Combine the CMS with a “stream summary” (e.g. a fixed‐size min-heap or Misra–Gries) to extract the top-k candidates.\n",
        "   * On each new timestamped event, ingest the field into both the sketch and the summary, then evict any expired events from the window structures.\n",
        "   * When evicting, decrement the sketch counts or adjust summary counters to maintain approximate correctness.\n",
        "   * Break ties in estimated counts by lexicographical order of field names.\n",
        "\n",
        "2. **Solution Expectations**\n",
        "\n",
        "   * Process each of the N events in amortized O(log k) time (for summary updates) plus the CMS update cost O(d), where d = number of hash functions.\n",
        "   * Use O(k + d·m) space, where m is sketch width; do *not* store all N events explicitly.\n",
        "   * Provide approximate counts: errors within the standard CMS error bounds.\n",
        "   * Ensure that evictions correctly age‐out only those contributions older than W seconds.\n",
        "   * After each ingestion+eviction step, output exactly one line as specified.\n",
        "\n",
        "3. **Function Signatures**\n",
        "\n",
        "   ```python\n",
        "   def process_stream(\n",
        "       events: List[Tuple[int, str]],\n",
        "       W: int,\n",
        "       k: int,\n",
        "       sketch_depth: int,\n",
        "       sketch_width: int\n",
        "   ) -> List[str]:\n",
        "       \"\"\"\n",
        "       events: list of (timestamp, field_name) in non-decreasing order\n",
        "       W: sliding window size in seconds\n",
        "       k: number of heavy hitters to report\n",
        "       sketch_depth, sketch_width: CMS parameters\n",
        "       Returns: list of N output strings, one per event\n",
        "       \"\"\"\n",
        "   ```\n",
        "\n",
        "4. **Relevant Classes / Data Structures**\n",
        "\n",
        "   ```python\n",
        "   @dataclass\n",
        "   class CountMinSketch:\n",
        "       depth: int\n",
        "       width: int\n",
        "       # internal tables, hash functions, etc.\n",
        "       def update(self, key: str, delta: int = 1) -> None: ...\n",
        "       def estimate(self, key: str) -> int: ...\n",
        "\n",
        "   @dataclass\n",
        "   class StreamSummary:\n",
        "       capacity: int\n",
        "       # tracks top-k candidates with approximate counts\n",
        "       def add(self, key: str, count: int) -> None: ...\n",
        "       def topk(self) -> List[Tuple[str, int]]: ...\n",
        "   ```\n",
        "\n",
        "5. **Edge Case Behavior**\n",
        "\n",
        "   * If fewer than k distinct fields have appeared so far, fill missing slots with the lexicographically smallest fields not yet seen (with count 0).\n",
        "   * If W = 0, only count the current event (window of size one).\n",
        "   * Identical timestamps: treat as distinct events; eviction uses ≤ t_i–W.\n",
        "   * Very high event rates (N >> W): ensure your data structures never grow unbounded.\n",
        "\n",
        "6. **Constraints**\n",
        "\n",
        "   * Time complexity: O(N·(d + log k)), where d = CMS depth.\n",
        "   * Space complexity: O(k + d·width + W) for the sliding buffer.\n",
        "   * Only Python standard library (`collections`, `heapq`, `hashlib`, `time`, etc.) no external streaming or sketching libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSg66GeEbDAT"
      },
      "outputs": [],
      "source": [
        "# code\n",
        "import sys\n",
        "import collections\n",
        "import hashlib\n",
        "from typing import List, Tuple, Dict, Deque\n",
        "\n",
        "class CountMinSketch:\n",
        "    def __init__(self, depth: int, width: int):\n",
        "        self.depth = depth\n",
        "        self.width = width\n",
        "        self.table = [[0] * width for _ in range(depth)]\n",
        "        self.salts = list(range(depth))\n",
        "\n",
        "    def _idx(self, key: str, salt: int) -> Tuple[int, int]:\n",
        "        data = f\"{salt}-{key}\".encode(\"utf-8\")\n",
        "        digest = hashlib.sha256(data).digest()\n",
        "        val = int.from_bytes(digest, \"big\")\n",
        "        return salt, val % self.width\n",
        "\n",
        "    def update(self, key: str, delta: int = 1) -> None:\n",
        "        for salt in self.salts:\n",
        "            row, col = self._idx(key, salt)\n",
        "            self.table[row][col] += delta\n",
        "\n",
        "    def estimate(self, key: str) -> int:\n",
        "        min_val = float(\"inf\")\n",
        "        for salt in self.salts:\n",
        "            row, col = self._idx(key, salt)\n",
        "            cnt = self.table[row][col]\n",
        "            if cnt < min_val:\n",
        "                min_val = cnt\n",
        "        return int(min_val) if min_val != float(\"inf\") else 0\n",
        "\n",
        "class StreamSummary:\n",
        "    def __init__(self, capacity: int):\n",
        "        self.capacity = capacity\n",
        "        self.counters: Dict[str, int] = {}\n",
        "\n",
        "    def add(self, key: str, count: int) -> None:\n",
        "        if key in self.counters:\n",
        "            self.counters[key] += count\n",
        "        elif len(self.counters) < self.capacity:\n",
        "            self.counters[key] = count\n",
        "        else:\n",
        "            to_remove = []\n",
        "            for k in list(self.counters):\n",
        "                self.counters[k] -= count\n",
        "                if self.counters[k] <= 0:\n",
        "                    to_remove.append(k)\n",
        "            for k in to_remove:\n",
        "                del self.counters[k]\n",
        "            if len(self.counters) < self.capacity:\n",
        "                self.counters[key] = count\n",
        "\n",
        "    def decrement(self, key: str, count: int) -> None:\n",
        "        if key in self.counters:\n",
        "            self.counters[key] -= count\n",
        "            if self.counters[key] <= 0:\n",
        "                del self.counters[key]\n",
        "\n",
        "    def topk(self) -> List[Tuple[str, int]]:\n",
        "        items = list(self.counters.items())\n",
        "        items.sort(key=lambda x: (-x[1], x[0]))\n",
        "        return items[:self.capacity]\n",
        "\n",
        "\n",
        "def process_stream(\n",
        "    events: List[Tuple[int, str]],\n",
        "    W: int,\n",
        "    k: int,\n",
        "    sketch_depth: int,\n",
        "    sketch_width: int\n",
        ") -> List[str]:\n",
        "    # Precompute all distinct fields for padding (global list)\n",
        "    distinct_fields = sorted({f for _, f in events})\n",
        "\n",
        "    cms = CountMinSketch(sketch_depth, sketch_width)\n",
        "    summary = StreamSummary(k)\n",
        "    buffer: Deque[Tuple[int, str]] = collections.deque()\n",
        "    results: List[str] = []\n",
        "\n",
        "    for t, field in events:\n",
        "        # ingest\n",
        "        cms.update(field, 1)\n",
        "        summary.add(field, 1)\n",
        "        buffer.append((t, field))\n",
        "\n",
        "        # evict old events\n",
        "        if W == 0:\n",
        "            while buffer and buffer[0][0] < t:\n",
        "                old_t, old_f = buffer.popleft()\n",
        "                cms.update(old_f, -1)\n",
        "                summary.decrement(old_f, 1)\n",
        "        else:\n",
        "            cutoff = t - W\n",
        "            while buffer and buffer[0][0] <= cutoff:\n",
        "                old_t, old_f = buffer.popleft()\n",
        "                cms.update(old_f, -1)\n",
        "                summary.decrement(old_f, 1)\n",
        "\n",
        "        # get top-k candidates and re-rank by CMS estimates\n",
        "        candidates = [f for f, _ in summary.topk()]\n",
        "        est_list = [(f, cms.estimate(f)) for f in candidates]\n",
        "        est_list.sort(key=lambda x: (-x[1], x[0]))\n",
        "        top = est_list[:k]\n",
        "\n",
        "        # pad with lexicographically smallest distinct fields not in top\n",
        "        present = {f for f, _ in top}\n",
        "        for f in distinct_fields:\n",
        "            if len(top) >= k:\n",
        "                break\n",
        "            if f not in present:\n",
        "                top.append((f, 0))\n",
        "                present.add(f)\n",
        "\n",
        "        # pad with empty strings if still under k\n",
        "        while len(top) < k:\n",
        "            top.append((\"\", 0))\n",
        "\n",
        "        # format output with comma+space\n",
        "        line = f\"{t} [\" + \", \".join(f\"{f}:{c}\" for f, c in top) + \"]\"\n",
        "        results.append(line)\n",
        "\n",
        "    return results\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    data = sys.stdin\n",
        "    header = next(data).split()\n",
        "    N, W, k = map(int, header)\n",
        "    events: List[Tuple[int, str]] = []\n",
        "    for _ in range(N):\n",
        "        parts = next(data).strip().split()\n",
        "        if not parts:\n",
        "            continue\n",
        "        t = int(parts[0]); field = parts[1]\n",
        "        events.append((t, field))\n",
        "\n",
        "    sketch_depth = 5\n",
        "    sketch_width = max(1000, k * 10)\n",
        "    for line in process_stream(events, W, k, sketch_depth, sketch_width):\n",
        "        print(line)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUlcq7ycbHYw"
      },
      "outputs": [],
      "source": [
        "# tests\n",
        "\n",
        "import unittest\n",
        "import re\n",
        "from main import CountMinSketch, StreamSummary, process_stream\n",
        "\n",
        "class TestCountMinSketch(unittest.TestCase):\n",
        "    def test_initial_estimate_zero(self):\n",
        "        cms = CountMinSketch(depth=3, width=100)\n",
        "        self.assertEqual(cms.estimate(\"foo\"), 0)\n",
        "        self.assertEqual(cms.estimate(\"bar\"), 0)\n",
        "\n",
        "    def test_update_and_estimate(self):\n",
        "        cms = CountMinSketch(depth=4, width=50)\n",
        "        cms.update(\"alpha\", 5)\n",
        "        self.assertEqual(cms.estimate(\"alpha\"), 5)\n",
        "        cms.update(\"alpha\", 3)\n",
        "        self.assertEqual(cms.estimate(\"alpha\"), 8)\n",
        "\n",
        "    def test_decrement_below_zero(self):\n",
        "        cms = CountMinSketch(depth=2, width=20)\n",
        "        cms.update(\"x\", 4)\n",
        "        self.assertEqual(cms.estimate(\"x\"), 4)\n",
        "        cms.update(\"x\", -1)\n",
        "        self.assertEqual(cms.estimate(\"x\"), 3)\n",
        "        cms.update(\"x\", -5)\n",
        "        self.assertEqual(cms.estimate(\"x\"), -2)\n",
        "\n",
        "    def test_hash_positions_vary(self):\n",
        "        cms = CountMinSketch(depth=2, width=10)\n",
        "        posns = {cms._idx(f\"key{i}\", salt)\n",
        "                  for i in range(20) for salt in range(cms.depth)}\n",
        "        self.assertTrue(len(posns) > 10)\n",
        "\n",
        "class TestStreamSummary(unittest.TestCase):\n",
        "    def test_add_within_capacity(self):\n",
        "        ss = StreamSummary(capacity=2)\n",
        "        ss.add(\"a\", 1)\n",
        "        ss.add(\"b\", 1)\n",
        "        top = ss.topk()\n",
        "        self.assertEqual(len(top), 2)\n",
        "        self.assertIn((\"a\", 1), top)\n",
        "        self.assertIn((\"b\", 1), top)\n",
        "\n",
        "    def test_add_eviction(self):\n",
        "        ss = StreamSummary(capacity=2)\n",
        "        ss.add(\"a\", 1)\n",
        "        ss.add(\"b\", 1)\n",
        "        ss.add(\"c\", 1)\n",
        "        top = dict(ss.topk())\n",
        "        self.assertIn(\"c\", top)\n",
        "\n",
        "    def test_topk_tiebreak_lex(self):\n",
        "        ss = StreamSummary(capacity=3)\n",
        "        ss.add(\"b\", 2)\n",
        "        ss.add(\"a\", 2)\n",
        "        ss.add(\"c\", 2)\n",
        "        top = ss.topk()\n",
        "        self.assertEqual([f for f,_ in top], [\"a\",\"b\",\"c\"] )\n",
        "\n",
        "class TestProcessStream(unittest.TestCase):\n",
        "    def setUp(self):\n",
        "        self.depth = 3\n",
        "        self.width = 50\n",
        "\n",
        "    def test_empty_events(self):\n",
        "        out = process_stream([], W=10, k=3,\n",
        "                             sketch_depth=self.depth,\n",
        "                             sketch_width=self.width)\n",
        "        self.assertEqual(out, [])\n",
        "\n",
        "    def test_single_event_format_and_count(self):\n",
        "        logs = [(100, \"fieldA\")]\n",
        "        out = process_stream(logs, W=5, k=1,\n",
        "                              sketch_depth=self.depth,\n",
        "                              sketch_width=self.width)\n",
        "        self.assertEqual(len(out), 1)\n",
        "        # strict format with spaces after commas\n",
        "        self.assertRegex(out[0], r\"^100 \\[[^,]+:[0-9]+(?:, [^,]+:[0-9]+)*\\]$\")\n",
        "        inner = re.match(r\"^\\d+ \\[(.+)\\]$\", out[0]).group(1)\n",
        "        self.assertEqual(inner, \"fieldA:1\")\n",
        "\n",
        "    def test_padding_when_less_than_k(self):\n",
        "        logs = [(1, \"x\"), (2, \"y\")]\n",
        "        out = process_stream(logs, W=10, k=4,\n",
        "                              sketch_depth=self.depth,\n",
        "                              sketch_width=self.width)\n",
        "        for idx, (t, _) in enumerate(logs):\n",
        "            parts = re.match(r\"^\\d+ \\[(.*)\\]$\", out[idx]).group(1).split(', ')\n",
        "            self.assertEqual(len(parts), 4)\n",
        "            seen_names = {p.split(\":\")[0] for p in parts}\n",
        "            self.assertTrue(\"x\" in seen_names)\n",
        "            self.assertTrue(\"y\" in seen_names)\n",
        "            # one slot should be the lex smallest unseen field (e.g. 'a','b',...)\n",
        "            unseen = [p for p in parts if p.split(':')[1]=='0']\n",
        "            self.assertTrue(len(unseen) >= 1)\n",
        "\n",
        "    def test_empty_string_padding(self):\n",
        "        logs = [(1, \"a\")]\n",
        "        out = process_stream(logs, W=1, k=3,\n",
        "                              sketch_depth=self.depth,\n",
        "                              sketch_width=self.width)\n",
        "        parts = out[0].split('[')[1].rstrip(']').split(', ')\n",
        "        names = [p.split(':')[0] for p in parts]\n",
        "        # only 'a' seen, so we expect two empty-string slots to reach k=3\n",
        "        self.assertEqual(names.count(''), 2)\n",
        "\n",
        "    def test_sliding_window_eviction(self):\n",
        "        logs = [(1, \"x\"), (2, \"y\"), (15, \"x\")]\n",
        "        # W=10 → window is [t-9, t], so event at t=2 should be evicted at t=15\n",
        "        out = process_stream(logs, W=10, k=2,\n",
        "                              sketch_depth=self.depth,\n",
        "                              sketch_width=self.width)\n",
        "        inner = out[-1].split('[')[1].rstrip(']')\n",
        "        parts = inner.split(', ')\n",
        "        counts = {name: int(cnt) for name,cnt in (p.split(':') for p in parts)}\n",
        "        self.assertEqual(counts[\"x\"], 1)\n",
        "        self.assertEqual(counts[\"y\"], 0)\n",
        "\n",
        "    def test_identical_timestamps(self):\n",
        "        logs = [(5,\"a\"), (5,\"b\"), (5,\"c\")]\n",
        "        out = process_stream(logs, W=1, k=2,\n",
        "                              sketch_depth=self.depth,\n",
        "                              sketch_width=self.width)\n",
        "        inner = re.match(r\"^\\d+ \\[(.*)\\]$\", out[0]).group(1).split(', ')\n",
        "        self.assertEqual(inner[0].split(':')[0], \"a\")\n",
        "        self.assertEqual(inner[1].split(':')[0], \"b\")\n",
        "\n",
        "    def test_zero_window_size(self):\n",
        "        logs = [(10,\"f1\"), (20,\"f1\")]\n",
        "        out = process_stream(logs, W=0, k=1,\n",
        "                              sketch_depth=self.depth,\n",
        "                              sketch_width=self.width)\n",
        "        self.assertEqual(out[0], \"10 [f1:1]\")\n",
        "        self.assertEqual(out[1], \"20 [f1:1]\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxZs2ehLYWkm"
      },
      "source": [
        "# Model Breaking Proof\n",
        "\n",
        "#### Model Breaking Task URL: <Add the URL here>\n",
        "\n",
        "#### Model code:\n",
        "\n",
        "```python\n",
        "import time\n",
        "import collections\n",
        "import heapq\n",
        "import hashlib\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class CountMinSketch:\n",
        "    depth: int\n",
        "    width: int\n",
        "    tables: List[List[int]]\n",
        "    seeds: List[int]\n",
        "\n",
        "    def __init__(self, depth: int, width: int):\n",
        "        self.depth = depth\n",
        "        self.width = width\n",
        "        self.tables = [[0] * width for _ in range(depth)]\n",
        "        self.seeds = list(range(depth))\n",
        "\n",
        "    def _hash(self, item: str, seed: int) -> int:\n",
        "        h = hashlib.md5(f\"{seed}-{item}\".encode('utf-8')).hexdigest()\n",
        "        return int(h, 16) % self.width\n",
        "\n",
        "    def update(self, key: str, delta: int = 1) -> None:\n",
        "        for i in range(self.depth):\n",
        "            hash_value = self._hash(key, self.seeds[i])\n",
        "            self.tables[i][hash_value] += delta\n",
        "\n",
        "    def estimate(self, key: str) -> int:\n",
        "        return min(self.tables[i][self._hash(key, self.seeds[i])] for i in range(self.depth))\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class StreamSummary:\n",
        "    capacity: int\n",
        "    items: dict\n",
        "    heap: List[Tuple[int, str]]\n",
        "\n",
        "    def __init__(self, capacity: int):\n",
        "        self.capacity = capacity\n",
        "        self.items = {}\n",
        "        self.heap = []\n",
        "\n",
        "    def add(self, key: str, count: int) -> None:\n",
        "        if key in self.items:\n",
        "            self.items[key] = count\n",
        "        else:\n",
        "            if len(self.items) < self.capacity:\n",
        "                self.items[key] = count\n",
        "                heapq.heappush(self.heap, (count, key))\n",
        "            elif count > self.heap[0][0]:\n",
        "                _, min_key = heapq.heappop(self.heap)\n",
        "                del self.items[min_key]\n",
        "                self.items[key] = count\n",
        "                heapq.heappush(self.heap, (count, key))  # <Issue> never evicts keys when their *true* counts drop on window eviction\n",
        "\n",
        "    def topk(self) -> List[Tuple[str, int]]:\n",
        "        return sorted(self.items.items(), key=lambda x: (-x[1], x[0]))[:self.capacity]\n",
        "\n",
        "\n",
        "def process_stream(\n",
        "    events: List[Tuple[int, str]],\n",
        "    W: int,\n",
        "    k: int,\n",
        "    sketch_depth: int,\n",
        "    sketch_width: int\n",
        ") -> List[str]:\n",
        "    cms = CountMinSketch(sketch_depth, sketch_width)\n",
        "    stream_summary = StreamSummary(k)\n",
        "    time_buffer = collections.deque()\n",
        "    output = []\n",
        "\n",
        "    for timestamp, field_name in events:\n",
        "        # Add new event\n",
        "        cms.update(field_name, 1)\n",
        "        estimate = cms.estimate(field_name)\n",
        "        stream_summary.add(field_name, estimate)\n",
        "        time_buffer.append((timestamp, field_name))\n",
        "\n",
        "        # Evict old events\n",
        "        while time_buffer and time_buffer[0][0] <= timestamp - W:\n",
        "            old_timestamp, old_field_name = time_buffer.popleft()\n",
        "            cms.update(old_field_name, -1)    # <Issue> StreamSummary isn’t decremented when entries leave the window\n",
        "\n",
        "        # Collect top-k for output\n",
        "        top_fields = stream_summary.topk()\n",
        "\n",
        "        # Pad to k entries (incorrect per spec)\n",
        "        seen_fields = {name for _, name in top_fields}\n",
        "        missing_count = max(0, k - len(seen_fields))\n",
        "        not_seen = sorted(set(name for _, name in time_buffer) - seen_fields)[:missing_count]\n",
        "\n",
        "        topk_output = [f\"{name}:{count}\" for name, count in top_fields] + [f\"{name}:0\" for name in not_seen]\n",
        "        topk_output.sort(key=lambda x: (int(x.split(':')[1]), x.split(':')[0]))\n",
        "        output.append(\" \".join(topk_output))   # <Issue> output missing timestamp, brackets, and commas per spec\n",
        "\n",
        "    return output\n",
        "\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
