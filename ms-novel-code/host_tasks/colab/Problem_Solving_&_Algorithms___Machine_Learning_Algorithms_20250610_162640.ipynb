{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKe-t1pIOo2f"
      },
      "source": [
        "# Metadata\n",
        "\n",
        "**L1 Taxonomy** - Problem Solving & Algorithms\n",
        "\n",
        "**L2 Taxonomy** - Machine Learning Algorithms\n",
        "\n",
        "**Subtopic** - Python reinforcement learning for recommendations\n",
        "\n",
        "**Use Case** - Create a recommendation engine using Python-based reinforcement learning. Develop a system that processes distributed datasets with synchronized graph and matrix operations while applying Q-learning strategies to optimize recommendations. This solution should adapt to users’ historical behaviors and improve over time by leveraging parallel processing techniques discussed in the surveyed frameworks fileciteturn0file9.\n",
        "\n",
        "**Programming Language** - Python\n",
        "\n",
        "**Target Model** - o1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27zR8fZhSbtC"
      },
      "source": [
        "# Model Breaking Hints\n",
        "\n",
        "\n",
        "1) **What is the initial use case?**\n",
        "\n",
        "   - The initial use case involves creating a recommendation engine using Python-based reinforcement learning.\n",
        "   - It processes distributed datasets with synchronized graph and matrix operations.\n",
        "   - Applies Q-learning strategies to optimize recommendations.\n",
        "   - Adapts to users' historical behaviors and leverages parallel processing techniques from surveyed frameworks.\n",
        "\n",
        "2) **Why is the initial use case easy?**\n",
        "\n",
        "   - It utilizes standard Q-learning, which is a basic reinforcement learning algorithm.\n",
        "   - Processing distributed datasets with synchronized graph and matrix operations is manageable with existing libraries like NumPy, Pandas, and Dask.\n",
        "   - There's no requirement to handle streaming data or concept drift.\n",
        "   - Lacks advanced algorithms like dynamic graph neural networks or temporal embeddings.\n",
        "   - Does not address multi-objective optimization, privacy concerns, or adversarial robustness.\n",
        "   - The problem scope is limited and doesn't include complex, non-obvious subproblems or logical traps.\n",
        "\n",
        "3) **How could we make it harder?**\n",
        "\n",
        "   - **Incorporate dynamic graph neural networks** (Hint 1) to model time-evolving user-item interactions, introducing advanced algorithms like temporal graph embeddings.\n",
        "   - **Require handling of streaming data with concept drift** (Hint 2), necessitating online reinforcement learning and continuous adaptation.\n",
        "   - **Introduce multi-agent coordination protocols** for distributed environments (Hint 2), adding complexity in synchronization and collaboration.\n",
        "   - **Implement multi-objective optimization** (Hint 3) to balance accuracy, diversity, and fairness using techniques like Pareto efficiency and constrained policy optimization.\n",
        "   - **Utilize federated reinforcement learning across decentralized data silos** (Hint 4), involving differential privacy guarantees, secure aggregation algorithms, and homomorphic encryption.\n",
        "   - **Add adversarial scenarios** (Hint 5) requiring robustness against manipulation or spoofing attacks, incorporating adversarial training and robust Markov decision processes.\n",
        "   - These additions introduce advanced data structures, algorithms, and multiple interacting constraints, significantly increasing complexity.\n",
        "\n",
        "4) **Which parameters can we change?**\n",
        "\n",
        "   - **Algorithm Complexity**: Switch from basic Q-learning to federated reinforcement learning with dynamic graph neural networks.\n",
        "   - **Data Handling**: Move from static datasets to streaming data with concept drift, requiring real-time processing and adaptation.\n",
        "   - **System Architecture**: Transition to a decentralized system requiring multi-agent coordination and communication protocols.\n",
        "   - **Optimization Goals**: Introduce multi-objective optimization to balance conflicting goals like accuracy, diversity, and fairness.\n",
        "   - **Privacy and Security**: Incorporate differential privacy, secure aggregation, and homomorphic encryption to protect decentralized data.\n",
        "   - **Adversarial Robustness**: Design the system to be robust against adversarial attacks using methods like adversarial training and robust MDPs.\n",
        "   - **Complex Interactions**: Combine these elements to create non-obvious subproblems and logical traps that challenge naive approaches.\n",
        "\n",
        "5) **What can be a final hard prompt?**\n",
        "\n",
        "   - *As stated in the Final Complex Question above.*"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup\n",
        "\n",
        "```requirements.txt\n",
        "```\n"
      ],
      "metadata": {
        "id": "yMkfXPNSSdNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt\n",
        "\n",
        "I want to build a Python based recommendation engine that uses reinforcement learning to adaptively suggest items to users. The system should simulate user interaction data, apply Q learning to model user preferences, and improve recommendations based on historical user behavior over time.\n",
        "\n",
        "**Input Format**\n",
        "\n",
        "Your function will take the following inputs:\n",
        "\n",
        "- num_users: An integer representing the total number of users.\n",
        "- num_items: An integer representing the total number of items to recommend.\n",
        "- interactions: A dictionary where each key is a user ID (int) and the value is a list of item IDs (int) the user interacted with positively.\n",
        "- num_episodes: Integer specifying how many episodes to run the learning process.\n",
        "- epsilon: A float between 0 and 1 used for exploration in the epsilon greedy strategy.\n",
        "- alpha: A float between 0 and 1 for the Q learning learning rate.\n",
        "- gamma: A float between 0 and 1 for the discount factor.\n",
        "\n",
        "All inputs will be valid, and user/item IDs range from 0 to num_users - 1 and 0 to num_items - 1, respectively.\n",
        "\n",
        "**Output Format**\n",
        "\n",
        "Return a dictionary of recommended items:\n",
        "\n",
        "```python\n",
        "{\n",
        "  user_id (int): [recommended_item_ids (list of int)]\n",
        "}\n",
        "```\n",
        "\n",
        "Each user ID in the output should have a list of the top 3 recommended item IDs, based on the learned Q values after all episodes.\n",
        "\n",
        "**Examples**\n",
        "\n",
        "Example 1\n",
        "\n",
        "```python\n",
        "Input:\n",
        "num_users = 2\n",
        "num_items = 5\n",
        "interactions = {\n",
        "  0: [1, 2],\n",
        "  1: [0, 3]\n",
        "}\n",
        "num_episodes = 100\n",
        "epsilon = 0.2\n",
        "alpha = 0.5\n",
        "gamma = 0.9\n",
        "\n",
        "Output:\n",
        "{\n",
        "  0: [1, 2, 0],\n",
        "  1: [3, 0, 2]\n",
        "}\n",
        "```\n",
        "\n",
        "Example 2\n",
        "\n",
        "```python\n",
        "Input:\n",
        "num_users = 1\n",
        "num_items = 4\n",
        "interactions = {\n",
        "  0: [2]\n",
        "}\n",
        "num_episodes = 50\n",
        "epsilon = 0.1\n",
        "alpha = 0.3\n",
        "gamma = 0.8\n",
        "\n",
        "Output:\n",
        "{\n",
        "  0: [2, 0, 1]\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "w91X1rdLTlHK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q79gFg5DOtlN"
      },
      "source": [
        "# Requirements\n",
        "\n",
        "**Explicit and Implicit Points**\n",
        "\n",
        "- Model users as individual states and items as actions.\n",
        "- Use a Q table of shape (num_users, num_items) to store learned values.\n",
        "- Use an epsilon greedy strategy for action selection during training episodes.\n",
        "- Reward must be +1 if the item is in the user’s interaction list, and 0 otherwise.\n",
        "- Update Q values using the standard Q learning formula.\n",
        "- After training, return top 3 item IDs for each user with the highest Q values.\n",
        "\n",
        "**Solution Expectations**\n",
        "\n",
        "- The function must initialize the Q table and apply Q learning over the specified number of episodes.\n",
        "- Each user’s Q values must be updated based on simulated rewards.\n",
        "- Final recommendations should be the top 3 items with the highest Q values per user.\n",
        "- You must use random.seed(42) to ensure deterministic behavior.\n",
        "- Exploration and exploitation must be balanced based on the value of epsilon.\n",
        "\n",
        "**Signature of Expected Function**\n",
        "\n",
        "```python\n",
        "def q_learning_recommendation_engine(\n",
        "    num_users: int,\n",
        "    num_items: int,\n",
        "    interactions: dict[int, list[int]],\n",
        "    num_episodes: int,\n",
        "    epsilon: float,\n",
        "    alpha: float,\n",
        "    gamma: float\n",
        ") -> dict[int, list[int]]:\n",
        "```\n",
        "\n",
        "**Edge Case Behavior**\n",
        "\n",
        "- If num_users or num_items is 0, raise a ValueError.\n",
        "- If a user has no positive interactions, they should still be trained with reward 0 and receive the top 3 learned items (can be any).\n",
        "- If num_episodes is 0, return recommendations based on the initial Q table (which may be all zeros).\n",
        "- If multiple items have the same Q value, break ties by choosing the lower item ID.\n",
        "- You must demonstrate a test for each of these edge cases.\n",
        "\n",
        "**Constraints**\n",
        "\n",
        "1. Use random.seed(42) exactly once inside the function.\n",
        "2. Do not use external libraries such as pandas, numpy, scikit learn, or any RL specific frameworks.\n",
        "3. Do not use recursion; all logic must be written iteratively.\n",
        "4. You must not use global variables or modify input parameters.\n",
        "5. The Q table must be represented as a list of lists or dictionary, not a NumPy array.\n",
        "6. You must not hardcode item IDs or user IDs; they must be derived from input.\n",
        "7. Action selection must be done using random.random() and random.choice() only.\n",
        "8. No custom classes are required. You should use primitive Python data structures (lists, dictionaries, etc.)\n",
        "9. Return exactly 3 recommended items per user, even if Q values are all tied or zero.\n",
        "10. Do not use any of the following functions: eval(), exec(), globals(), heapq, or sorted() with custom keys.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSg66GeEbDAT"
      },
      "outputs": [],
      "source": [
        "#code\n",
        "\n",
        "import random\n",
        "\n",
        "\n",
        "def q_learning_recommendation_engine(\n",
        "    num_users: int,\n",
        "    num_items: int,\n",
        "    interactions: dict[int, list[int]],\n",
        "    num_episodes: int,\n",
        "    epsilon: float,\n",
        "    alpha: float,\n",
        "    gamma: float\n",
        ") -> dict[int, list[int]]:\n",
        "    \"\"\"Q learning based recommendation engine that learns user preferences through reinforcement learning.\"\"\"\n",
        "\n",
        "    # Edge case validations as per constraints\n",
        "    if num_users <= 0 or num_items <= 0:\n",
        "        raise ValueError(\"num_users and num_items must be positive integers\")\n",
        "\n",
        "    # Set seed exactly once as required\n",
        "    random.seed(42)\n",
        "\n",
        "    # Initialize Q table as list of lists\n",
        "    q_table = [[0.0 for _ in range(num_items)] for _ in range(num_users)]\n",
        "\n",
        "    # Training loop over episodes\n",
        "    for episode in range(num_episodes):\n",
        "        # Train each user in every episode\n",
        "        for user_id in range(num_users):\n",
        "            # Epsilon greedy action selection\n",
        "            if random.random() < epsilon:\n",
        "                # Exploration: random action\n",
        "                action = random.choice(list(range(num_items)))\n",
        "            else:\n",
        "                # Exploitation: choose action with highest Q value\n",
        "                max_q_value = max(q_table[user_id])\n",
        "                # Break ties by choosing lower item ID as per constraint\n",
        "                action = q_table[user_id].index(max_q_value)\n",
        "\n",
        "            # Calculate reward based on user interactions\n",
        "            user_interactions = interactions.get(user_id, [])\n",
        "            reward = 1 if action in user_interactions else 0\n",
        "\n",
        "            max_next_q = max(q_table[user_id])\n",
        "            q_table[user_id][action] += alpha * (reward + gamma * max_next_q - q_table[user_id][action])\n",
        "\n",
        "    # Generate recommendations: top 3 items with highest Q values per user\n",
        "    recommendations = {}\n",
        "    for user_id in range(num_users):\n",
        "        # Create list of (q_value, item_id) pairs for sorting\n",
        "        q_items = [(q_table[user_id][item_id], item_id) for item_id in range(num_items)]\n",
        "\n",
        "        for i in range(len(q_items)):\n",
        "            for j in range(i + 1, len(q_items)):\n",
        "                # Sort by Q value descending, then by item_id ascending\n",
        "                if (q_items[i][0] < q_items[j][0] or\n",
        "                    (q_items[i][0] == q_items[j][0] and q_items[i][1] > q_items[j][1])):\n",
        "                    q_items[i], q_items[j] = q_items[j], q_items[i]\n",
        "\n",
        "        # Extract top 3 item IDs\n",
        "        top_3_items = [item_id for _, item_id in q_items[:3]]\n",
        "        recommendations[user_id] = top_3_items\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "\n",
        "def test_q_learning_recommendation_engine():\n",
        "    \"\"\"Comprehensive test suite covering all requirements and edge cases.\"\"\"\n",
        "\n",
        "    print(\"Running comprehensive tests...\")\n",
        "\n",
        "    #  Example 1 from prompt\n",
        "    print(\"\\n1. Testing Example 1...\")\n",
        "    result1 = q_learning_recommendation_engine(\n",
        "        num_users=2,\n",
        "        num_items=5,\n",
        "        interactions={0: [1, 2], 1: [0, 3]},\n",
        "        num_episodes=100,\n",
        "        epsilon=0.2,\n",
        "        alpha=0.5,\n",
        "        gamma=0.9\n",
        "    )\n",
        "    print(f\"Result: {result1}\")\n",
        "    assert len(result1) == 2, \"Should have 2 users\"\n",
        "    assert all(len(items) == 3 for items in result1.values()), \"Each user should have 3 recommendations\"\n",
        "    assert all(0 <= item < 5 for items in result1.values() for item in items), \"All items should be valid\"\n",
        "    print(\" Example 1 passed\")\n",
        "\n",
        "    #  Example 2 from prompt\n",
        "    print(\"\\n2. Testing Example 2...\")\n",
        "    result2 = q_learning_recommendation_engine(\n",
        "        num_users=1,\n",
        "        num_items=4,\n",
        "        interactions={0: [2]},\n",
        "        num_episodes=50,\n",
        "        epsilon=0.1,\n",
        "        alpha=0.3,\n",
        "        gamma=0.8\n",
        "    )\n",
        "    print(f\"Result: {result2}\")\n",
        "    assert len(result2) == 1, \"Should have 1 user\"\n",
        "    assert len(result2[0]) == 3, \"User should have 3 recommendations\"\n",
        "    assert all(0 <= item < 4 for item in result2[0]), \"All items should be valid\"\n",
        "    print(\" Example 2 passed\")\n",
        "\n",
        "    #  Edge case  zero episodes\n",
        "    print(\"\\n3. Testing zero episodes...\")\n",
        "    result3 = q_learning_recommendation_engine(\n",
        "        num_users=2,\n",
        "        num_items=3,\n",
        "        interactions={0: [1], 1: [2]},\n",
        "        num_episodes=0,\n",
        "        epsilon=0.1,\n",
        "        alpha=0.5,\n",
        "        gamma=0.9\n",
        "    )\n",
        "    print(f\"Result: {result3}\")\n",
        "    assert len(result3) == 2, \"Should have 2 users\"\n",
        "    assert all(len(items) == 3 for items in result3.values()), \"Each user should have 3 recommendations\"\n",
        "    print(\" Zero episodes test passed\")\n",
        "\n",
        "    #  Edge case  user with no interactions\n",
        "    print(\"\\n4. Testing user with no interactions...\")\n",
        "    result4 = q_learning_recommendation_engine(\n",
        "        num_users=3,\n",
        "        num_items=4,\n",
        "        interactions={0: [1, 2], 2: [0]},  # User 1 has no interactions\n",
        "        num_episodes=50,\n",
        "        epsilon=0.2,\n",
        "        alpha=0.4,\n",
        "        gamma=0.8\n",
        "    )\n",
        "    print(f\"Result: {result4}\")\n",
        "    assert len(result4) == 3, \"Should have 3 users\"\n",
        "    assert len(result4[1]) == 3, \"User with no interactions should still get 3 recommendations\"\n",
        "    print(\" No interactions test passed\")\n",
        "\n",
        "    #  Edge case  invalid num_users\n",
        "    print(\"\\n5. Testing invalid num_users...\")\n",
        "    try:\n",
        "        q_learning_recommendation_engine(\n",
        "            num_users=0,\n",
        "            num_items=5,\n",
        "            interactions={},\n",
        "            num_episodes=10,\n",
        "            epsilon=0.1,\n",
        "            alpha=0.5,\n",
        "            gamma=0.9\n",
        "        )\n",
        "        assert False, \"Should have raised ValueError\"\n",
        "    except ValueError as e:\n",
        "        print(f\" Correctly raised ValueError: {e}\")\n",
        "\n",
        "    #  Edge case  invalid num_items\n",
        "    print(\"\\n6. Testing invalid num_items...\")\n",
        "    try:\n",
        "        q_learning_recommendation_engine(\n",
        "            num_users=2,\n",
        "            num_items=0,\n",
        "            interactions={0: [1]},\n",
        "            num_episodes=10,\n",
        "            epsilon=0.1,\n",
        "            alpha=0.5,\n",
        "            gamma=0.9\n",
        "        )\n",
        "        assert False, \"Should have raised ValueError\"\n",
        "    except ValueError as e:\n",
        "        print(f\" Correctly raised ValueError: {e}\")\n",
        "\n",
        "    #  Tie-breaking behavior (lower item ID wins)\n",
        "    print(\"\\n7. Testing tie-breaking with equal Q values...\")\n",
        "    result7 = q_learning_recommendation_engine(\n",
        "        num_users=1,\n",
        "        num_items=3,\n",
        "        interactions={},  # No interactions, all Q values should be equal\n",
        "        num_episodes=0,   # No learning, Q values remain 0\n",
        "        epsilon=0.1,\n",
        "        alpha=0.5,\n",
        "        gamma=0.9\n",
        "    )\n",
        "    print(f\"Result: {result7}\")\n",
        "    expected_order = [0, 1, 2]  # Should be sorted by item ID for ties\n",
        "    assert result7[0] == expected_order, f\"Expected {expected_order}, got {result7[0]}\"\n",
        "    print(\" Tie-breaking test passed\")\n",
        "\n",
        "    #  Boundary value testing\n",
        "    print(\"\\n8. Testing boundary values...\")\n",
        "    result8 = q_learning_recommendation_engine(\n",
        "        num_users=1,\n",
        "        num_items=1,\n",
        "        interactions={0: [0]},\n",
        "        num_episodes=1,\n",
        "        epsilon=1.0,  # Always explore\n",
        "        alpha=1.0,    # Maximum learning rate\n",
        "        gamma=0.0     # No future reward consideration\n",
        "    )\n",
        "    print(f\"Result: {result8}\")\n",
        "    assert len(result8[0]) == 1, \"Should recommend only 1 item when num_items=1\"\n",
        "    print(\" Boundary values test passed\")\n",
        "\n",
        "    #  Large scale test\n",
        "    print(\"\\n9. Testing larger scale...\")\n",
        "    large_interactions = {i: [i % 10, (i + 1) % 10] for i in range(20)}\n",
        "    result9 = q_learning_recommendation_engine(\n",
        "        num_users=20,\n",
        "        num_items=15,\n",
        "        interactions=large_interactions,\n",
        "        num_episodes=200,\n",
        "        epsilon=0.3,\n",
        "        alpha=0.1,\n",
        "        gamma=0.95\n",
        "    )\n",
        "    print(f\"Result size: {len(result9)} users\")\n",
        "    assert len(result9) == 20, \"Should have 20 users\"\n",
        "    assert all(len(items) == 3 for items in result9.values()), \"Each user should have 3 recommendations\"\n",
        "    print(\" Large scale test passed\")\n",
        "\n",
        "    #  Deterministic behavior verification\n",
        "    print(\"\\n10. Testing deterministic behavior...\")\n",
        "    result10a = q_learning_recommendation_engine(\n",
        "        num_users=2,\n",
        "        num_items=4,\n",
        "        interactions={0: [1], 1: [2]},\n",
        "        num_episodes=30,\n",
        "        epsilon=0.2,\n",
        "        alpha=0.5,\n",
        "        gamma=0.8\n",
        "    )\n",
        "    result10b = q_learning_recommendation_engine(\n",
        "        num_users=2,\n",
        "        num_items=4,\n",
        "        interactions={0: [1], 1: [2]},\n",
        "        num_episodes=30,\n",
        "        epsilon=0.2,\n",
        "        alpha=0.5,\n",
        "        gamma=0.8\n",
        "    )\n",
        "    print(f\"First run: {result10a}\")\n",
        "    print(f\"Second run: {result10b}\")\n",
        "    assert result10a == result10b, \"Results should be deterministic due to random.seed(42)\"\n",
        "    print(\" Deterministic behavior test passed\")\n",
        "\n",
        "    print(\"\\n All tests passed successfully!\")\n",
        "\n",
        "\n",
        "def validate_input_constraints(\n",
        "    num_users: int,\n",
        "    num_items: int,\n",
        "    interactions: dict[int, list[int]],\n",
        "    num_episodes: int,\n",
        "    epsilon: float,\n",
        "    alpha: float,\n",
        "    gamma: float\n",
        "):\n",
        "    \"\"\"Validate all input parameters meet the specified constraints.\"\"\"\n",
        "\n",
        "    print(\"Validating input constraints...\")\n",
        "\n",
        "    # Type validation\n",
        "    assert isinstance(num_users, int), \"num_users must be an integer\"\n",
        "    assert isinstance(num_items, int), \"num_items must be an integer\"\n",
        "    assert isinstance(interactions, dict), \"interactions must be a dictionary\"\n",
        "    assert isinstance(num_episodes, int), \"num_episodes must be an integer\"\n",
        "    assert isinstance(epsilon, float), \"epsilon must be a float\"\n",
        "    assert isinstance(alpha, float), \"alpha must be a float\"\n",
        "    assert isinstance(gamma, float), \"gamma must be a float\"\n",
        "\n",
        "    # Range validation\n",
        "    assert num_users > 0, \"num_users must be positive\"\n",
        "    assert num_items > 0, \"num_items must be positive\"\n",
        "    assert num_episodes >= 0, \"num_episodes must be non-negative\"\n",
        "    assert 0 <= epsilon <= 1, \"epsilon must be between 0 and 1\"\n",
        "    assert 0 <= alpha <= 1, \"alpha must be between 0 and 1\"\n",
        "    assert 0 <= gamma <= 1, \"gamma must be between 0 and 1\"\n",
        "\n",
        "    # Interactions validation\n",
        "    for user_id, item_list in interactions.items():\n",
        "        assert isinstance(user_id, int), \"User IDs must be integers\"\n",
        "        assert isinstance(item_list, list), \"Interaction values must be lists\"\n",
        "        assert 0 <= user_id < num_users, f\"User ID {user_id} out of range\"\n",
        "        for item_id in item_list:\n",
        "            assert isinstance(item_id, int), \"Item IDs must be integers\"\n",
        "            assert 0 <= item_id < num_items, f\"Item ID {item_id} out of range\"\n",
        "\n",
        "    print(\" All input constraints validated\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run comprehensive tests\n",
        "    test_q_learning_recommendation_engine()\n",
        "\n",
        "    # Validate input constraints\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    validate_input_constraints(\n",
        "        num_users=2,\n",
        "        num_items=5,\n",
        "        interactions={0: [1, 2], 1: [0, 3]},\n",
        "        num_episodes=100,\n",
        "        epsilon=0.2,\n",
        "        alpha=0.5,\n",
        "        gamma=0.9\n",
        "    )\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\" Q Learning Recommendation Engine is fully tested and ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUlcq7ycbHYw"
      },
      "outputs": [],
      "source": [
        "# tests\n",
        "\n",
        "import unittest\n",
        "from main import q_learning_recommendation_engine\n",
        "\n",
        "\n",
        "class TestQLearningRecommendationEngine(unittest.TestCase):\n",
        "    \"\"\"\n",
        "    Comprehensive unit test suite for q_learning_recommendation_engine\n",
        "\n",
        "    Tests based on prompt requirements and specifications.\n",
        "    Covers all edge cases, constraints, and expected behaviors.\n",
        "    \"\"\"\n",
        "\n",
        "    def test_deterministic_behavior(self):\n",
        "        \"\"\"Test that same inputs produce same outputs due to random.seed(42)\"\"\"\n",
        "        params = {\n",
        "            'num_users': 2,\n",
        "            'num_items': 4,\n",
        "            'interactions': {0: [1], 1: [2]},\n",
        "            'num_episodes': 20,\n",
        "            'epsilon': 0.3,\n",
        "            'alpha': 0.5,\n",
        "            'gamma': 0.9\n",
        "        }\n",
        "\n",
        "        result1 = q_learning_recommendation_engine(**params)\n",
        "        result2 = q_learning_recommendation_engine(**params)\n",
        "        result3 = q_learning_recommendation_engine(**params)\n",
        "\n",
        "        self.assertEqual(result1, result2)\n",
        "        self.assertEqual(result2, result3)\n",
        "\n",
        "    def test_output_format_validation(self):\n",
        "        \"\"\"Test output format matches specification requirements\"\"\"\n",
        "        result = q_learning_recommendation_engine(\n",
        "            num_users=3,\n",
        "            num_items=4,\n",
        "            interactions={0: [1], 1: [2]},\n",
        "            num_episodes=5,\n",
        "            epsilon=0.1,\n",
        "            alpha=0.5,\n",
        "            gamma=0.9\n",
        "        )\n",
        "\n",
        "        # Must be dictionary\n",
        "        self.assertIsInstance(result, dict)\n",
        "\n",
        "        # Must have all users\n",
        "        self.assertEqual(len(result), 3)\n",
        "        for user_id in range(3):\n",
        "            self.assertIn(user_id, result)\n",
        "\n",
        "        # Each user must have exactly 3 recommendations\n",
        "        for user_id, recommendations in result.items():\n",
        "            self.assertIsInstance(recommendations, list)\n",
        "            self.assertEqual(len(recommendations), 3)\n",
        "\n",
        "            # All recommendations must be valid item IDs\n",
        "            for item_id in recommendations:\n",
        "                self.assertIsInstance(item_id, int)\n",
        "                self.assertGreaterEqual(item_id, 0)\n",
        "                self.assertLess(item_id, 4)\n",
        "\n",
        "    def test_value_error_zero_users(self):\n",
        "        \"\"\"Test ValueError raised when num_users is 0\"\"\"\n",
        "        with self.assertRaises(ValueError):\n",
        "            q_learning_recommendation_engine(\n",
        "                num_users=0,\n",
        "                num_items=5,\n",
        "                interactions={},\n",
        "                num_episodes=10,\n",
        "                epsilon=0.1,\n",
        "                alpha=0.5,\n",
        "                gamma=0.9\n",
        "            )\n",
        "\n",
        "    def test_value_error_zero_items(self):\n",
        "        \"\"\"Test ValueError raised when num_items is 0\"\"\"\n",
        "        with self.assertRaises(ValueError):\n",
        "            q_learning_recommendation_engine(\n",
        "                num_users=2,\n",
        "                num_items=0,\n",
        "                interactions={},\n",
        "                num_episodes=10,\n",
        "                epsilon=0.1,\n",
        "                alpha=0.5,\n",
        "                gamma=0.9\n",
        "            )\n",
        "\n",
        "    def test_user_with_no_interactions(self):\n",
        "        \"\"\"Test user with no positive interactions still gets recommendations\"\"\"\n",
        "        result = q_learning_recommendation_engine(\n",
        "            num_users=3,\n",
        "            num_items=4,\n",
        "            interactions={0: [1, 2], 2: [0]},  # User 1 has no interactions\n",
        "            num_episodes=10,\n",
        "            epsilon=0.2,\n",
        "            alpha=0.5,\n",
        "            gamma=0.9\n",
        "        )\n",
        "\n",
        "        # All users should be present\n",
        "        self.assertEqual(len(result), 3)\n",
        "        self.assertIn(0, result)\n",
        "        self.assertIn(1, result)  # User with no interactions\n",
        "        self.assertIn(2, result)\n",
        "\n",
        "        # User with no interactions should still get 3 recommendations\n",
        "        self.assertEqual(len(result[1]), 3)\n",
        "\n",
        "        # All recommendations should be valid item IDs\n",
        "        for item_id in result[1]:\n",
        "            self.assertIsInstance(item_id, int)\n",
        "            self.assertGreaterEqual(item_id, 0)\n",
        "            self.assertLess(item_id, 4)\n",
        "\n",
        "    def test_zero_episodes_behavior(self):\n",
        "        \"\"\"Test behavior when num_episodes is 0 (no training)\"\"\"\n",
        "        result = q_learning_recommendation_engine(\n",
        "            num_users=2,\n",
        "            num_items=3,\n",
        "            interactions={0: [1], 1: [2]},\n",
        "            num_episodes=0,  # No training\n",
        "            epsilon=0.1,\n",
        "            alpha=0.5,\n",
        "            gamma=0.9\n",
        "        )\n",
        "\n",
        "        # Should return recommendations based on initial Q-table (all zeros)\n",
        "        # Tie-breaking should prefer lower item IDs\n",
        "        expected = {0: [0, 1, 2], 1: [0, 1, 2]}\n",
        "        self.assertEqual(result, expected)\n",
        "\n",
        "    def test_tie_breaking_lower_item_id(self):\n",
        "        \"\"\"Test tie-breaking chooses lower item ID when Q-values are equal\"\"\"\n",
        "        result = q_learning_recommendation_engine(\n",
        "            num_users=1,\n",
        "            num_items=5,\n",
        "            interactions={},  # No interactions = all Q-values remain equal\n",
        "            num_episodes=0,   # No training = Q-values stay at initial values\n",
        "            epsilon=0.1,\n",
        "            alpha=0.5,\n",
        "            gamma=0.9\n",
        "        )\n",
        "\n",
        "        # All Q-values should be equal, so should return [0, 1, 2]\n",
        "        expected = {0: [0, 1, 2]}\n",
        "        self.assertEqual(result, expected)\n",
        "\n",
        "    def test_empty_interactions_dictionary(self):\n",
        "        \"\"\"Test system handles empty interactions dictionary gracefully\"\"\"\n",
        "        result = q_learning_recommendation_engine(\n",
        "            num_users=2,\n",
        "            num_items=3,\n",
        "            interactions={},  # No user interactions at all\n",
        "            num_episodes=5,\n",
        "            epsilon=0.1,\n",
        "            alpha=0.5,\n",
        "            gamma=0.9\n",
        "        )\n",
        "\n",
        "        # Should provide recommendations for all users\n",
        "        self.assertEqual(len(result), 2)\n",
        "        self.assertIn(0, result)\n",
        "        self.assertIn(1, result)\n",
        "\n",
        "        # Each user should have exactly 3 recommendations\n",
        "        for user_id, recommendations in result.items():\n",
        "            self.assertEqual(len(recommendations), 3)\n",
        "            for item_id in recommendations:\n",
        "                self.assertIsInstance(item_id, int)\n",
        "                self.assertGreaterEqual(item_id, 0)\n",
        "                self.assertLess(item_id, 3)\n",
        "\n",
        "    def test_single_user_single_item(self):\n",
        "        \"\"\"Test boundary case with single user and single item\"\"\"\n",
        "        result = q_learning_recommendation_engine(\n",
        "            num_users=1,\n",
        "            num_items=1,\n",
        "            interactions={0: [0]},\n",
        "            num_episodes=5,\n",
        "            epsilon=0.1,\n",
        "            alpha=0.5,\n",
        "            gamma=0.9\n",
        "        )\n",
        "\n",
        "        # Should have one user\n",
        "        self.assertEqual(len(result), 1)\n",
        "        self.assertIn(0, result)\n",
        "\n",
        "        # User should have exactly 3 recommendations (requirement)\n",
        "        self.assertEqual(len(result[0]), 3)\n",
        "\n",
        "        # Since only 1 item exists, the implementation needs to handle this\n",
        "        # All recommendations should be valid (even if repeated)\n",
        "        for item_id in result[0]:\n",
        "            self.assertIsInstance(item_id, int)\n",
        "            self.assertGreaterEqual(item_id, 0)\n",
        "            self.assertLess(item_id, 1)\n",
        "\n",
        "    def test_extreme_epsilon_values(self):\n",
        "        \"\"\"Test boundary epsilon values (pure exploration and exploitation)\"\"\"\n",
        "        base_params = {\n",
        "            'num_users': 1,\n",
        "            'num_items': 3,\n",
        "            'interactions': {0: [2]},\n",
        "            'num_episodes': 5,\n",
        "            'alpha': 0.5,\n",
        "            'gamma': 0.9\n",
        "        }\n",
        "\n",
        "        # Test epsilon = 0.0 (pure exploitation)\n",
        "        result_exploit = q_learning_recommendation_engine(\n",
        "            **base_params,\n",
        "            epsilon=0.0\n",
        "        )\n",
        "        self.assertEqual(len(result_exploit), 1)\n",
        "        self.assertEqual(len(result_exploit[0]), 3)\n",
        "\n",
        "        # Test epsilon = 1.0 (pure exploration)\n",
        "        result_explore = q_learning_recommendation_engine(\n",
        "            **base_params,\n",
        "            epsilon=1.0\n",
        "        )\n",
        "        self.assertEqual(len(result_explore), 1)\n",
        "        self.assertEqual(len(result_explore[0]), 3)\n",
        "\n",
        "    def test_large_scale_input(self):\n",
        "        \"\"\"Test system handles larger user and item counts\"\"\"\n",
        "        # Create interactions for subset of users\n",
        "        interactions = {i: [i % 10, (i + 1) % 10] for i in range(0, 20, 2)}\n",
        "\n",
        "        result = q_learning_recommendation_engine(\n",
        "            num_users=20,\n",
        "            num_items=15,\n",
        "            interactions=interactions,\n",
        "            num_episodes=10,\n",
        "            epsilon=0.2,\n",
        "            alpha=0.3,\n",
        "            gamma=0.8\n",
        "        )\n",
        "\n",
        "        # Should have all 20 users\n",
        "        self.assertEqual(len(result), 20)\n",
        "\n",
        "        # Each user should have exactly 3 recommendations\n",
        "        for user_id in range(20):\n",
        "            self.assertIn(user_id, result)\n",
        "            self.assertEqual(len(result[user_id]), 3)\n",
        "\n",
        "            # All recommendations should be valid item IDs\n",
        "            for item_id in result[user_id]:\n",
        "                self.assertIsInstance(item_id, int)\n",
        "                self.assertGreaterEqual(item_id, 0)\n",
        "                self.assertLess(item_id, 15)\n",
        "\n",
        "    def test_reward_system_validation(self):\n",
        "        \"\"\"Test that reward system correctly identifies user interactions\"\"\"\n",
        "        # Simple case to verify learning occurs\n",
        "        result = q_learning_recommendation_engine(\n",
        "            num_users=1,\n",
        "            num_items=3,\n",
        "            interactions={0: [2]},  # User 0 likes item 2\n",
        "            num_episodes=10,\n",
        "            epsilon=0.1,  # Mostly exploitation\n",
        "            alpha=0.8,    # Fast learning\n",
        "            gamma=0.5\n",
        "        )\n",
        "\n",
        "        # Verify output format is correct\n",
        "        self.assertEqual(len(result), 1)\n",
        "        self.assertIn(0, result)\n",
        "        self.assertEqual(len(result[0]), 3)\n",
        "\n",
        "        # All items should be valid\n",
        "        for item_id in result[0]:\n",
        "            self.assertIsInstance(item_id, int)\n",
        "            self.assertGreaterEqual(item_id, 0)\n",
        "            self.assertLess(item_id, 3)\n",
        "\n",
        "    def test_interaction_key_types(self):\n",
        "        \"\"\"Test that interaction dictionary keys are handled correctly\"\"\"\n",
        "        result = q_learning_recommendation_engine(\n",
        "            num_users=2,\n",
        "            num_items=3,\n",
        "            interactions={0: [1], 1: [0, 2]},  # Integer keys as specified\n",
        "            num_episodes=5,\n",
        "            epsilon=0.2,\n",
        "            alpha=0.5,\n",
        "            gamma=0.9\n",
        "        )\n",
        "\n",
        "        # Standard validation\n",
        "        self.assertEqual(len(result), 2)\n",
        "        for user_id, recommendations in result.items():\n",
        "            self.assertIsInstance(user_id, int)\n",
        "            self.assertEqual(len(recommendations), 3)\n",
        "            for item_id in recommendations:\n",
        "                self.assertIsInstance(item_id, int)\n",
        "                self.assertGreaterEqual(item_id, 0)\n",
        "                self.assertLess(item_id, 3)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxZs2ehLYWkm"
      },
      "source": [
        "# Model Breaking Proof\n",
        "\n",
        "#### Model Breaking Task URL: <https://rlhf-v3.turing.com/prompt/f9255ef6-35c7-49d2-b36c-082cafc04870?origin=https%3A%2F%2Flabeling-ms.turing.com&redirect_url=https%3A%2F%2Flabeling-ms.turing.com%2Fconversations%2F44828%2Fview>\n",
        "\n",
        "#### Model code:\n",
        "\n",
        "```python\n",
        "def q_learning_recommendation_engine(\n",
        "    num_users: int,\n",
        "    num_items: int,\n",
        "    interactions: dict[int, list[int]],\n",
        "    num_episodes: int,\n",
        "    epsilon: float,\n",
        "    alpha: float,\n",
        "    gamma: float\n",
        ") -> dict[int, list[int]]:\n",
        "    \"\"\"\n",
        "    Builds a simple Q-learning-based recommendation engine for num_users users and num_items items.\n",
        "    \n",
        "    Each user is treated as a state, and each item as an action. A Q table (list of lists) with shape\n",
        "    (num_users, num_items) is maintained and updated over num_episodes episodes. The reward is +1 if\n",
        "    the user interacted positively with the item and 0 otherwise. Epsilon-greedy action selection is\n",
        "    used, and Q values are updated with the standard Q-learning update. After training, the function\n",
        "    returns a dictionary mapping each user to the top 3 items (by Q value).\n",
        "\n",
        "    Constraints and notes:\n",
        "     - Exactly one call to random.seed(42).\n",
        "     - No external libraries (pandas, numpy, etc.).\n",
        "     - No recursion, no global variables, no RL frameworks.\n",
        "     - Q must be stored in primitive Python structures (list of lists, etc.).\n",
        "     - Ties in Q values should be broken by smaller item ID.\n",
        "     - If num_users == 0 or num_items == 0, raise ValueError.\n",
        "     - If num_episodes == 0, return from the initial Q table (likely all zeros).\n",
        "     - If a user has no positive interactions, they still get Q updates with reward 0.\n",
        "     - Must return exactly 3 recommended items per user in a dictionary.\n",
        "    \"\"\"\n",
        "\n",
        "    import random\n",
        "    random.seed(42)  # Exactly once, as required\n",
        "\n",
        "    # Edge cases\n",
        "    if num_users == 0 or num_items == 0:\n",
        "        raise ValueError(\"num_users and num_items must be greater than 0.\")\n",
        "\n",
        "    # Initialize Q table (all zeros)\n",
        "    Q = []\n",
        "    for _ in range(num_users):\n",
        "        Q.append([0.0] * num_items)\n",
        "\n",
        "    # Convert interactions dict to a set for quick reward lookup\n",
        "    # interactions[user] = set_of_items\n",
        "    interaction_sets = {}\n",
        "    for user_id in range(num_users):\n",
        "        if user_id in interactions:\n",
        "            interaction_sets[user_id] = set(interactions[user_id])\n",
        "        else:\n",
        "            # If user not in interactions, they simply have an empty set\n",
        "            interaction_sets[user_id] = set()\n",
        "\n",
        "    # Run Q-learning for num_episodes\n",
        "    for _ in range(num_episodes):\n",
        "        # For each user, pick an action (item) according to epsilon-greedy\n",
        "        for user_id in range(num_users):\n",
        "            # Epsilon greedy selection\n",
        "            if random.random() < epsilon:\n",
        "                # Explore\n",
        "                action = random.choice(range(num_items))\n",
        "            else:\n",
        "                # Exploit: pick item with best Q value (break ties by item ID)\n",
        "                best_action = 0\n",
        "                best_q = Q[user_id][0]\n",
        "                for item_id in range(1, num_items):\n",
        "                    if Q[user_id][item_id] > best_q:\n",
        "                        best_q = Q[user_id][item_id]\n",
        "                        best_action = item_id\n",
        "                    elif Q[user_id][item_id] == best_q and item_id < best_action:\n",
        "                        best_action = item_id\n",
        "                action = best_action\n",
        "\n",
        "            # Determine reward\n",
        "            reward = 1 if action in interaction_sets[user_id] else 0\n",
        "\n",
        "            # Update Q value\n",
        "            # Next state is effectively the same user, so we take max Q over all items\n",
        "            max_q_next = max(Q[user_id])  # best future Q from the same state\n",
        "            old_q = Q[user_id][action]\n",
        "            Q[user_id][action] = old_q + alpha * (reward + gamma * max_q_next - old_q)\n",
        "\n",
        "    # After training, pick the top 3 items per user\n",
        "    recommendations = {}\n",
        "    for user_id in range(num_users):\n",
        "        # Build a list of ( -Qvalue, item_id ) so default sorted() will sort by highest Q first\n",
        "        # and by ascending item_id for ties in Q since negative sign inverts order for Q.\n",
        "        items_with_q = []\n",
        "        for item_id in range(num_items):\n",
        "            items_with_q.append((-Q[user_id][item_id], item_id))\n",
        "        # Sort by the tuple: first by -Q (ascending => Q descending), tie-break by item_id (ascending)\n",
        "\n",
        "        #  <Issue>: <Prompt explicit mentioned to avoid use of `sorted()` with custom keys or tuple tricks. This line uses tuple sorting to simulate key behavior.>\n",
        "    \n",
        "        items_with_q = sorted(items_with_q)\n",
        "        top_3_items = [tup[1] for tup in items_with_q[:3]]\n",
        "        recommendations[user_id] = top_3_items\n",
        "\n",
        "    return recommendations\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------\n",
        "# Below are demonstration tests for the specified edge cases.\n",
        "# You can comment them out or remove them as needed.\n",
        "# They are included here to illustrate compliance with the requirement:\n",
        "# \"You must demonstrate a test for each of these edge cases.\"\n",
        "# --------------------------------------------------------------------------\n",
        "\n",
        "def _demo_edge_cases():\n",
        "    print(\"Demonstrating edge cases...\")\n",
        "\n",
        "    # 1. num_users=0 or num_items=0 => raise ValueError\n",
        "    try:\n",
        "        q_learning_recommendation_engine(0, 5, {}, 10, 0.1, 0.2, 0.9)\n",
        "    except ValueError as e:\n",
        "        print(\"Edge case 1 passed: \", e)\n",
        "\n",
        "    try:\n",
        "        q_learning_recommendation_engine(5, 0, {}, 10, 0.1, 0.2, 0.9)\n",
        "    except ValueError as e:\n",
        "        print(\"Edge case 2 passed: \", e)\n",
        "\n",
        "    # 2. If num_episodes=0 => return recommendations from initial Q (all zeros)\n",
        "    #    Ties mean smallest item IDs come first => recommended [0,1,2] for each user.\n",
        "    result = q_learning_recommendation_engine(\n",
        "        num_users=2,\n",
        "        num_items=5,\n",
        "        interactions={},\n",
        "        num_episodes=0,  # no training\n",
        "        epsilon=0.3,\n",
        "        alpha=0.1,\n",
        "        gamma=0.9\n",
        "    )\n",
        "    print(\"Edge case 3 (num_episodes=0) =>\", result)\n",
        "\n",
        "    # 3. If a user has no positive interactions => they still get Q=0. Top 3 are [0,1,2].\n",
        "    #    We'll do a small training to see that it remains presumably 0.\n",
        "    result = q_learning_recommendation_engine(\n",
        "        num_users=1,\n",
        "        num_items=3,\n",
        "        interactions={},  # user 0 has no interactions\n",
        "        num_episodes=10,\n",
        "        epsilon=0.5,\n",
        "        alpha=0.5,\n",
        "        gamma=0.9\n",
        "    )\n",
        "    print(\"Edge case 4 (no interactions) =>\", result)\n",
        "\n",
        "# Uncomment to run the edge case demonstrations:\n",
        "# _demo_edge_cases()\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}