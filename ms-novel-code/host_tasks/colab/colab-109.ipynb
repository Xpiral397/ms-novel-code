{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKe-t1pIOo2f"
      },
      "source": [
        "# Metadata\n",
        "\n",
        "**L1 Taxonomy** - Backend Development\n",
        "\n",
        "**L2 Taxonomy** - API Gateways\n",
        "\n",
        "**Subtopic** - Implementing API Gateway Rate Limiting\n",
        "\n",
        "**Use Case** - Develop an anomaly detection module using scikit-learn to identify and alert on unusual spikes in API request rates within an Istio service mesh.\n",
        "\n",
        "**Programming Language** - Python\n",
        "\n",
        "**Target Model** - o1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS0xHSaZoEJO"
      },
      "source": [
        "# Setup\n",
        "\n",
        "```requirements.txt\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YToTLlRqOqmj"
      },
      "source": [
        "# Prompt\n",
        "\n",
        "TechAI Corp operates an AI platform serving three subscriber tiers (Free, Professional, Enterprise) with different usage limits and priorities. Different AI operations consume varying computational resources based on input content length. The system must enforce rate limits across multiple time windows (per-second, per-minute, per-day) while providing priority queuing during high traffic. You must implement a decorator-based rate limiting system that handles tokenization, multi-tier limits, and intelligent queuing to protect API endpoints while ensuring fair resource allocation.\n",
        "\n",
        "**Input Format**\n",
        "* Methods: list of method names to call [\"AIRateLimiter\", \"create_limiter\", \"generate_text\"]\n",
        "* Parameters: list of parameter lists for each method call\n",
        "* Constructor parameters: tier_configs, time_windows, operation_costs, limiter_type\n",
        "\n",
        "**Output Format**\n",
        "* Results: list of return values for each method call\n",
        "* Constructor/void methods return null\n",
        "* Successful calls: original response + rate limiting metadata\n",
        "* Queued calls: queue position, wait time, priority level\n",
        "* Rejected calls: error response with retry timing\n",
        "\n",
        "**Examples**\n",
        "\n",
        "```python\n",
        "# Example 1: Request-based rate limiting\n",
        "Input: [\"AIRateLimiter\", \"create_limiter\", \"generate_text\", \"generate_text\", \"generate_text\"]\n",
        "       [[{\"pro\": {\"limits\": [2, 10], \"priority\": 2}}, [1, 60], {}, \"request\"], [], [\"user1\", \"pro\", \"text_gen\", \"hello\"], [\"user1\", \"pro\", \"text_gen\", \"world\"], [\"user1\", \"pro\", \"text_gen\", \"test\"]]\n",
        "Output: [null, decorator_function, {\"result\": \"hello\", \"rate_limit_info\": {\"requests_consumed\": 1, \"remaining_requests\": [1, 9]}}, {\"result\": \"world\", \"rate_limit_info\": {\"requests_consumed\": 1, \"remaining_requests\": [0, 8]}}, {\"status\": \"queued\", \"queue_position\": 1, \"estimated_wait_time\": 1}]\n",
        "\n",
        "# Example 2: Token-based rate limiting  \n",
        "Input: [\"AIRateLimiter\", \"create_limiter\", \"generate_text\"]\n",
        "       [[{\"pro\": {\"token_limits\": [5, 50], \"priority\": 2}}, [1, 60], {\"text_gen\": 2}, \"token\"], [], [\"user1\", \"pro\", \"text_gen\", \"one two three\"]]\n",
        "Output: [null, decorator_function, {\"status\": \"queued\", \"queue_position\": 1, \"estimated_wait_time\": 1}]\n",
        "\n",
        "# Example 3: Successful token consumption\n",
        "Input: [\"AIRateLimiter\", \"create_limiter\", \"get_token_count\", \"generate_text\"]\n",
        "       [[{\"pro\": {\"token_limits\": [5, 50], \"priority\": 2}}, [1, 60], {\"text_gen\": 2}, \"token\"], [], [\"hello\"], [\"user1\", \"pro\", \"text_gen\", \"hello\"]]\n",
        "Output: [null, decorator_function, 1, {\"result\": \"hello\", \"rate_limit_info\": {\"tokens_consumed\": 2, \"remaining_tokens\": [3, 48]}}]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q79gFg5DOtlN"
      },
      "source": [
        "# Requirements\n",
        "\n",
        "**Explicit and Implicit Points**\n",
        "* Implement factory pattern creating two rate limiter types: \"token\" and \"request\" based\n",
        "* Token type: calculates consumption as content_word_count Ã— operation_cost\n",
        "* Request type: each API call consumes 1 from limits regardless of content size\n",
        "* Support three time windows simultaneously with independent limits\n",
        "* Implement priority queuing when limits exceeded (enterprise > pro > free)\n",
        "* Return appropriate metadata based on limiter type (tokens_consumed vs requests_consumed)\n",
        "\n",
        "**Solution Expectations**\n",
        "* Create AIRateLimiter class as factory for generating decorator functions\n",
        "* Implement get_token_count method for content tokenization by splitting on whitespace\n",
        "* Design extensible architecture allowing dynamic configuration changes\n",
        "* Store per-user state across multiple time windows using dictionaries\n",
        "* Handle queue management with priority levels and wait time estimation\n",
        "* Preserve original function behavior while adding rate limit metadata\n",
        "\n",
        "**Function Signatures**\n",
        "```python\n",
        "class AIRateLimiter:\n",
        "    def __init__(self, tier_configs: dict, time_windows: list, operation_costs: dict, limiter_type: str):\n",
        "        # Initialize factory with configurations and limiter type (\"token\" or \"request\")\n",
        "        pass\n",
        "    \n",
        "    def get_token_count(self, content: str) -> int:\n",
        "        # Tokenization method - split content and return token count\n",
        "        pass\n",
        "    \n",
        "    def create_limiter(self):\n",
        "        # Factory method that returns appropriate decorator based on limiter_type\n",
        "        pass\n",
        "```\n",
        "\n",
        "**Edge Case Behavior**\n",
        "* New users start with full token allocation for their tier\n",
        "* Handle time window boundary crossings and limit resets accurately\n",
        "* Empty content defaults to 1 token minimum consumption\n",
        "* Process queued requests by priority then FIFO order\n",
        "\n",
        "**Constraints**\n",
        "* Use only Python standard library modules\n",
        "* Store all state in memory using dictionaries\n",
        "* Support up to 1000 users with 10000 total requests\n",
        "* Content limited to 1000 words maximum\n",
        "* All configuration values must be positive integers\n",
        "* Decorated functions must have (user_id, tier, operation, content) as first parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bSg66GeEbDAT"
      },
      "outputs": [],
      "source": [
        "# code\n",
        "\n",
        "\"\"\" AI Rate Limiter. \"\"\"\n",
        "\n",
        "import time\n",
        "from typing import Dict, List, Any, Callable\n",
        "\n",
        "\n",
        "class AIRateLimiter:\n",
        "    \"\"\"Factory for creating AI platform rate limiters with priority queuing.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tier_configs: Dict[str, Dict],\n",
        "        time_windows: List[int],\n",
        "        operation_costs: Dict[str, int],\n",
        "        limiter_type: str,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize rate limiter factory.\n",
        "\n",
        "        Args:\n",
        "            tier_configs: {tier: {limits/token_limits: [w1, w2, w3],\n",
        "                          priority: int}}\n",
        "            time_windows: [seconds] for each window\n",
        "            operation_costs: {operation: cost_multiplier}\n",
        "            limiter_type: \"request\" or \"token\"\n",
        "        \"\"\"\n",
        "        self.tier_configs = tier_configs\n",
        "        self.time_windows = time_windows\n",
        "        self.operation_costs = operation_costs\n",
        "        self.limiter_type = limiter_type\n",
        "\n",
        "\n",
        "        self.user_usage: Dict[str, Dict[int, List[tuple]]] = (\n",
        "            {}\n",
        "        )  # user -> window -> [(timestamp, consumption)]\n",
        "        self.queue: List[Dict] = []  # Priority queue for blocked requests\n",
        "        self._queue_id_counter = 0  # Unique ID for each queue entry\n",
        "\n",
        "    def get_token_count(self, content: str) -> int:\n",
        "        \"\"\"Calculate token count from content by splitting on whitespace.\"\"\"\n",
        "        if not content or not content.strip():\n",
        "            return 1\n",
        "        return len(content.strip().split())\n",
        "\n",
        "    def create_limiter(self) -> Callable:\n",
        "        \"\"\"Return decorator based on limiter_type.\"\"\"\n",
        "\n",
        "        def decorator(func: Callable) -> Callable:\n",
        "            def wrapper(\n",
        "                user_id: str,\n",
        "                tier: str,\n",
        "                operation: str,\n",
        "                content: str,\n",
        "                *args,\n",
        "                **kwargs\n",
        "            ):\n",
        "                current_time = time.time()\n",
        "\n",
        "                # Calculate consumption based on mode\n",
        "                if self.limiter_type == \"request\":\n",
        "                    consumption = 1\n",
        "                else:  # token mode\n",
        "                    token_count = self.get_token_count(content)\n",
        "                    op_cost = self.operation_costs.get(operation, 1)\n",
        "                    consumption = token_count * op_cost\n",
        "\n",
        "                # Check if request can proceed\n",
        "                if self._can_proceed(user_id, tier, consumption, current_time):\n",
        "                    # Record usage and execute\n",
        "                    self._record_usage(user_id, consumption, current_time)\n",
        "                    result = func(\n",
        "                        user_id, tier, operation, content, *args, **kwargs\n",
        "                    )\n",
        "\n",
        "                    # Return with rate limit metadata\n",
        "                    return self._format_success_response(\n",
        "                        user_id, tier, result, consumption, current_time\n",
        "                    )\n",
        "                else:\n",
        "                    # Queue the request\n",
        "                    return self._add_to_queue(\n",
        "                        user_id, tier, operation, content, current_time\n",
        "                    )\n",
        "\n",
        "            return wrapper\n",
        "\n",
        "        return decorator\n",
        "\n",
        "    def _can_proceed(\n",
        "        self, user_id: str, tier: str, consumption: int, current_time: float\n",
        "    ) -> bool:\n",
        "        \"\"\"Check if request can proceed within all time window limits.\"\"\"\n",
        "        # Clean expired entries first\n",
        "        self._cleanup_expired_entries(user_id, current_time)\n",
        "\n",
        "        # Get tier limits\n",
        "        tier_config = self.tier_configs.get(tier, {})\n",
        "        limits_key = (\n",
        "            \"token_limits\" if self.limiter_type == \"token\" else \"limits\"\n",
        "        )\n",
        "        limits = tier_config.get(limits_key, [])\n",
        "\n",
        "        # Check each window - if ANY window would be exceeded, queue the\n",
        "        # request\n",
        "        for window_idx, limit in enumerate(limits):\n",
        "            current_usage = self._get_window_usage(user_id, window_idx)\n",
        "            if current_usage + consumption > limit:\n",
        "                return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _cleanup_expired_entries(\n",
        "        self, user_id: str, current_time: float\n",
        "    ) -> None:\n",
        "        \"\"\"Remove expired entries from user usage tracking.\"\"\"\n",
        "        if user_id not in self.user_usage:\n",
        "            return\n",
        "\n",
        "        for window_idx, window_seconds in enumerate(self.time_windows):\n",
        "            if window_idx in self.user_usage[user_id]:\n",
        "                cutoff_time = current_time - window_seconds\n",
        "                self.user_usage[user_id][window_idx] = [\n",
        "                    (timestamp, consumption)\n",
        "                    for timestamp, consumption in self.user_usage[user_id][\n",
        "                        window_idx\n",
        "                    ]\n",
        "                    if timestamp\n",
        "                    > cutoff_time  # Changed from >= to > for proper reset\n",
        "                ]\n",
        "\n",
        "    def _get_window_usage(self, user_id: str, window_idx: int) -> int:\n",
        "        \"\"\"Get current usage for specific time window.\"\"\"\n",
        "        if (\n",
        "            user_id not in self.user_usage\n",
        "            or window_idx not in self.user_usage[user_id]\n",
        "        ):\n",
        "            return 0\n",
        "\n",
        "        return sum(\n",
        "            consumption\n",
        "            for _, consumption in self.user_usage[user_id][window_idx]\n",
        "        )\n",
        "\n",
        "    def _record_usage(\n",
        "        self, user_id: str, consumption: int, timestamp: float\n",
        "    ) -> None:\n",
        "        \"\"\"Record usage across all time windows.\"\"\"\n",
        "        if user_id not in self.user_usage:\n",
        "            self.user_usage[user_id] = {}\n",
        "\n",
        "        for window_idx in range(len(self.time_windows)):\n",
        "            if window_idx not in self.user_usage[user_id]:\n",
        "                self.user_usage[user_id][window_idx] = []\n",
        "\n",
        "            self.user_usage[user_id][window_idx].append(\n",
        "                (timestamp, consumption)\n",
        "            )\n",
        "\n",
        "    def _format_success_response(\n",
        "        self,\n",
        "        user_id: str,\n",
        "        tier: str,\n",
        "        result: Any,\n",
        "        consumption: int,\n",
        "        current_time: float,\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Format successful response with rate limit metadata.\"\"\"\n",
        "        remaining = self._calculate_remaining_capacity(\n",
        "            user_id, tier, current_time\n",
        "        )\n",
        "\n",
        "        if self.limiter_type == \"request\":\n",
        "            return {\n",
        "                \"result\": result,\n",
        "                \"rate_limit_info\": {\n",
        "                    \"requests_consumed\": consumption,\n",
        "                    \"remaining_requests\": remaining,\n",
        "                },\n",
        "            }\n",
        "        else:  # token mode\n",
        "            return {\n",
        "                \"result\": result,\n",
        "                \"tokens_consumed\": consumption,\n",
        "                \"remaining_tokens\": remaining,\n",
        "            }\n",
        "\n",
        "    def _calculate_remaining_capacity(\n",
        "        self, user_id: str, tier: str, current_time: float\n",
        "    ) -> List[int]:\n",
        "        \"\"\"Calculate remaining capacity for each time window.\"\"\"\n",
        "        # Clean expired entries\n",
        "        self._cleanup_expired_entries(user_id, current_time)\n",
        "\n",
        "        # Get tier limits\n",
        "        tier_config = self.tier_configs.get(tier, {})\n",
        "        limits_key = (\n",
        "            \"token_limits\" if self.limiter_type == \"token\" else \"limits\"\n",
        "        )\n",
        "        limits = tier_config.get(limits_key, [])\n",
        "\n",
        "        remaining = []\n",
        "        for window_idx, limit in enumerate(limits):\n",
        "            current_usage = self._get_window_usage(user_id, window_idx)\n",
        "            remaining.append(max(0, limit - current_usage))\n",
        "\n",
        "        return remaining\n",
        "\n",
        "    def _add_to_queue(\n",
        "        self,\n",
        "        user_id: str,\n",
        "        tier: str,\n",
        "        operation: str,\n",
        "        content: str,\n",
        "        current_time: float,\n",
        "    ) -> Dict[str, Any]:\n",
        "        \"\"\"Add request to priority queue and return queue status.\"\"\"\n",
        "        # Get tier priority\n",
        "        tier_config = self.tier_configs.get(tier, {})\n",
        "        priority = tier_config.get(\"priority\", 1)\n",
        "\n",
        "        # Generate unique ID for this queue entry\n",
        "        self._queue_id_counter += 1\n",
        "        queue_id = self._queue_id_counter\n",
        "\n",
        "        # Add to queue\n",
        "        queue_entry = {\n",
        "            \"queue_id\": queue_id,\n",
        "            \"user_id\": user_id,\n",
        "            \"tier\": tier,\n",
        "            \"operation\": operation,\n",
        "            \"content\": content,\n",
        "            \"priority\": priority,\n",
        "            \"timestamp\": current_time,\n",
        "        }\n",
        "\n",
        "        self.queue.append(queue_entry)\n",
        "\n",
        "        # Sort queue by priority (descending) then by queue_id (ascending)\n",
        "        # for FIFO\n",
        "        self.queue.sort(key=lambda x: (-x[\"priority\"], x[\"queue_id\"]))\n",
        "\n",
        "        # Find position in queue using unique ID - AFTER sorting\n",
        "        queue_position = 1\n",
        "        for i, entry in enumerate(self.queue):\n",
        "            if entry[\"queue_id\"] == queue_id:\n",
        "                queue_position = i + 1\n",
        "                break\n",
        "\n",
        "        return {\n",
        "            \"status\": \"queued\",\n",
        "            \"queue_position\": queue_position,\n",
        "            \"estimated_wait_time\": self._estimate_wait_time(queue_position),\n",
        "            \"priority_level\": priority,\n",
        "        }\n",
        "\n",
        "    def _estimate_wait_time(self, queue_position: int) -> int:\n",
        "        \"\"\"Estimate wait time based on queue position.\"\"\"\n",
        "        # Simple estimation: 1 second per position ahead\n",
        "        return max(1, queue_position - 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KUlcq7ycbHYw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "8dc37438-99ed-47df-c1c1-2ffc521f0237"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'main'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-13-18485086.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0munittest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmock\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAIRateLimiter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'main'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# tests\n",
        "\n",
        "import unittest\n",
        "from unittest.mock import patch\n",
        "\n",
        "from main import AIRateLimiter\n",
        "\n",
        "\n",
        "class FakeClock:\n",
        "    \"\"\"Fake clock to control time.time() in tests.\"\"\"\n",
        "\n",
        "    def __init__(self, start: int = 0) -> None:\n",
        "        self._now = start\n",
        "\n",
        "    def time(self) -> int:\n",
        "        return self._now\n",
        "\n",
        "    def tick(self, seconds: int = 1) -> None:\n",
        "        self._now += seconds\n",
        "\n",
        "\n",
        "class TestAIRateLimiter(unittest.TestCase):\n",
        "    \"\"\"Unit tests for AIRateLimiter.\"\"\"\n",
        "\n",
        "    @classmethod\n",
        "    def setUpClass(cls):\n",
        "        cls.clock = FakeClock()\n",
        "        cls._patcher = patch(\"time.time\", cls.clock.time)\n",
        "        cls._patcher.start()\n",
        "\n",
        "    @classmethod\n",
        "    def tearDownClass(cls):\n",
        "        cls._patcher.stop()\n",
        "\n",
        "    def _request_limiter(self):\n",
        "        config = {\n",
        "            \"free\": {\"limits\": [2, 5, 10], \"priority\": 1},\n",
        "            \"enterprise\": {\"limits\": [2, 5, 10], \"priority\": 3},\n",
        "        }\n",
        "        windows = [1, 60, 86400]\n",
        "        return AIRateLimiter(config, windows, {}, \"request\").create_limiter()\n",
        "\n",
        "    def _token_limiter(self):\n",
        "        config = {\"pro\": {\"token_limits\": [5, 50, 100], \"priority\": 2}}\n",
        "        windows = [1, 60, 86400]\n",
        "        costs = {\"text_gen\": 2}\n",
        "        rl = AIRateLimiter(config, windows, costs, \"token\")\n",
        "        return rl.create_limiter(), rl\n",
        "\n",
        "    def test_request_success_counts(self):\n",
        "        limiter = self._request_limiter()\n",
        "\n",
        "        @limiter\n",
        "        def api(user, tier, op, content):\n",
        "            return content\n",
        "\n",
        "        response = api(\"u1\", \"free\", \"op\", \"x\")\n",
        "        self.assertEqual(\n",
        "            response[\"rate_limit_info\"][\"requests_consumed\"], 1\n",
        "        )\n",
        "        self.assertEqual(\n",
        "            response[\"rate_limit_info\"][\"remaining_requests\"][0], 1\n",
        "        )\n",
        "\n",
        "    def test_request_window_exhaustion_queue(self):\n",
        "        limiter = self._request_limiter()\n",
        "\n",
        "        @limiter\n",
        "        def api(user, tier, op, content):\n",
        "            return content\n",
        "\n",
        "        api(\"u2\", \"free\", \"op\", \"a\")\n",
        "        api(\"u2\", \"free\", \"op\", \"b\")\n",
        "        queued = api(\"u2\", \"free\", \"op\", \"c\")\n",
        "        self.assertEqual(queued[\"status\"], \"queued\")\n",
        "\n",
        "    def test_request_reset_after_window(self):\n",
        "        limiter = self._request_limiter()\n",
        "\n",
        "        @limiter\n",
        "        def api(user, tier, op, content):\n",
        "            return content\n",
        "\n",
        "        api(\"u3\", \"free\", \"op\", \"a\")\n",
        "        api(\"u3\", \"free\", \"op\", \"b\")\n",
        "        self.clock.tick(1)\n",
        "        response = api(\"u3\", \"free\", \"op\", \"c\")\n",
        "        self.assertIn(\"rate_limit_info\", response)\n",
        "\n",
        "    def test_token_success_counts(self):\n",
        "        limiter, _ = self._token_limiter()\n",
        "\n",
        "        @limiter\n",
        "        def api(user, tier, op, content):\n",
        "            return content\n",
        "\n",
        "        response = api(\"u4\", \"pro\", \"text_gen\", \"one two\")\n",
        "        self.assertEqual(response[\"tokens_consumed\"], 4)\n",
        "        self.assertEqual(response[\"remaining_tokens\"][0], 1)\n",
        "\n",
        "    def test_token_empty_content_minimum(self):\n",
        "        limiter, rl = self._token_limiter()\n",
        "\n",
        "        @limiter\n",
        "        def api(user, tier, op, content):\n",
        "            return content\n",
        "\n",
        "        self.assertEqual(rl.get_token_count(\"\"), 1)\n",
        "        response = api(\"u5\", \"pro\", \"text_gen\", \"\")\n",
        "        self.assertEqual(response[\"tokens_consumed\"], 2)\n",
        "\n",
        "    def test_token_queue_on_exceed(self):\n",
        "        limiter, _ = self._token_limiter()\n",
        "\n",
        "        @limiter\n",
        "        def api(user, tier, op, content):\n",
        "            return content\n",
        "\n",
        "        api(\"u6\", \"pro\", \"text_gen\", \"one two\")\n",
        "        queued = api(\"u6\", \"pro\", \"text_gen\", \"one two three\")\n",
        "        self.assertEqual(queued[\"status\"], \"queued\")\n",
        "\n",
        "    def test_priority_enterprise_ahead_of_free(self):\n",
        "        limiter = self._request_limiter()\n",
        "\n",
        "        @limiter\n",
        "        def api(user, tier, op, content):\n",
        "            return content\n",
        "\n",
        "        # Exhaust free-tier user\n",
        "        api(\"free_user\", \"free\", \"op\", \"x\")\n",
        "        api(\"free_user\", \"free\", \"op\", \"y\")\n",
        "\n",
        "        # Queue third call\n",
        "        q_free = api(\"free_user\", \"free\", \"op\", \"z\")\n",
        "\n",
        "        # Enterprise still succeeds\n",
        "        response = api(\"e1\", \"enterprise\", \"op\", \"z\")\n",
        "        self.assertIn(\"rate_limit_info\", response)\n",
        "        self.assertNotIn(\"status\", response)\n",
        "\n",
        "        # Now enterprise hits its own limit\n",
        "        api(\"e1\", \"enterprise\", \"op\", \"a\")\n",
        "        api(\"e1\", \"enterprise\", \"op\", \"b\")\n",
        "        q_ent = api(\"e1\", \"enterprise\", \"op\", \"c\")\n",
        "        self.assertEqual(q_ent[\"status\"], \"queued\")\n",
        "        self.assertLess(\n",
        "            q_ent[\"queue_position\"], q_free[\"queue_position\"]\n",
        "        )\n",
        "\n",
        "    def test_fifo_within_same_priority(self):\n",
        "        limiter = self._request_limiter()\n",
        "\n",
        "        @limiter\n",
        "        def api(user, tier, op, content):\n",
        "            return content\n",
        "\n",
        "        api(\"fifo_user\", \"free\", \"op\", \"x\")\n",
        "        api(\"fifo_user\", \"free\", \"op\", \"y\")\n",
        "\n",
        "        q1 = api(\"fifo_user\", \"free\", \"op\", \"z\")\n",
        "        q2 = api(\"fifo_user\", \"free\", \"op\", \"w\")\n",
        "        self.assertLess(q1[\"queue_position\"], q2[\"queue_position\"])\n",
        "\n",
        "    def test_helper_token_count_accuracy(self):\n",
        "        _, rl = self._token_limiter()\n",
        "        self.assertEqual(rl.get_token_count(\"one two three\"), 3)\n",
        "\n",
        "    def test_helper_token_count_empty(self):\n",
        "        _, rl = self._token_limiter()\n",
        "        self.assertEqual(rl.get_token_count(\"\"), 1)\n",
        "\n",
        "    def test_new_user_full_allocation(self):\n",
        "        limiter = self._request_limiter()\n",
        "\n",
        "        @limiter\n",
        "        def api(user, tier, op, content):\n",
        "            return content\n",
        "\n",
        "        response = api(\"newbie\", \"free\", \"op\", \"hi\")\n",
        "        self.assertEqual(\n",
        "            response[\"rate_limit_info\"][\"remaining_requests\"][0], 1\n",
        "        )\n",
        "\n",
        "    def test_daily_window_reset(self):\n",
        "        limiter = self._request_limiter()\n",
        "\n",
        "        @limiter\n",
        "        def api(user, tier, op, content):\n",
        "            return content\n",
        "\n",
        "        api(\"u7\", \"free\", \"op\", \"a\")\n",
        "        self.cloud.tick(86400)\n",
        "        response = api(\"u7\", \"free\", \"op\", \"b\")\n",
        "        self.assertEqual(\n",
        "            response[\"rate_limit_info\"][\"remaining_requests\"][2], 9\n",
        "        )\n",
        "\n",
        "    def test_multi_user_isolated_state(self):\n",
        "        limiter = self._request_limiter()\n",
        "\n",
        "        @limiter\n",
        "        def api(user, tier, op, content):\n",
        "            return content\n",
        "\n",
        "        r1 = api(\"userA\", \"free\", \"op\", \"x\")\n",
        "        r2 = api(\"userB\", \"free\", \"op\", \"x\")\n",
        "        self.assertEqual(\n",
        "            r1[\"rate_limit_info\"][\"remaining_requests\"][0], 1\n",
        "        )\n",
        "        self.assertEqual(\n",
        "            r2[\"rate_limit_info\"][\"remaining_requests\"][0], 1\n",
        "        )\n",
        "    def test_queued_requests_are_never_retried(self):\n",
        "      limiter = self._request_limiter()\n",
        "\n",
        "      @limiter\n",
        "      def api(user, tier, op, content):\n",
        "          return content\n",
        "\n",
        "      # Exhaust the user's quota\n",
        "      api(\"qtest\", \"free\", \"op\", \"1\")\n",
        "      api(\"qtest\", \"free\", \"op\", \"2\")\n",
        "\n",
        "      # This should be queued\n",
        "      queued = api(\"qtest\", \"free\", \"op\", \"3\")\n",
        "      self.assertEqual(queued[\"status\"], \"queued\")\n",
        "\n",
        "      # Wait for window to reset\n",
        "      self.clock.tick(1)\n",
        "\n",
        "      # Call again to prove fresh requests work\n",
        "      fresh = api(\"qtest\", \"free\", \"op\", \"4\")\n",
        "      self.assertIn(\"rate_limit_info\", fresh)\n",
        "\n",
        "      # The previously queued request is still not reprocessed\n",
        "      # So queue should still contain that item\n",
        "      self.assertEqual(len(limiter.__self__.queue), 1)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxZs2ehLYWkm"
      },
      "source": [
        "# Model Breaking Proof\n",
        "\n",
        "#### Model Breaking Task URL: <https://rlhf-v3.turing.com/prompt/b8ad475f-17f2-42bb-9adf-c79be63d9a89?origin=https%3A%2F%2Flabeling-ms.turing.com&redirect_url=https%3A%2F%2Flabeling-ms.turing.com%2Fconversations%2F66273%2Fview>\n",
        "\n",
        "#### Model code:\n",
        "\n",
        "```python\n",
        "# code generated by the model\n",
        "\n",
        "import time\n",
        "from collections import deque, defaultdict\n",
        "\n",
        "class AIRateLimiter:\n",
        "    def __init__(self, tier_configs: dict, time_windows: list, operation_costs: dict, limiter_type: str):\n",
        "        \"\"\"\n",
        "        tier_configs: ex) {\n",
        "            'free': {'limits': [5, 100, 1000], 'priority': 1},\n",
        "            'pro': {'limits': [20, 300, 3000], 'priority': 2},\n",
        "            'enterprise': {'limits': [100, 2000, 10000], 'priority': 3},\n",
        "            ...\n",
        "        }\n",
        "        or for token-based:\n",
        "            'pro': {'token_limits': [5, 50], 'priority': 2}, ...\n",
        "        \n",
        "        time_windows: list of time window sizes in seconds (e.g., [1, 60, 86400] for second, minute, day)\n",
        "        operation_costs: dict mapping operation -> token cost (for token-based mode)\n",
        "        limiter_type: \"request\" or \"token\"\n",
        "        \"\"\"\n",
        "        # <Issue>: Missing validation of positive integer constraints for configuration values\n",
        "        self.tier_configs = tier_configs\n",
        "        self.time_windows = time_windows  # e.g. [1, 60, 86400]\n",
        "        self.operation_costs = operation_costs  # e.g. {\"text_gen\": 2}\n",
        "        self.limiter_type = limiter_type  # \"token\" or \"request\"\n",
        "\n",
        "        # usage stores rolling usage for each user & tier & window index\n",
        "        # usage[(user_id, tier, i)] = {\"count\": <count_in_current_window>, \"start_time\": <epoch>}\n",
        "        self.usage = {}\n",
        "\n",
        "        # A global queue for requests that exceed rate limits.\n",
        "        # In a real system, you might process this queue asynchronously.\n",
        "        # For simplicity, we store them, and return \"queued\" responses.\n",
        "        self.request_queue = deque()\n",
        "\n",
        "        # This will track a global incremental ID for queued requests.\n",
        "        # In a real system, you might want more robust queue management.\n",
        "        # <Issue>: Global counter doesn't provide correct per-user queue positions\n",
        "        self.queue_counter = 0\n",
        "\n",
        "    def get_token_count(self, content: str) -> int:\n",
        "        \"\"\"\n",
        "        Tokenization method - split content and return token count.\n",
        "        Content with zero tokens defaults to 1 token consumption.\n",
        "        \"\"\"\n",
        "        tokens = content.split()\n",
        "        count = len(tokens)\n",
        "        return count if count > 0 else 1\n",
        "\n",
        "    def _check_and_consume_tokens(self, user_id: str, tier: str, operation: str, content: str):\n",
        "        \"\"\"\n",
        "        Internal helper: checks token usage against the user's tier token limits.\n",
        "        Returns (proceed: bool, data: dict).\n",
        "           If proceed == True, data contains rate limit metadata.\n",
        "           If proceed == False, data contains queue or reject info.\n",
        "        \"\"\"\n",
        "        now = time.time()\n",
        "        tier_conf = self.tier_configs.get(tier)\n",
        "        if not tier_conf or \"token_limits\" not in tier_conf:\n",
        "            return (False, {\"status\": \"error\", \"message\": f\"No token limits for tier '{tier}'\"})\n",
        "\n",
        "        token_limits = tier_conf[\"token_limits\"]  # e.g. [5, 50, 500]\n",
        "        priority = tier_conf.get(\"priority\", 1)\n",
        "\n",
        "        # Calculate how many tokens are required for this request\n",
        "        token_count = self.get_token_count(content)\n",
        "        operation_cost = self.operation_costs.get(operation, 1)\n",
        "        tokens_needed = token_count * operation_cost\n",
        "\n",
        "        # Check usage in each time window\n",
        "        for i, window_size in enumerate(self.time_windows):\n",
        "            limit = token_limits[i]\n",
        "            usage_key = (user_id, tier, i)\n",
        "            usage_data = self.usage.get(usage_key, {\"count\": 0, \"start_time\": now})\n",
        "\n",
        "            # Reset window if the time has elapsed\n",
        "            if (now - usage_data[\"start_time\"]) >= window_size:\n",
        "                usage_data[\"count\"] = 0\n",
        "                usage_data[\"start_time\"] = now\n",
        "\n",
        "            # If adding tokens_needed exceeds the limit, queue or reject\n",
        "            if usage_data[\"count\"] + tokens_needed > limit:\n",
        "                # Queue the request\n",
        "                # <Issue>: Queue counter incremented before assignment, causing incorrect position numbering\n",
        "           \n",
        "                self.queue_counter += 1\n",
        "                queue_pos = self.queue_counter\n",
        "                queued_info = {\n",
        "                    \"status\": \"queued\",\n",
        "                    \"queue_position\": queue_pos,\n",
        "                    \"estimated_wait_time\": 1,  # simplistic wait estimate\n",
        "                    \"priority_level\": priority\n",
        "                }\n",
        "                return (False, queued_info)\n",
        "\n",
        "        # If we can proceed, update usage across all windows\n",
        "        for i, window_size in enumerate(self.time_windows):\n",
        "            usage_key = (user_id, tier, i)\n",
        "            usage_data = self.usage.get(usage_key, {\"count\": 0, \"start_time\": now})\n",
        "            if (now - usage_data[\"start_time\"]) >= window_size:\n",
        "                usage_data[\"count\"] = 0\n",
        "                usage_data[\"start_time\"] = now\n",
        "            usage_data[\"count\"] += tokens_needed\n",
        "            self.usage[usage_key] = usage_data\n",
        "\n",
        "        # Build metadata about consumed tokens and remaining for each window\n",
        "        remaining_tokens = []\n",
        "        for i, limit in enumerate(token_limits):\n",
        "            usage_key = (user_id, tier, i)\n",
        "            usage_data = self.usage[usage_key]\n",
        "            used = usage_data[\"count\"]\n",
        "            remain = limit - used\n",
        "            remaining_tokens.append(remain)\n",
        "\n",
        "        rate_limit_info = {\n",
        "            \"tokens_consumed\": tokens_needed,\n",
        "            \"remaining_tokens\": remaining_tokens\n",
        "        }\n",
        "        return (True, rate_limit_info)\n",
        "\n",
        "    def _check_and_consume_requests(self, user_id: str, tier: str):\n",
        "        \"\"\"\n",
        "        Internal helper: checks request usage against the user's tier request limits.\n",
        "        Returns (proceed: bool, data: dict).\n",
        "        \"\"\"\n",
        "        now = time.time()\n",
        "        tier_conf = self.tier_configs.get(tier)\n",
        "        if not tier_conf or \"limits\" not in tier_conf:\n",
        "            return (False, {\"status\": \"error\", \"message\": f\"No request limits for tier '{tier}'\"})\n",
        "\n",
        "        request_limits = tier_conf[\"limits\"]  # e.g. [2, 10, 100]\n",
        "        priority = tier_conf.get(\"priority\", 1)\n",
        "\n",
        "        # Each request consumes 1 usage\n",
        "        for i, window_size in enumerate(self.time_windows):\n",
        "            limit = request_limits[i]\n",
        "            usage_key = (user_id, tier, i)\n",
        "            usage_data = self.usage.get(usage_key, {\"count\": 0, \"start_time\": now})\n",
        "\n",
        "            # Reset window if the time has elapsed\n",
        "            if (now - usage_data[\"start_time\"]) >= window_size:\n",
        "                usage_data[\"count\"] = 0\n",
        "                usage_data[\"start_time\"] = now\n",
        "\n",
        "            # Check if adding 1 usage would exceed the limit\n",
        "            if usage_data[\"count\"] + 1 > limit:\n",
        "                self.queue_counter += 1\n",
        "                queue_pos = self.queue_counter\n",
        "                queued_info = {\n",
        "                    \"status\": \"queued\",\n",
        "                    \"queue_position\": queue_pos,\n",
        "                    \"estimated_wait_time\": 1,  # simplistic wait estimate\n",
        "                    \"priority_level\": priority\n",
        "                }\n",
        "                return (False, queued_info)\n",
        "\n",
        "        # If we can proceed, update usage\n",
        "        for i, window_size in enumerate(self.time_windows):\n",
        "            usage_key = (user_id, tier, i)\n",
        "            usage_data = self.usage.get(usage_key, {\"count\": 0, \"start_time\": now})\n",
        "            if (now - usage_data[\"start_time\"]) >= window_size:\n",
        "                usage_data[\"count\"] = 0\n",
        "                usage_data[\"start_time\"] = now\n",
        "            usage_data[\"count\"] += 1\n",
        "            self.usage[usage_key] = usage_data\n",
        "\n",
        "        # Build rate limit metadata\n",
        "        remaining_requests = []\n",
        "        for i, limit in enumerate(request_limits):\n",
        "            usage_key = (user_id, tier, i)\n",
        "            usage_data = self.usage[usage_key]\n",
        "            used = usage_data[\"count\"]\n",
        "            remain = limit - used\n",
        "            remaining_requests.append(remain)\n",
        "\n",
        "        rate_limit_info = {\n",
        "            \"requests_consumed\": 1,\n",
        "            \"remaining_requests\": remaining_requests\n",
        "        }\n",
        "        return (True, rate_limit_info)\n",
        "\n",
        "    def create_limiter(self):\n",
        "        \"\"\"\n",
        "        Factory method that returns a decorator enforcing either token-based or request-based\n",
        "        rate limiting, depending on the initialized limiter_type.\n",
        "        \"\"\"\n",
        "        if self.limiter_type == \"token\":\n",
        "            def token_decorator(func):\n",
        "                def wrapper(user_id, tier, operation, content, *args, **kwargs):\n",
        "                    can_proceed, info = self._check_and_consume_tokens(user_id, tier, operation, content)\n",
        "                    if can_proceed:\n",
        "                        # Proceed with the original function call\n",
        "                        result = func(user_id, tier, operation, content, *args, **kwargs)\n",
        "                        return {\n",
        "                            \"result\": result,\n",
        "                            \"rate_limit_info\": info\n",
        "                        }\n",
        "                    else:\n",
        "                        # Return the queue or error info\n",
        "                        return info\n",
        "                return wrapper\n",
        "            return token_decorator\n",
        "\n",
        "        elif self.limiter_type == \"request\":\n",
        "            def request_decorator(func):\n",
        "                def wrapper(user_id, tier, operation, content, *args, **kwargs):\n",
        "                    can_proceed, info = self._check_and_consume_requests(user_id, tier)\n",
        "                    if can_proceed:\n",
        "                        result = func(user_id, tier, operation, content, *args, **kwargs)\n",
        "                        return {\n",
        "                            \"result\": result,\n",
        "                            \"rate_limit_info\": info\n",
        "                        }\n",
        "                    else:\n",
        "                        return info\n",
        "                return wrapper\n",
        "            return request_decorator\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Invalid limiter type. Must be 'token' or 'request'.\")\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# Below is an optional helper to demonstrate how one might process inputs and\n",
        "# produce outputs in the format described in the problem statement.\n",
        "###############################################################################\n",
        "\n",
        "def simulate_api_calls(methods, parameters):\n",
        "    \"\"\"\n",
        "    Demonstrates how to handle the Input/Output format described in the prompt.\n",
        "\n",
        "    methods: list of method names, e.g. [\"AIRateLimiter\", \"create_limiter\", \"generate_text\", ...]\n",
        "    parameters: list of parameter lists, one for each method call.\n",
        "    \n",
        "    Returns a list of results parallel to the method calls:\n",
        "      - Constructor or void methods return None to match \"null\" in the output\n",
        "      - Decorator creation returns a placeholder string (e.g., \"decorator_function\")\n",
        "      - Actual API calls return either success or queue info as specified\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    instance = None\n",
        "    decorator = None\n",
        "\n",
        "    # We'll create a dummy function to illustrate usage:\n",
        "    def generate_text(user_id, tier, operation, content, *args, **kwargs):\n",
        "        # Just echo the content as if we \"generated\" it\n",
        "        return content\n",
        "\n",
        "    for method, args in zip(methods, parameters):\n",
        "        if method == \"AIRateLimiter\":\n",
        "            # Instantiate the rate limiter\n",
        "            # args[0] = tier_configs, args[1] = time_windows, args[2] = operation_costs, args[3] = limiter_type\n",
        "            tier_configs, time_windows, operation_costs, limiter_type = args\n",
        "            instance = AIRateLimiter(tier_configs, time_windows, operation_costs, limiter_type)\n",
        "            results.append(None)  # constructor returns null\n",
        "        elif method == \"create_limiter\":\n",
        "            # Must have an instance\n",
        "            if not instance:\n",
        "                results.append({\"error\": \"No AIRateLimiter instance exists\"})\n",
        "                continue\n",
        "            decorator = instance.create_limiter()\n",
        "            # Return a placeholder to mimic the example (decorator functions can't easily be serialized)\n",
        "            results.append(\"decorator_function\")\n",
        "        elif method == \"get_token_count\":\n",
        "            # Simple pass-through\n",
        "            if not instance:\n",
        "                results.append({\"error\": \"No AIRateLimiter instance exists\"})\n",
        "                continue\n",
        "            content = args[0]\n",
        "            count = instance.get_token_count(content)\n",
        "            results.append(count)\n",
        "        else:\n",
        "            # Any other method is presumably an API call like \"generate_text\"\n",
        "            if not decorator:\n",
        "                results.append({\"error\": f\"No decorator created before calling {method}\"})\n",
        "                continue\n",
        "            # Wrap the dummy function with the chosen decorator\n",
        "            wrapped_func = decorator(generate_text)\n",
        "            # Call the wrapped function with the provided arguments\n",
        "            outputs = wrapped_func(*args)\n",
        "            results.append(outputs)\n",
        "\n",
        "    return results\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}