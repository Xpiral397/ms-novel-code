{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKe-t1pIOo2f"
      },
      "source": [
        "# Metadata\n",
        "\n",
        "**L1 Taxonomy** - Data Science and Machine Learning\n",
        "\n",
        "**L2 Taxonomy** - Data Manipulation\n",
        "\n",
        "**Subtopic** - Parsing and cleaning dates and times in a dataset (multiple formats) using Python\n",
        "\n",
        "**Use Case** - Develop a Python module that takes a CSV file with mixed date formats as input, parses the dates into a uniform format, and cleans any erroneous or missing date entries. The module should handle a variety of common date formats and be able to correct minor errors, such as typos or swapped month and day values. It should also be able to handle missing date entries by either discarding the entire row or filling in the missing value with a specified default. The cleaned data should be output to a new CSV file.\n",
        "\n",
        "**Programming Language** - Python\n",
        "\n",
        "**Target Model** - GPT-4o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS0xHSaZoEJO"
      },
      "source": [
        "# Setup\n",
        "\n",
        "```requirements.txt\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YToTLlRqOqmj"
      },
      "source": [
        "# Prompt\n",
        "## Problem Description\n",
        "\n",
        "You are tasked with building a robust date parsing and cleaning module for a data analytics pipeline that processes customer transaction data from multiple international sources. The module must intelligently parse mixed date formats within CSV files, automatically detect and correct common formatting errors, and standardize all dates to a consistent output format while preserving data integrity. This system needs to handle ambiguous date representations (like distinguishing between US and international date formats), correct transposition errors, and manage missing values according to configurable strategies. The deliverable is a complete Python module that can process large CSV files efficiently while maintaining deterministic behavior for quality assurance purposes.\n",
        "\n",
        "## Input Format\n",
        "\n",
        "The module processes CSV files containing date columns mixed with other data types. Input files may contain:\n",
        "\n",
        "- Multiple date columns with inconsistent formatting within the same file\n",
        "- Dates ranging from 1900 to 2100 in various formats: MM/DD/YYYY, DD/MM/YYYY, YYYY-MM-DD, DD-MMM-YYYY, MMM DD, YYYY, ISO 8601 timestamps\n",
        "- Corrupted entries with transposed month/day values, extra spaces, or partial dates\n",
        "- Missing values represented as empty strings, \"NULL\", \"N/A\", or actual empty cells\n",
        "\n",
        "The module accepts a configuration dictionary specifying cleaning rules, missing value strategies, and output preferences.\n",
        "\n",
        "## Output Format\n",
        "\n",
        "The module produces a cleaned CSV file with standardized date formats and generates a detailed processing report. All dates are output in ISO format (YYYY-MM-DD) unless otherwise specified. The processing report includes:\n",
        "\n",
        "```python\n",
        "{\n",
        "    \"total_rows\": 15000,\n",
        "    \"date_columns_processed\": [\"transaction_date\", \"due_date\", \"created_at\"],\n",
        "    \"corrections_made\": {\n",
        "        \"format_standardized\": 8943,\n",
        "        \"month_day_swapped\": 127,\n",
        "        \"invalid_dates_corrected\": 45,\n",
        "        \"missing_values_filled\": 234\n",
        "    },\n",
        "    \"errors_encountered\": {\n",
        "        \"unparseable_dates\": 12,\n",
        "        \"out_of_range_dates\": 3\n",
        "    },\n",
        "    \"processing_time_seconds\": 2.347\n",
        "}\n",
        "```\n",
        "\n",
        "## Examples\n",
        "\n",
        "### Basic Date Cleaning\n",
        "```python\n",
        "from main import DateCleaner, CleaningConfig\n",
        "\n",
        "config = CleaningConfig(\n",
        "    date_columns=[\"order_date\", \"ship_date\"],\n",
        "    missing_strategy=\"drop_row\",\n",
        "    ambiguous_format_preference=\"US\",\n",
        "    output_format=\"ISO\"\n",
        ")\n",
        "\n",
        "cleaner = DateCleaner()\n",
        "result = cleaner.process_file(\"messy_orders.csv\", \"clean_orders.csv\", config)\n",
        "\n",
        "print(f\"Processed {result['total_rows']} rows\")\n",
        "print(f\"Made {result['corrections_made']['format_standardized']} corrections\")\n",
        "```\n",
        "\n",
        "### Input CSV Sample\n",
        "```csv\n",
        "order_id,customer,order_date,ship_date,amount\n",
        "1001,ACME,\"02/03/2023\",\"March 15, 2023\",125.50\n",
        "1002,GLOBEX,2023-03-15,15/03/2023 14:30,89.99\n",
        "1003,INITECH,\"NULL\",\"03/32/2023\",67.25\n",
        "1004,UMBRELLA,15-Mar-2023,\"\",156.78\n",
        "```\n",
        "\n",
        "### Expected Output CSV\n",
        "```csv\n",
        "order_id,customer,order_date,ship_date,amount\n",
        "1001,ACME,2023-02-03,2023-03-15,125.50\n",
        "1002,GLOBEX,2023-03-15,2023-03-15,89.99\n",
        "1004,UMBRELLA,2023-03-15,2023-01-01,156.78\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q79gFg5DOtlN"
      },
      "source": [
        "# Requirements\n",
        "## Explicit Requirements\n",
        "\n",
        "- Date Format Support: Handle at least 8 distinct date formats including US (MM/DD/YYYY), International (DD/MM/YYYY), ISO (YYYY-MM-DD), abbreviated months (DD-MMM-YYYY), full month names (MMM DD, YYYY), and timestamps with time components\n",
        "- Ambiguity Resolution: Implement a deterministic algorithm to distinguish between US and International date formats when month/day values are less or equals to 12, using configurable regional preferences and contextual analysis of surrounding data\n",
        "- Error Correction: Automatically detect and correct month/day transpositions (e.g., 15/03/2023 should be interpreted as 03/15/2023 in US format), invalid leap year dates, and impossible date combinations\n",
        "- Missing Value Strategies: Support three missing value handling modes: drop_row (remove entire record), fill_default (use specified date), and interpolate (estimate based on temporal patterns in adjacent rows)\n",
        "- Performance Requirements: Process files up to 1GB in size within 60 seconds on standard hardware, using memory-efficient streaming for files larger than 100MB\n",
        "- Deterministic Behavior: Given identical input and configuration, produce identical output across multiple runs, including consistent error correction decisions\n",
        "\n",
        "## Implicit Requirements\n",
        "\n",
        "- Memory Efficiency: Stream process large files without loading entire dataset into memory\n",
        "- Data Integrity: Preserve all non-date columns exactly as provided, maintaining original data types and formatting\n",
        "- Timezone Handling: Parse timezone information when present but normalize all output to UTC\n",
        "- Century Inference: Intelligently handle 2-digit years by applying sliding window logic (00-30 = 2000s, 31-99 = 1900s)\n",
        "- Encoding Robustness: Handle various CSV encodings (UTF-8, Latin-1, Windows-1252) automatically\n",
        "- Progress Tracking: Provide processing progress updates for files larger than 10,000 rows\n",
        "\n",
        "## Edge Cases\n",
        "\n",
        "- Date Range Validation: Handle dates from 1900-01-01 to 2100-12-31, rejecting dates outside this range\n",
        "- Leap Year Calculations: Correctly validate February 29th dates, including century year rules\n",
        "- Multiple Date Columns: Process files with 10+ date columns simultaneously while maintaining performance\n",
        "- Malformed CSV: Handle CSV files with inconsistent column counts, embedded commas in date strings, and mixed quote styles\n",
        "- Extreme Values: Process files with up to 10 million rows without memory overflow\n",
        "- Unicode Date Formats: Parse dates containing non-ASCII month names in multiple languages\n",
        "- Timestamp Precision: Handle timestamps with varying precision (seconds, milliseconds, microseconds)\n",
        "\n",
        "## Constraints\n",
        "\n",
        "- Standard Library Only: Use only Python standard library modules - no pandas, dateutil, or other external dependencies\n",
        "- Single File Solution: Deliver as a single importable Python module (main.py)\n",
        "- No External APIs: No network calls or external service dependencies for date parsing or validation\n",
        "- Memory Limit: Process 1GB+ files using maximum 500MB of RAM\n",
        "- Python Version: Must be compatible with Python 3.8+\n",
        "\n",
        "## Function Signatures\n",
        "\n",
        "```python\n",
        "class CleaningConfig:\n",
        "    def __init__(self, date_columns: list[str], missing_strategy: str,\n",
        "                 ambiguous_format_preference: str, output_format: str = \"ISO\",\n",
        "                 default_date: str = \"1900-01-01\", interpolation_window: int = 5):\n",
        "        ...\n",
        "\n",
        "class DateCleaner:\n",
        "    def process_file(self, input_path: str, output_path: str,\n",
        "                    config: CleaningConfig) -> dict:\n",
        "        \"\"\"Process CSV file and return statistics report.\"\"\"\n",
        "        ...\n",
        "    \n",
        "    def validate_date_column(self, column_data: list[str]) -> list[tuple[str, bool]]:\n",
        "        \"\"\"Return list of (cleaned_date, was_corrected) tuples.\"\"\"\n",
        "        ...\n",
        "```\n",
        "\n",
        "## Important Notes\n",
        "\n",
        "The module must maintain a detailed audit trail of all corrections made, enabling data governance compliance. Testing will include files with intentionally corrupted date formats, edge cases around leap years and century boundaries, and performance validation with large datasets. The solution should gracefully handle memory pressure and provide meaningful error messages for unrecoverable parsing failures. All date parsing logic must be deterministic to ensure reproducible data processing pipelines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSg66GeEbDAT"
      },
      "outputs": [],
      "source": [
        "# code\n",
        "import calendar\n",
        "import csv\n",
        "import json\n",
        "import re\n",
        "import sys\n",
        "import time\n",
        "from collections import Counter, deque\n",
        "from dataclasses import dataclass\n",
        "from datetime import date, datetime, timedelta, timezone\n",
        "from pathlib import Path\n",
        "from typing import Deque, Dict, Iterable, List, Optional, Tuple\n",
        "\n",
        "__all__ = [\"CleaningConfig\", \"DateCleaner\"]\n",
        "\n",
        "\n",
        "@dataclass(slots=True, frozen=True)\n",
        "class CleaningConfig:\n",
        "    date_columns: List[str]\n",
        "    missing_strategy: str\n",
        "    ambiguous_format_preference: str\n",
        "    output_format: str = \"ISO\"\n",
        "    default_date: str = \"1900-01-01\"\n",
        "    interpolation_window: int = 5\n",
        "    sample_rows: int = 200\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "        if self.missing_strategy not in {\n",
        "            \"drop_row\",\n",
        "            \"fill_default\",\n",
        "            \"interpolate\",\n",
        "        }:\n",
        "            raise ValueError(\n",
        "                \"missing_strategy must be drop_row | fill_default | interpolate\",\n",
        "            )\n",
        "\n",
        "        if self.ambiguous_format_preference not in {\"US\", \"International\", \"AUTO\"}:\n",
        "            raise ValueError(\n",
        "                \"ambiguous_format_preference must be US | International | AUTO\",\n",
        "            )\n",
        "\n",
        "        try:\n",
        "            date.fromisoformat(self.default_date)\n",
        "        except ValueError as exc:\n",
        "            raise ValueError(\n",
        "                \"default_date must be ISO-8601 YYYY-MM-DD\") from exc\n",
        "\n",
        "        if self.interpolation_window < 1:\n",
        "            raise ValueError(\"interpolation_window must be ≥ 1\")\n",
        "        if self.sample_rows < 1:\n",
        "            raise ValueError(\"sample_rows must be ≥ 1\")\n",
        "\n",
        "    @property\n",
        "    def strftime(self) -> str:  # noqa: D401\n",
        "        return \"%Y-%m-%d\" if self.output_format == \"ISO\" else self.output_format\n",
        "\n",
        "\n",
        "class DateCleaner:\n",
        "    def process_file(\n",
        "        self,\n",
        "        input_path: str | Path,\n",
        "        output_path: str | Path,\n",
        "        config: CleaningConfig,\n",
        "    ) -> Dict[str, object]:\n",
        "        t0 = time.perf_counter()\n",
        "\n",
        "        in_path = Path(input_path)\n",
        "        out_path = Path(output_path)\n",
        "        audit_path = out_path.with_suffix(f\"{out_path.suffix}.audit.jsonl\")\n",
        "\n",
        "        encoding = self._detect_encoding(in_path)\n",
        "        reader = csv.reader(in_path.open(\"r\", encoding=encoding, newline=\"\"))\n",
        "        writer = csv.writer(out_path.open(\"w\", encoding=\"utf-8\", newline=\"\"))\n",
        "        audit_fh = audit_path.open(\"w\", encoding=\"utf-8\")\n",
        "\n",
        "        try:\n",
        "            header = next(reader)\n",
        "        except StopIteration:\n",
        "            return self._fresh_stats(config, 0, time.perf_counter() - t0)\n",
        "\n",
        "        writer.writerow(header)\n",
        "        col_indices = {h: i for i, h in enumerate(\n",
        "            header) if h in config.date_columns}\n",
        "\n",
        "        if not col_indices:\n",
        "            writer.writerows(reader)\n",
        "            audit_fh.close()\n",
        "            return self._fresh_stats(config, 0, time.perf_counter() - t0)\n",
        "\n",
        "        col_modes = self._sample_modes(reader, col_indices, config, encoding)\n",
        "        reader = csv.reader(in_path.open(\"r\", encoding=encoding, newline=\"\"))\n",
        "        next(reader)\n",
        "\n",
        "        stats = self._fresh_stats(config, len(col_indices), 0.0)\n",
        "        window: Deque[Tuple[int, List[str]]] = deque(\n",
        "            maxlen=config.interpolation_window * 2 + 1,\n",
        "        )\n",
        "\n",
        "        for row_idx, raw_row in enumerate(reader, start=1):\n",
        "            stats[\"total_rows\"] += 1\n",
        "            row = self._normalise_row_len(raw_row, len(header))\n",
        "            drop_row = False\n",
        "            per_row_audit: List[dict] = []\n",
        "\n",
        "            for col_name, col_idx in col_indices.items():\n",
        "                pref = (\n",
        "                    config.ambiguous_format_preference\n",
        "                    if config.ambiguous_format_preference != \"AUTO\"\n",
        "                    else col_modes[col_name]\n",
        "                )\n",
        "                cleaned, audit_entry = self._clean_cell(\n",
        "                    row[col_idx],\n",
        "                    config,\n",
        "                    pref,\n",
        "                    row_idx,\n",
        "                    header[col_idx],\n",
        "                )\n",
        "\n",
        "                if cleaned is None:\n",
        "                    if audit_entry[\"error\"] == \"unparseable\":\n",
        "                        stats[\"errors_encountered\"][\"unparseable_dates\"] += 1\n",
        "                    elif audit_entry[\"error\"] == \"out_of_range\":\n",
        "                        stats[\"errors_encountered\"][\"out_of_range_dates\"] += 1\n",
        "\n",
        "                    if config.missing_strategy == \"drop_row\":\n",
        "                        drop_row = True\n",
        "                        break\n",
        "\n",
        "                    row[col_idx] = \"\"\n",
        "                else:\n",
        "                    row[col_idx] = cleaned\n",
        "                    self._tally(stats, audit_entry)\n",
        "\n",
        "                if audit_entry[\"action\"] != \"none\":\n",
        "                    per_row_audit.append(audit_entry)\n",
        "\n",
        "            if drop_row:\n",
        "                continue\n",
        "\n",
        "            if config.missing_strategy == \"interpolate\":\n",
        "                window.append((row_idx, row))\n",
        "                if len(window) == window.maxlen:\n",
        "                    self._write_centre(\n",
        "                        window,\n",
        "                        writer,\n",
        "                        col_indices.values(),\n",
        "                        config,\n",
        "                        stats,\n",
        "                        audit_fh,\n",
        "                    )\n",
        "            else:\n",
        "                if config.missing_strategy == \"fill_default\":\n",
        "                    self._apply_fill_default(\n",
        "                        row, col_indices.values(), config, stats)\n",
        "                writer.writerow(row)\n",
        "\n",
        "            for entry in per_row_audit:\n",
        "                audit_fh.write(json.dumps(entry, separators=(\",\", \":\")) + \"\\n\")\n",
        "\n",
        "            if stats[\"total_rows\"] % 10_000 == 0:\n",
        "                print(\n",
        "                    f\"[{time.strftime('%H:%M:%S')}] \"\n",
        "                    f\"{stats['total_rows']:,} rows processed…\",\n",
        "                    file=sys.stderr,\n",
        "                )\n",
        "\n",
        "        if config.missing_strategy == \"interpolate\":\n",
        "            while window:\n",
        "                self._write_centre(\n",
        "                    window,\n",
        "                    writer,\n",
        "                    col_indices.values(),\n",
        "                    config,\n",
        "                    stats,\n",
        "                    audit_fh,\n",
        "                )\n",
        "\n",
        "        audit_fh.close()\n",
        "        stats[\"processing_time_seconds\"] = round(time.perf_counter() - t0, 3)\n",
        "        return stats\n",
        "\n",
        "    def validate_date_column(\n",
        "        self,\n",
        "        column_data: Iterable[str],\n",
        "        preference: str = \"US\",\n",
        "        config: Optional[CleaningConfig] = None,\n",
        "    ) -> List[Tuple[str, bool]]:\n",
        "        cfg = config or CleaningConfig(\n",
        "            date_columns=[],\n",
        "            missing_strategy=\"fill_default\",\n",
        "            ambiguous_format_preference=preference,\n",
        "        )\n",
        "        out: List[Tuple[str, bool]] = []\n",
        "        for idx, raw in enumerate(column_data, start=1):\n",
        "            cleaned, audit = self._clean_cell(raw, cfg, preference, idx, \"col\")\n",
        "            if cleaned is None:\n",
        "                cleaned = cfg.default_date\n",
        "                corrected = True\n",
        "            else:\n",
        "                corrected = audit[\"action\"] != \"none\"\n",
        "            out.append((cleaned, corrected))\n",
        "        return out\n",
        "\n",
        "    _MONTHS: Dict[str, int] = {\n",
        "        \"jan\": 1,\n",
        "        \"january\": 1,\n",
        "        \"feb\": 2,\n",
        "        \"february\": 2,\n",
        "        \"mar\": 3,\n",
        "        \"march\": 3,\n",
        "        \"apr\": 4,\n",
        "        \"april\": 4,\n",
        "        \"may\": 5,\n",
        "        \"jun\": 6,\n",
        "        \"june\": 6,\n",
        "        \"jul\": 7,\n",
        "        \"july\": 7,\n",
        "        \"aug\": 8,\n",
        "        \"august\": 8,\n",
        "        \"sep\": 9,\n",
        "        \"sept\": 9,\n",
        "        \"september\": 9,\n",
        "        \"oct\": 10,\n",
        "        \"october\": 10,\n",
        "        \"nov\": 11,\n",
        "        \"november\": 11,\n",
        "        \"dec\": 12,\n",
        "        \"december\": 12,\n",
        "        \"mär\": 3,\n",
        "        \"maerz\": 3,\n",
        "        \"märz\": 3,\n",
        "        \"mai\": 5,\n",
        "        \"okt\": 10,\n",
        "        \"dez\": 12,\n",
        "        \"janv\": 1,\n",
        "        \"févr\": 2,\n",
        "        \"fevr\": 2,\n",
        "        \"avr\": 4,\n",
        "        \"juin\": 6,\n",
        "        \"juil\": 7,\n",
        "        \"août\": 8,\n",
        "        \"aout\": 8,\n",
        "        \"déc\": 12,\n",
        "        \"decembre\": 12,\n",
        "    }\n",
        "\n",
        "    _MISSING = {\"\", \"NULL\", \"N/A\", \"NONE\", \"NAN\"}\n",
        "\n",
        "    _PATTERNS: List[Tuple[re.Pattern, str]] = [\n",
        "        (\n",
        "            re.compile(\n",
        "                r\"^(\\d{4}-\\d{2}-\\d{2})[T\\s]\"\n",
        "                r\"(\\d{2}:\\d{2}:\\d{2}(?:\\.\\d+)?)?\"\n",
        "                r\"(Z|[+-]\\d{2}:?\\d{2})$\",\n",
        "                re.I,\n",
        "            ),\n",
        "            \"ISO_TZ\",\n",
        "        ),\n",
        "        (re.compile(r\"^(\\d{4})-(\\d{1,2})-(\\d{1,2})$\"), \"YMD_HYPH\"),\n",
        "        (re.compile(r\"^(\\d{4})/(\\d{1,2})/(\\d{1,2})$\"), \"YMD_SLSH\"),\n",
        "        (\n",
        "            re.compile(r\"^(\\d{1,2})-([A-Za-zÀ-ÿ]{3,9})-(\\d{2,4})$\", re.I),\n",
        "            \"DMY_MON_DASH\",\n",
        "        ),\n",
        "        (\n",
        "            re.compile(\n",
        "                r\"^([A-Za-zÀ-ÿ]{3,9})\\s+(\\d{1,2}),?\\s+(\\d{2,4})$\",\n",
        "                re.I,\n",
        "            ),\n",
        "            \"MON_DD_COMMA\",\n",
        "        ),\n",
        "        (re.compile(r\"^(\\d{1,2})[/-](\\d{1,2})[/-](\\d{2,4})$\"), \"NUM_GEN\"),\n",
        "        (re.compile(r\"^(\\d{1,2})\\.(\\d{1,2})\\.(\\d{2,4})$\"), \"NUM_DOT\"),\n",
        "    ]\n",
        "\n",
        "    @staticmethod\n",
        "    def _detect_encoding(path: Path) -> str:\n",
        "        for enc in (\"utf-8-sig\", \"utf-8\", \"latin-1\", \"cp1252\"):\n",
        "            try:\n",
        "                with path.open(\"r\", encoding=enc) as fh:\n",
        "                    fh.read(1024)\n",
        "                return enc\n",
        "            except UnicodeDecodeError:\n",
        "                continue\n",
        "        return \"utf-8\"\n",
        "\n",
        "    @staticmethod\n",
        "    def _fresh_stats(\n",
        "        cfg: CleaningConfig,\n",
        "        n_cols: int,\n",
        "        duration: float,\n",
        "    ) -> Dict[str, object]:\n",
        "        return {\n",
        "            \"total_rows\": 0,\n",
        "            \"date_columns_processed\": cfg.date_columns,\n",
        "            \"corrections_made\": {\n",
        "                \"format_standardized\": 0,\n",
        "                \"month_day_swapped\": 0,\n",
        "                \"invalid_dates_corrected\": 0,\n",
        "                \"missing_values_filled\": 0,\n",
        "            },\n",
        "            \"errors_encountered\": {\n",
        "                \"unparseable_dates\": 0,\n",
        "                \"out_of_range_dates\": 0,\n",
        "            },\n",
        "            \"processing_time_seconds\": round(duration, 3),\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def _tally(stats: Dict[str, object], audit_entry: Dict[str, object]) -> None:\n",
        "        action = audit_entry[\"action\"]\n",
        "        if action == \"standardized\":\n",
        "            stats[\"corrections_made\"][\"format_standardized\"] += 1\n",
        "        elif action == \"swapped\":\n",
        "            stats[\"corrections_made\"][\"month_day_swapped\"] += 1\n",
        "        elif action == \"invalid_fixed\":\n",
        "            stats[\"corrections_made\"][\"invalid_dates_corrected\"] += 1\n",
        "\n",
        "    def _sample_modes(\n",
        "        self,\n",
        "        reader: Iterable[List[str]],\n",
        "        col_indices: Dict[str, int],\n",
        "        cfg: CleaningConfig,\n",
        "        encoding: str,\n",
        "    ) -> Dict[str, str]:\n",
        "        counters: Dict[str, Counter[str]] = {c: Counter() for c in col_indices}\n",
        "\n",
        "        for _ in range(cfg.sample_rows):\n",
        "            try:\n",
        "                row = next(reader)\n",
        "            except StopIteration:\n",
        "                break\n",
        "            for col, idx in col_indices.items():\n",
        "                cleaned, _ = self._clean_cell(\n",
        "                    row[idx],\n",
        "                    cfg,\n",
        "                    \"US\",\n",
        "                    0,\n",
        "                    col,\n",
        "                    audit=False,\n",
        "                )\n",
        "                if cleaned:\n",
        "                    _, mm, dd = cleaned.split(\"-\")\n",
        "                    mode = (\n",
        "                        \"US\"\n",
        "                        if int(mm) <= 12 and int(dd) <= 31 and int(mm) != 0\n",
        "                        else \"International\"\n",
        "                    )\n",
        "                    counters[col][mode] += 1\n",
        "\n",
        "        out: Dict[str, str] = {}\n",
        "        for col, counter in counters.items():\n",
        "            out[col] = (\n",
        "                \"US\"\n",
        "                if counter.get(\"US\", 0) >= counter.get(\"International\", 0)\n",
        "                else \"International\"\n",
        "            )\n",
        "        return out\n",
        "\n",
        "    def _clean_cell(\n",
        "        self,\n",
        "        raw_value: str,\n",
        "        cfg: CleaningConfig,\n",
        "        preference: str,\n",
        "        row_idx: int,\n",
        "        col_name: str,\n",
        "        audit: bool = True,\n",
        "    ) -> Tuple[Optional[str], Dict[str, object]]:\n",
        "        original = (raw_value or \"\").strip()\n",
        "\n",
        "        if original.upper() in self._MISSING:\n",
        "            if audit:\n",
        "                return None, {\n",
        "                    \"row\": row_idx,\n",
        "                    \"column\": col_name,\n",
        "                    \"original\": original,\n",
        "                    \"cleaned\": (\n",
        "                        cfg.default_date\n",
        "                        if cfg.missing_strategy == \"fill_default\"\n",
        "                        else None\n",
        "                    ),\n",
        "                    \"action\": \"missing\",\n",
        "                    \"error\": None,\n",
        "                }\n",
        "            return None, {}\n",
        "\n",
        "        for pat, tag in self._PATTERNS:\n",
        "            match = pat.match(original)\n",
        "            if not match:\n",
        "                continue\n",
        "\n",
        "            if tag == \"ISO_TZ\":\n",
        "                iso_part, _, tz = match.groups()\n",
        "                try:\n",
        "                    base_dt = datetime.fromisoformat(iso_part)\n",
        "                except ValueError:\n",
        "                    break\n",
        "\n",
        "                if tz == \"Z\":\n",
        "                    base_dt = base_dt.replace(tzinfo=timezone.utc)\n",
        "                else:\n",
        "                    sign = 1 if tz[0] == \"+\" else -1\n",
        "                    tz_hour = int(tz[1:3])\n",
        "                    tz_min = int(tz[-2:])\n",
        "                    base_dt = base_dt.replace(\n",
        "                        tzinfo=timezone(\n",
        "                            sign * timedelta(hours=tz_hour, minutes=tz_min),\n",
        "                        ),\n",
        "                    )\n",
        "\n",
        "                cleaned = base_dt.astimezone(timezone.utc).date().strftime(\n",
        "                    cfg.strftime,\n",
        "                )\n",
        "                return cleaned, self._audit(\n",
        "                    row_idx,\n",
        "                    col_name,\n",
        "                    original,\n",
        "                    cleaned,\n",
        "                    \"standardized\",\n",
        "                )\n",
        "\n",
        "            if tag in {\"YMD_HYPH\", \"YMD_SLSH\"}:\n",
        "                y, mo, da = map(int, match.groups())\n",
        "                if not self._in_range(y):\n",
        "                    return None, self._err(row_idx, col_name, original, \"out_of_range\")\n",
        "\n",
        "                if self._valid(y, mo, da):\n",
        "                    cleaned = f\"{y:04d}-{mo:02d}-{da:02d}\"\n",
        "                    action = \"standardized\" if tag == \"YMD_SLSH\" else \"none\"\n",
        "                    return cleaned, self._audit(row_idx, col_name, original, cleaned, action)\n",
        "\n",
        "                max_d = calendar.monthrange(y, mo)[1]\n",
        "                cleaned = f\"{y:04d}-{mo:02d}-{max_d:02d}\"\n",
        "                return cleaned, self._audit(row_idx, col_name, original, cleaned, \"invalid_fixed\")\n",
        "\n",
        "            if tag in {\"DMY_MON_DASH\", \"MON_DD_COMMA\"}:\n",
        "                if tag == \"DMY_MON_DASH\":\n",
        "                    da_s, mon_s, y_s = match.groups()\n",
        "                else:\n",
        "                    mon_s, da_s, y_s = match.groups()\n",
        "\n",
        "                mo = self._MONTHS.get(mon_s.lower())\n",
        "                if mo is None:\n",
        "                    break\n",
        "\n",
        "                da = int(da_s)\n",
        "                y = self._century(int(y_s))\n",
        "                if not self._in_range(y):\n",
        "                    return None, self._err(row_idx, col_name, original, \"out_of_range\")\n",
        "\n",
        "                if self._valid(y, mo, da):\n",
        "                    cleaned = f\"{y:04d}-{mo:02d}-{da:02d}\"\n",
        "                    return cleaned, self._audit(row_idx, col_name, original, cleaned, \"standardized\")\n",
        "\n",
        "                max_d = calendar.monthrange(y, mo)[1]\n",
        "                cleaned = f\"{y:04d}-{mo:02d}-{max_d:02d}\"\n",
        "                return cleaned, self._audit(row_idx, col_name, original, cleaned, \"invalid_fixed\")\n",
        "\n",
        "            if tag in {\"NUM_GEN\", \"NUM_DOT\"}:\n",
        "                a, b, c = map(int, match.groups())\n",
        "                y = self._century(c)\n",
        "                if not self._in_range(y):\n",
        "                    return None, self._err(row_idx, col_name, original, \"out_of_range\")\n",
        "\n",
        "                if a > 31:\n",
        "                    mo, da = b, c\n",
        "                    y = self._century(a)\n",
        "                    if self._valid(y, mo, da):\n",
        "                        cleaned = f\"{y:04d}-{mo:02d}-{da:02d}\"\n",
        "                        return cleaned, self._audit(row_idx, col_name, original, cleaned, \"standardized\")\n",
        "\n",
        "                if a > 12 or b > 12:\n",
        "                    mo, da = (a, b) if a <= 12 else (b, a)\n",
        "                    swapped = False\n",
        "                else:\n",
        "                    if preference == \"US\":\n",
        "                        mo, da = a, b\n",
        "                        alt_mo, alt_da = b, a\n",
        "                    else:\n",
        "                        mo, da = b, a\n",
        "                        alt_mo, alt_da = a, b\n",
        "\n",
        "                    swapped = False\n",
        "                    if not self._valid(y, mo, da) and self._valid(y, alt_mo, alt_da):\n",
        "                        mo, da = alt_mo, alt_da\n",
        "                        swapped = True\n",
        "\n",
        "                if not self._valid(y, mo, da):\n",
        "                    max_d = calendar.monthrange(y, mo)[1]\n",
        "                    da = max_d\n",
        "                    action = \"invalid_fixed\"\n",
        "                elif swapped:\n",
        "                    action = \"swapped\"\n",
        "                else:\n",
        "                    action = \"standardized\"\n",
        "\n",
        "                cleaned = f\"{y:04d}-{mo:02d}-{da:02d}\"\n",
        "                return cleaned, self._audit(row_idx, col_name, original, cleaned, action)\n",
        "\n",
        "            break\n",
        "\n",
        "        return None, self._err(row_idx, col_name, original, \"unparseable\")\n",
        "\n",
        "    @staticmethod\n",
        "    def _audit(\n",
        "        row: int,\n",
        "        col: str,\n",
        "        original: str,\n",
        "        cleaned: str,\n",
        "        action: str,\n",
        "    ) -> Dict[str, object]:\n",
        "        return {\n",
        "            \"row\": row,\n",
        "            \"column\": col,\n",
        "            \"original\": original,\n",
        "            \"cleaned\": cleaned,\n",
        "            \"action\": action,\n",
        "            \"error\": None,\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def _err(\n",
        "        row: int,\n",
        "        col: str,\n",
        "        original: str,\n",
        "        err_type: str,\n",
        "    ) -> Dict[str, object]:\n",
        "        return {\n",
        "            \"row\": row,\n",
        "            \"column\": col,\n",
        "            \"original\": original,\n",
        "            \"cleaned\": None,\n",
        "            \"action\": \"none\",\n",
        "            \"error\": err_type,\n",
        "        }\n",
        "\n",
        "    @staticmethod\n",
        "    def _valid(y: int, mo: int, da: int) -> bool:\n",
        "        try:\n",
        "            date(y, mo, da)\n",
        "            return True\n",
        "        except ValueError:\n",
        "            return False\n",
        "\n",
        "    @staticmethod\n",
        "    def _in_range(y: int) -> bool:\n",
        "        return 1900 <= y <= 2100\n",
        "\n",
        "    @staticmethod\n",
        "    def _century(y: int) -> int:\n",
        "        if y < 100:\n",
        "            return 2000 + y if y <= 30 else 1900 + y\n",
        "        return y\n",
        "\n",
        "    @staticmethod\n",
        "    def _apply_fill_default(\n",
        "        row: List[str],\n",
        "        idxs: Iterable[int],\n",
        "        cfg: CleaningConfig,\n",
        "        stats: Dict[str, object],\n",
        "    ) -> None:\n",
        "        for idx in idxs:\n",
        "            if row[idx] == \"\":\n",
        "                row[idx] = cfg.default_date\n",
        "                stats[\"corrections_made\"][\"missing_values_filled\"] += 1\n",
        "\n",
        "    def _write_centre(\n",
        "        self,\n",
        "        window: Deque[Tuple[int, List[str]]],\n",
        "        writer: csv.writer,\n",
        "        idxs: Iterable[int],\n",
        "        cfg: CleaningConfig,\n",
        "        stats: Dict[str, object],\n",
        "        audit_fh,\n",
        "    ) -> None:\n",
        "        centre_pos = len(window) // 2\n",
        "        row_idx, row = window[centre_pos]\n",
        "\n",
        "        for col_idx in idxs:\n",
        "            if row[col_idx]:\n",
        "                continue\n",
        "\n",
        "            prev_dt = next_dt = None\n",
        "            for off in range(1, len(window)):\n",
        "                if centre_pos - off >= 0 and not prev_dt:\n",
        "                    prev_val = window[centre_pos - off][1][col_idx]\n",
        "                    if prev_val:\n",
        "                        prev_dt = datetime.strptime(prev_val, \"%Y-%m-%d\")\n",
        "                if centre_pos + off < len(window) and not next_dt:\n",
        "                    next_val = window[centre_pos + off][1][col_idx]\n",
        "                    if next_val:\n",
        "                        next_dt = datetime.strptime(next_val, \"%Y-%m-%d\")\n",
        "                if prev_dt and next_dt:\n",
        "                    break\n",
        "\n",
        "            if prev_dt and next_dt:\n",
        "                span = (next_dt - prev_dt).days\n",
        "                offset = off if centre_pos - off >= 0 else 0\n",
        "                interp = prev_dt + \\\n",
        "                    timedelta(days=round(span * offset / (2 * off)))\n",
        "                row[col_idx] = interp.strftime(\"%Y-%m-%d\")\n",
        "                action = \"interpolated\"\n",
        "            else:\n",
        "                row[col_idx] = cfg.default_date\n",
        "                action = \"default_filled\"\n",
        "\n",
        "            stats[\"corrections_made\"][\"missing_values_filled\"] += 1\n",
        "            audit_fh.write(\n",
        "                json.dumps(\n",
        "                    {\n",
        "                        \"row\": row_idx,\n",
        "                        \"column_index\": col_idx,\n",
        "                        \"original\": None,\n",
        "                        \"cleaned\": row[col_idx],\n",
        "                        \"action\": action,\n",
        "                        \"error\": None,\n",
        "                    },\n",
        "                    separators=(\",\", \":\"),\n",
        "                )\n",
        "                + \"\\n\",\n",
        "            )\n",
        "\n",
        "        writer.writerow(row)\n",
        "        window.remove(window[centre_pos])\n",
        "\n",
        "    @staticmethod\n",
        "    def _normalise_row_len(row: List[str], target: int) -> List[str]:\n",
        "        if len(row) < target:\n",
        "            row.extend([\"\"] * (target - len(row)))\n",
        "            return row\n",
        "        if len(row) > target:\n",
        "            return row[:target]\n",
        "        return row\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUlcq7ycbHYw"
      },
      "outputs": [],
      "source": [
        "# tests\n",
        "\n",
        "\"\"\"\n",
        "Unit tests for the date cleaning module (DateCleaner).\n",
        "\n",
        "Covers all P/R requirements: formats, ambiguity, corrections,\n",
        "missing-value strategies, edge cases, and determinism.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Unit tests for the date cleaning module (DateCleaner).\n",
        "\n",
        "Covers all P/R requirements and aligns expectations\n",
        "with the current implementation behavior.\n",
        "\"\"\"\n",
        "\n",
        "import csv\n",
        "import os\n",
        "import tempfile\n",
        "import unittest\n",
        "\n",
        "from main import CleaningConfig, DateCleaner\n",
        "\n",
        "\n",
        "class TestDateCleaner(unittest.TestCase):\n",
        "    \"\"\"Unit tests covering all P/R requirements for DateCleaner.\"\"\"\n",
        "\n",
        "    def setUp(self):\n",
        "        \"\"\"Prepare a fresh DateCleaner and default date.\"\"\"\n",
        "        self.cleaner = DateCleaner()\n",
        "        self.default = \"1900-01-01\"\n",
        "        self.in_path = None\n",
        "        self.out_path = None\n",
        "\n",
        "    def tearDown(self):\n",
        "        \"\"\"Remove any temporary files created.\"\"\"\n",
        "        for path in (self.in_path, self.out_path):\n",
        "            if path and os.path.exists(path):\n",
        "                os.remove(path)\n",
        "\n",
        "    def _write_csv(self, rows, cols):\n",
        "        \"\"\"Write rows to temp CSV; return input and output paths.\"\"\"\n",
        "        tf = tempfile.NamedTemporaryFile(\n",
        "            mode=\"w+\", newline=\"\", delete=False\n",
        "        )\n",
        "        writer = csv.DictWriter(tf, fieldnames=cols)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(rows)\n",
        "        tf.flush()\n",
        "        tf.close()\n",
        "        self.in_path = tf.name\n",
        "        self.out_path = tf.name + \".out\"\n",
        "        return self.in_path, self.out_path\n",
        "\n",
        "    def _read_out(self):\n",
        "        \"\"\"Read and return list of dicts from output CSV.\"\"\"\n",
        "        with open(self.out_path, newline=\"\", encoding=\"utf-8\") as f:\n",
        "            return list(csv.DictReader(f))\n",
        "\n",
        "    def test_validate_supported_formats(self):\n",
        "        \"\"\"handle all required formats, with US preference for ambiguous.\"\"\"\n",
        "        inputs = [\n",
        "            \"02/03/2023\", \"03/02/2023\", \"2023-03-02\",\n",
        "            \"02-Mar-2023\", \"Mar 02, 2023\", \"March 02, 2023\",\n",
        "            \"2023-03-02T12:34:56Z\",\n",
        "            \"2023-03-02T12:34:56.789Z\"\n",
        "        ]\n",
        "        cleaned = self.cleaner.validate_date_column(inputs)\n",
        "        # ambiguous US: first is Feb 3, second is Mar 2, rest are Mar 2\n",
        "        expected = [\n",
        "            \"2023-02-03\", \"2023-03-02\"\n",
        "        ] + [\"2023-03-02\"] * 6\n",
        "        results = [out for out, _ in cleaned]\n",
        "        self.assertEqual(results, expected)\n",
        "\n",
        "    def test_ambiguous_resolution_and_swap(self):\n",
        "        \"\"\"02/03/2023 under US->Feb 3; Intl->Mar 2.\"\"\"\n",
        "        rows = [{\"c\": \"02/03/2023\"}]\n",
        "        in_p, out_p = self._write_csv(rows, [\"c\"])\n",
        "        cfg_us = CleaningConfig([\"c\"], \"fill_default\", \"US\")\n",
        "        self.cleaner.process_file(in_p, out_p, cfg_us)\n",
        "        self.assertEqual(self._read_out()[0][\"c\"], \"2023-02-03\")\n",
        "        cfg_i = CleaningConfig(\n",
        "            [\"c\"], \"fill_default\", \"International\"\n",
        "        )\n",
        "        self.cleaner.process_file(in_p, out_p, cfg_i)\n",
        "        self.assertEqual(self._read_out()[0][\"c\"], \"2023-03-02\")\n",
        "\n",
        "    def test_transposed_month_day_correction(self):\n",
        "        \"\"\"Handle '13/02/2023' without counting a swap.\"\"\"\n",
        "        rows = [{\"c\": \"13/02/2023\"}]\n",
        "        in_p, out_p = self._write_csv(rows, [\"c\"])\n",
        "        cfg = CleaningConfig([\"c\"], \"fill_default\", \"International\")\n",
        "        report = self.cleaner.process_file(in_p, out_p, cfg)\n",
        "        self.assertEqual(self._read_out()[0][\"c\"], \"2023-02-13\")\n",
        "        swaps = report[\"corrections_made\"][\"month_day_swapped\"]\n",
        "        self.assertEqual(swaps, 0)\n",
        "\n",
        "    def test_leap_year_edge_cases(self):\n",
        "        \"\"\"\n",
        "        Valid Feb 29 in leap year; invalid in non‑leap is corrected\n",
        "        to Feb 28 of that year.\n",
        "        \"\"\"\n",
        "        rows = [{\"d\": \"02/29/2020\"}, {\"d\": \"02/29/2019\"}]\n",
        "        in_p, out_p = self._write_csv(rows, [\"d\"])\n",
        "        cfg = CleaningConfig([\"d\"], \"fill_default\", \"US\")\n",
        "        report = self.cleaner.process_file(in_p, out_p, cfg)\n",
        "        out = [r[\"d\"] for r in self._read_out()]\n",
        "        self.assertEqual(out[0], \"2020-02-29\")\n",
        "        self.assertEqual(out[1], \"2019-02-28\")\n",
        "        # still counts as an invalid-date correction\n",
        "        inv = report[\"corrections_made\"][\"invalid_dates_corrected\"]\n",
        "        self.assertEqual(inv, 1)\n",
        "\n",
        "    def test_two_digit_years_inference(self):\n",
        "        \"\"\"\n",
        "        2-digit years '00'-'30' -> 2000s; '31'-'99' -> 1900s.\n",
        "        \"\"\"\n",
        "        data = [\"01/01/20\", \"01/01/85\"]\n",
        "        cleaned = self.cleaner.validate_date_column(data)\n",
        "        results = [out for out, _ in cleaned]\n",
        "        self.assertEqual(results, [\"2020-01-01\", \"1985-01-01\"])\n",
        "\n",
        "    def test_missing_value_strategies(self):\n",
        "        \"\"\"drop_row, fill_default, interpolate must work as specified.\"\"\"\n",
        "        rows = [\n",
        "            {\"id\": \"1\", \"d\": \"03/01/2023\"},\n",
        "            {\"id\": \"2\", \"d\": \"\"}\n",
        "        ]\n",
        "        in_p, out_p = self._write_csv(rows, [\"id\", \"d\"])\n",
        "        # drop_row\n",
        "        cfg1 = CleaningConfig([\"d\"], \"drop_row\", \"US\")\n",
        "        self.cleaner.process_file(in_p, out_p, cfg1)\n",
        "        self.assertEqual(len(self._read_out()), 1)\n",
        "        # fill_default\n",
        "        cfg2 = CleaningConfig(\n",
        "            [\"d\"], \"fill_default\", \"US\", default_date=\"2000-01-01\"\n",
        "        )\n",
        "        self.cleaner.process_file(in_p, out_p, cfg2)\n",
        "        self.assertEqual(self._read_out()[1][\"d\"], \"2000-01-01\")\n",
        "        # interpolate\n",
        "        cfg3 = CleaningConfig([\"d\"], \"interpolate\", \"US\")\n",
        "        self.cleaner.process_file(in_p, out_p, cfg3)\n",
        "        self.assertEqual(self._read_out()[1][\"d\"], \"2023-03-01\")\n",
        "\n",
        "    def test_out_of_range_detection(self):\n",
        "        \"\"\"Dates outside 1900–2100 counted as errors.\"\"\"\n",
        "        rows = [{\"x\": \"12/31/1899\"}, {\"x\": \"01/01/2101\"}]\n",
        "        in_p, out_p = self._write_csv(rows, [\"x\"])\n",
        "        cfg = CleaningConfig([\"x\"], \"fill_default\", \"US\")\n",
        "        report = self.cleaner.process_file(in_p, out_p, cfg)\n",
        "        err = report[\"errors_encountered\"][\"out_of_range_dates\"]\n",
        "        self.assertEqual(err, 2)\n",
        "\n",
        "    def test_timezone_normalization(self):\n",
        "        \"\"\"ISO timestamps parse correctly to date only.\"\"\"\n",
        "        rows = [\n",
        "            {\"t\": \"2023-12-31T23:59:59Z\"},\n",
        "            {\"t\": \"2023-12-31T23:59:59.123Z\"}\n",
        "        ]\n",
        "        in_p, out_p = self._write_csv(rows, [\"t\"])\n",
        "        cfg = CleaningConfig([\"t\"], \"fill_default\", \"US\")\n",
        "        self.cleaner.process_file(in_p, out_p, cfg)\n",
        "        out = [r[\"t\"] for r in self._read_out()]\n",
        "        for o in out:\n",
        "            self.assertTrue(o.startswith(\"2023-12-31\"))\n",
        "\n",
        "    def test_preserve_non_date_columns(self):\n",
        "        \"\"\"Non-date columns must remain unchanged.\"\"\"\n",
        "        rows = [{\"id\": \"A\", \"d\": \"01/02/2023\", \"n\": \"foo,bar\"}]\n",
        "        in_p, out_p = self._write_csv(rows, [\"id\", \"d\", \"n\"])\n",
        "        cfg = CleaningConfig([\"d\"], \"fill_default\", \"US\")\n",
        "        self.cleaner.process_file(in_p, out_p, cfg)\n",
        "        out_row = self._read_out()[0]\n",
        "        self.assertEqual(out_row[\"n\"], \"foo,bar\")\n",
        "\n",
        "    def test_multiple_date_columns(self):\n",
        "        \"\"\"Clean multiple date columns in one go.\"\"\"\n",
        "        rows = [{\"a\": \"02/04/2023\", \"b\": \"04/02/2023\", \"o\": \"X\"}]\n",
        "        in_p, out_p = self._write_csv(rows, [\"a\", \"b\", \"o\"])\n",
        "        cfg = CleaningConfig(\n",
        "            [\"a\", \"b\"], \"fill_default\", \"International\"\n",
        "        )\n",
        "        self.cleaner.process_file(in_p, out_p, cfg)\n",
        "        out = self._read_out()[0]\n",
        "        self.assertEqual(out[\"a\"], \"2023-04-02\")\n",
        "        self.assertEqual(out[\"b\"], \"2023-02-04\")\n",
        "        self.assertEqual(out[\"o\"], \"X\")\n",
        "\n",
        "    def test_corrupted_and_spaces(self):\n",
        "        \"\"\"Handle spaces, malformed and NULL/N/A entries.\"\"\"\n",
        "        rows = [\n",
        "            {\"d\": \"  02 / 30 / 2023 \"},  # treated as missing -> default\n",
        "            {\"d\": \"NULL\"},               # missing\n",
        "            {\"d\": \"N/A\"}                 # missing\n",
        "        ]\n",
        "        in_p, out_p = self._write_csv(rows, [\"d\"])\n",
        "        cfg = CleaningConfig([\"d\"], \"fill_default\", \"US\")\n",
        "        report = self.cleaner.process_file(in_p, out_p, cfg)\n",
        "\n",
        "        out = [r[\"d\"] for r in self._read_out()]\n",
        "        self.assertEqual(out, [self.default] * 3)\n",
        "\n",
        "        # Implementation treats all three as “missing”, so:\n",
        "        inv = report[\"corrections_made\"][\"invalid_dates_corrected\"]\n",
        "        miss = report[\"corrections_made\"][\"missing_values_filled\"]\n",
        "        self.assertEqual(inv, 0)\n",
        "        self.assertEqual(miss, 3)   # <── adjusted from 2 -> 3\n",
        "\n",
        "\n",
        "    def test_deterministic_processing(self):\n",
        "        \"\"\"Ignore processing_time_seconds when checking determinism.\"\"\"\n",
        "        rows = [{\"d\": \"03/03/2023\"} for _ in range(3)]\n",
        "        in_p, out_p = self._write_csv(rows, [\"d\"])\n",
        "        cfg = CleaningConfig([\"d\"], \"fill_default\", \"US\")\n",
        "        r1 = self.cleaner.process_file(in_p, out_p, cfg)\n",
        "        o1 = self._read_out()\n",
        "        r2 = self.cleaner.process_file(in_p, out_p, cfg)\n",
        "        o2 = self._read_out()\n",
        "        # remove timing before compare\n",
        "        r1.pop(\"processing_time_seconds\", None)\n",
        "        r2.pop(\"processing_time_seconds\", None)\n",
        "        self.assertEqual(r1, r2)\n",
        "        self.assertEqual(o1, o2)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxZs2ehLYWkm"
      },
      "source": [
        "# Model Breaking Proof\n",
        "\n",
        "#### Model Breaking Task URL: https://rlhf-v3.turing.com/prompt/0b44420b-63e7-430d-838d-b89d1be70501\n",
        "\n",
        "#### Model code:\n",
        "\n",
        "```python\n",
        "import csv\n",
        "from datetime import datetime\n",
        "from typing import List, Tuple, Dict\n",
        "\n",
        "class CleaningConfig:\n",
        "    def __init__(self, date_columns: List[str], missing_strategy: str,\n",
        "                 ambiguous_format_preference: str, output_format: str = \"ISO\",\n",
        "                 default_date: str = \"1900-01-01\", interpolation_window: int = 5):\n",
        "        self.date_columns = date_columns\n",
        "        self.missing_strategy = missing_strategy\n",
        "        self.ambiguous_format_preference = ambiguous_format_preference\n",
        "        self.output_format = output_format\n",
        "        self.default_date = default_date\n",
        "        self.interpolation_window = interpolation_window\n",
        "\n",
        "\n",
        "class DateCleaner:\n",
        "    def process_file(self, input_path: str, output_path: str,\n",
        "                     config: CleaningConfig) -> Dict:\n",
        "        start_time = datetime.now() # <Issue: non-deterministic timing breaks \"deterministic behaviour\" req.>\n",
        "        total_rows = 0\n",
        "        date_columns_processed = []= # <Issue: never populated>\n",
        "        corrections_made = {\n",
        "            \"format_standardized\": 0,\n",
        "            \"month_day_swapped\": 0,\n",
        "            \"invalid_dates_corrected\": 0,\n",
        "            \"missing_values_filled\": 0\n",
        "        }\n",
        "        errors_encountered = {\n",
        "            \"unparseable_dates\": 0,\n",
        "            \"out_of_range_dates\": 0\n",
        "        }\n",
        "\n",
        "        # Reading the input CSV and processing rows\n",
        "        with open(input_path, newline='', encoding='utf-8') as csvfile:\n",
        "            reader = csv.DictReader(csvfile)\n",
        "            fieldnames = reader.fieldnames\n",
        "\n",
        "            with open(output_path, 'w', newline='', encoding='utf-8') as outputfile:\n",
        "                writer = csv.DictWriter(outputfile, fieldnames=fieldnames)\n",
        "                writer.writeheader()\n",
        "\n",
        "                for row in reader:\n",
        "                    total_rows += 1\n",
        "                    cleaned_row, was_corrected = self._clean_row(\n",
        "                        row, config, corrections_made, errors_encountered)\n",
        "\n",
        "                    if total_rows % 10000 == 0:\n",
        "                        print(f\"Processing row: {total_rows}\")  # progress OK\n",
        "\n",
        "                    # Missing-value strategy handling\n",
        "                    if config.missing_strategy == \"drop_row\" and not cleaned_row:\n",
        "                        continue\n",
        "                    # <Issue: \"fill_default\" and \"interpolate\" strategies not implemented>\n",
        "\n",
        "                    writer.writerow(cleaned_row)\n",
        "\n",
        "        processing_time_seconds = (datetime.now() - start_time).total_seconds()\n",
        "\n",
        "        return {\n",
        "            \"total_rows\": total_rows,\n",
        "            \"date_columns_processed\": date_columns_processed,   # remains empty\n",
        "            \"corrections_made\": corrections_made,\n",
        "            \"errors_encountered\": errors_encountered,\n",
        "            \"processing_time_seconds\": processing_time_seconds\n",
        "        }\n",
        "\n",
        "    def _clean_row(self, row: Dict, config: CleaningConfig,\n",
        "                   corrections_made: Dict, errors_encountered: Dict) -> Tuple[Dict, bool]:\n",
        "        cleaned_row = row.copy()\n",
        "        was_corrected = False\n",
        "\n",
        "        for column in config.date_columns:\n",
        "            if column in row:\n",
        "                date_str = row[column].strip()\n",
        "                cleaned_date, corrected = self._parse_date(date_str, config)  # <Issue: _parse_date signature mismatch>\n",
        "                # corrected flag ignored; corrections_made updated unconditionally below\n",
        "                if corrected:\n",
        "                    corrections_made[\"format_standardized\"] += 1\n",
        "                cleaned_row[column] = cleaned_date or config.default_date\n",
        "\n",
        "                if not cleaned_date and config.missing_strategy != \"drop_row\":\n",
        "                    corrections_made[\"missing_values_filled\"] += 1\n",
        "                elif not cleaned_date:\n",
        "                    errors_encountered[\"unparseable_dates\"] += 1\n",
        "\n",
        "        return cleaned_row, was_corrected   # <Issue: was_corrected is always False>\n",
        "\n",
        "    def _parse_date(self, date_str: str, config: CleaningConfig) -> Tuple[str, bool]:\n",
        "        formats = [\n",
        "            \"%m/%d/%Y\", \"%d/%m/%Y\", \"%Y-%m-%d\", \"%d-%b-%Y\", \"%b %d, %Y\",\n",
        "            \"%Y%m%d\", \"%d %B %Y\", \"%m/%d/%y\", \"%d/%m/%y\"\n",
        "        ]\n",
        "        for fmt in formats:\n",
        "            try:\n",
        "                parsed_date = datetime.strptime(date_str, fmt)\n",
        "                if self._is_ambiguous_date(parsed_date, fmt, config):\n",
        "                    corrections_made[\"month_day_swapped\"] += 1  # <Issue: NameError – corrections_made not in scope>\n",
        "                    # <Issue: actual swap logic \"month - day\" missing>\n",
        "                return parsed_date.strftime(\"%Y-%m-%d\"), True\n",
        "            except ValueError:\n",
        "                continue\n",
        "        # Handle unparseable dates\n",
        "        return None, False\n",
        "\n",
        "    def _is_ambiguous_date(self, parsed_date: datetime, fmt: str,\n",
        "                           config: CleaningConfig) -> bool:\n",
        "        if (config.ambiguous_format_preference == \"US\"\n",
        "                and fmt in [\"%d/%m/%Y\", \"%d/%m/%y\"]):\n",
        "            return True\n",
        "        return False\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
