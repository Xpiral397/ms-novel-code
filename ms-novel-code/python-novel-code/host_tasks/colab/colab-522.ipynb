{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKe-t1pIOo2f"
      },
      "source": [
        "# Metadata\n",
        "\n",
        "**L1 Taxonomy** - Security\n",
        "\n",
        "**L2 Taxonomy** - Authorization\n",
        "\n",
        "**Subtopic** - Multi-tenant data partitioning: ensure queries always include tenant id from user context (to separate customer data)\n",
        "\n",
        "**Use Case** - Develop a Python module that simulates multi-tenant data partitioning. This module should generate mock data for multiple tenants and ensure that each query to retrieve data includes the tenant id, effectively separating the data for each customer. Utilize Python's built-in data structures and the 'random' module from the Python standard library for data generation and simulation.\n",
        "\n",
        "**Programming Language** - Python\n",
        "\n",
        "**Target Model** - o1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS0xHSaZoEJO"
      },
      "source": [
        "# Setup\n",
        "\n",
        "```requirements.txt\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YToTLlRqOqmj"
      },
      "source": [
        "# Prompt\n",
        "## Problem Description\n",
        "\n",
        "You are tasked with implementing a secure multi-tenant data layer for a Python SaaS application. This module must enforce strict data partitioning between tenants by ensuring that all queries include proper tenant identification from a user context object. The system should generate realistic mock data for multiple tenants and prevent any possibility of cross-tenant data access, simulating the critical security boundaries required in real multi-tenant architectures. The deliverable is a complete, importable Python module that other application components can use to safely query tenant-specific data.\n",
        "\n",
        "## Input Format\n",
        "\n",
        "All interactions occur through Python function calls with no external I/O. The module exposes two primary functions:\n",
        "\n",
        "- bootstrap(tenants: list[str], rng_seed: int | None = None) -> None - Initializes the data store with mock data for the specified tenants\n",
        "- query(user_ctx: dict, *, value_min: int | None = None, value_max: int | None = None) -> list[dict] - Retrieves tenant-specific records with optional numeric range filtering\n",
        "\n",
        "The user context is a dictionary containing at minimum a tenant_id key. Query parameters use keyword-only arguments for range filtering on a numeric value field.\n",
        "\n",
        "## Output Format\n",
        "\n",
        "Successful queries return lists of dictionaries, each representing a data record:\n",
        "\n",
        "```python\n",
        "{\"tenant_id\": \"acme\", \"obj_id\": \"a1b2c3d4e5f6...\", \"value\": 342}\n",
        "```\n",
        "\n",
        "All validation failures are communicated by raising appropriate built-in Python exceptions (KeyError, ValueError, etc.). The module produces no console output or logging.\n",
        "\n",
        "## Examples\n",
        "\n",
        "### Basic Multi-Tenant Usage\n",
        "```python\n",
        "from main import bootstrap, query\n",
        "\n",
        "# Initialize with multiple tenants\n",
        "bootstrap([\"acme\", \"globex\", \"initech\"], rng_seed=42)\n",
        "\n",
        "# Each user context represents a different tenant\n",
        "alice_ctx = {\"tenant_id\": \"acme\"}\n",
        "bob_ctx = {\"tenant_id\": \"globex\"}\n",
        "\n",
        "# Queries are automatically partitioned by tenant\n",
        "acme_records = query(alice_ctx)\n",
        "globex_records = query(bob_ctx, value_min=100, value_max=500)\n",
        "\n",
        "# Verify strict partitioning\n",
        "assert all(r[\"tenant_id\"] == \"acme\" for r in acme_records)\n",
        "assert all(r[\"tenant_id\"] == \"globex\" for r in globex_records)\n",
        "```\n",
        "\n",
        "### Security Boundary Testing\n",
        "```python\n",
        "# Attempting to access non-existent tenant fails\n",
        "try:\n",
        "    query({\"tenant_id\": \"evil_corp\"})\n",
        "except KeyError:\n",
        "    print(\"Correctly blocked unauthorized tenant access\")\n",
        "\n",
        "# Missing tenant_id in context fails\n",
        "try:\n",
        "    query({\"user_id\": \"alice\"})  # Wrong key\n",
        "except KeyError:\n",
        "    print(\"Correctly required tenant_id in context\")\n",
        "```\n",
        "\n",
        "### Range Query Edge Cases\n",
        "```python\n",
        "# Valid range filtering\n",
        "filtered = query(alice_ctx, value_min=200, value_max=800)\n",
        "assert all(200  max\n",
        "except ValueError:\n",
        "    print(\"Correctly rejected invalid range\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q79gFg5DOtlN"
      },
      "source": [
        "# Requirements\n",
        "## Explicit Requirements\n",
        "\n",
        "- Data Generation: Generate at least 50 records per tenant during bootstrap, each containing exactly three fields: `tenant_id` (string), `obj_id` (32-character hexadecimal string), and value (integer from 0-999)\n",
        "- Tenant Isolation: Query function must derive tenant access solely from user_ctx[\"tenant_id\"], never from global state or other sources\n",
        "- Reproducible Randomness: Same rng_seed value must produce identical datasets across different bootstrap calls\n",
        "- State Management: Support multiple bootstrap calls in the same process, with each call completely resetting the data store\n",
        "- Error Handling: Raise KeyError for missing or unknown tenant_id, ValueError for invalid query parameters or duplicate tenants in bootstrap\n",
        "- API Constraints: Expose only the two specified functions; all helper classes/functions must use private naming (underscore prefix)\n",
        "\n",
        "## Implicit Requirements\n",
        "\n",
        "- Memory Safety: Return copies of data to prevent external mutation of internal state\n",
        "- Parameter Validation: Validate range parameters before applying filters (value_min ≤ value_max)\n",
        "- Context Isolation: Never cache or persist user context information between query calls\n",
        "- Deterministic Behavior: Given identical inputs, produce identical outputs across program runs\n",
        "- Clean State Transitions: Ensure bootstrap completely replaces previous data, not appending to it\n",
        "\n",
        "## Edge Cases\n",
        "\n",
        "- Bootstrap Reset: Calling bootstrap multiple times must completely replace the previous dataset\n",
        "- Duplicate Tenants: Passing duplicate tenant IDs to bootstrap must raise ValueError\n",
        "- Missing Context Keys: Query with user_ctx lacking \"tenant_id\" key must raise KeyError\n",
        "- Invalid Ranges: Query where value_min > value_max must raise ValueError before data access\n",
        "- Empty Results: Range filters that match no records should return empty list, not raise exceptions\n",
        "- Uninitialized Access: Calling query before any bootstrap call must raise a clear error\n",
        "\n",
        "## Constraints\n",
        "\n",
        "- Standard Library Only: No external packages beyond Python's built-in modules\n",
        "- Python Version: Must work on CPython 3.8+\n",
        "- No I/O Operations: No file system access, network calls, or console output within the module\n",
        "- Single File Solution: Deliver as a single main.py file that can be imported\n",
        "- Memory Only: All data must exist purely in memory with no persistence mechanisms\n",
        "\n",
        "## Function Signatures\n",
        "\n",
        "```python\n",
        "def bootstrap(tenants: list[str], rng_seed: int | None = None) -> None:\n",
        "    \"\"\"Initialize the multi-tenant data store.\"\"\"\n",
        "    ...\n",
        "\n",
        "def query(user_ctx: dict, *, value_min: int | None = None, value_max: int | None = None) -> list[dict]:\n",
        "    \"\"\"Query tenant-specific data with optional range filtering.\"\"\"\n",
        "    ...\n",
        "```\n",
        "\n",
        "## Important Notes\n",
        "\n",
        "The solution must be a single importable Python file with no external dependencies. Testing will include attempts to forge user contexts and verify that tenant boundaries are completely impermeable. The random module usage will be monitored to ensure proper seed handling, so avoid caching random values outside of bootstrap calls. The module should raise appropriate exceptions for all error conditions rather than returning error indicators or printing messages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSg66GeEbDAT"
      },
      "outputs": [],
      "source": [
        "# code\n",
        "\n",
        "from typing import List, Dict, Any, Optional\n",
        "import random\n",
        "\n",
        "__all__ = [\"bootstrap\", \"query\"]\n",
        "\n",
        "_store: Optional[Dict[str, List[Dict[str, Any]]]] = None\n",
        "\n",
        "\n",
        "def bootstrap(tenants: List[str], rng_seed: Optional[int] = None) -> None:\n",
        "    \"\"\"\n",
        "    Initialize (or reinitialize) the multi-tenant data store.\n",
        "\n",
        "    Generates exactly 50 mock records per tenant, each record containing:\n",
        "      - tenant_id: str\n",
        "      - obj_id:   str (32-character hexadecimal)\n",
        "      - value:    int (0–999)\n",
        "\n",
        "    Args:\n",
        "        tenants:  List of unique tenant identifiers.\n",
        "        rng_seed: Optional seed for reproducible randomness.\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If `tenants` contains duplicates.\n",
        "    \"\"\"\n",
        "    global _store\n",
        "\n",
        "    # Detect duplicate tenant IDs\n",
        "    if len(tenants) != len(set(tenants)):\n",
        "        raise ValueError(\"Duplicate tenant IDs are not allowed\")\n",
        "\n",
        "    # Build a fresh store with a private RNG\n",
        "    rng = random.Random(rng_seed)\n",
        "    new_store: Dict[str, List[Dict[str, Any]]] = {}\n",
        "\n",
        "    for tenant_id in tenants:\n",
        "        new_store[tenant_id] = _generate_records(tenant_id, rng)\n",
        "\n",
        "    _store = new_store\n",
        "\n",
        "\n",
        "def query(\n",
        "    user_ctx: Dict[str, Any],\n",
        "    *,\n",
        "    value_min: Optional[int] = None,\n",
        "    value_max: Optional[int] = None\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Retrieve records for the tenant specified in user_ctx[\"tenant_id\"],\n",
        "    optionally filtered by a numeric range on the `value` field.\n",
        "\n",
        "    Args:\n",
        "        user_ctx:   Dict containing at least the key \"tenant_id\".\n",
        "        value_min:  Inclusive lower bound on record[\"value\"], if provided.\n",
        "        value_max:  Inclusive upper bound on record[\"value\"], if provided.\n",
        "\n",
        "    Returns:\n",
        "        List of record dicts matching the tenant and value filters.\n",
        "\n",
        "    Raises:\n",
        "        RuntimeError: If bootstrap() has not yet been called.\n",
        "        KeyError:     If \"tenant_id\" is missing or not recognized.\n",
        "        ValueError:   If value_min > value_max.\n",
        "    \"\"\"\n",
        "    global _store\n",
        "\n",
        "    # Must initialize first\n",
        "    if _store is None:\n",
        "        raise RuntimeError(\"bootstrap() must be called before query()\")\n",
        "\n",
        "    # Validate and extract tenant_id\n",
        "    if \"tenant_id\" not in user_ctx:\n",
        "        raise KeyError(\"tenant_id\")\n",
        "    tenant_id = user_ctx[\"tenant_id\"]\n",
        "    if tenant_id not in _store:\n",
        "        raise KeyError(tenant_id)\n",
        "\n",
        "    # Validate range parameters\n",
        "    if value_min is not None and value_max is not None and value_min > value_max:\n",
        "        raise ValueError(\"value_min cannot be greater than value_max\")\n",
        "\n",
        "    # Filter and return shallow copies\n",
        "    results: List[Dict[str, Any]] = []\n",
        "    for record in _store[tenant_id]:\n",
        "        val = record[\"value\"]\n",
        "        if value_min is not None and val < value_min:\n",
        "            continue\n",
        "        if value_max is not None and val > value_max:\n",
        "            continue\n",
        "        results.append(record.copy())\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def _generate_records(\n",
        "    tenant_id: str,\n",
        "    rng: random.Random\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Helper to generate 50 mock records for a single tenant.\n",
        "\n",
        "    Args:\n",
        "        tenant_id: The tenant identifier to embed in each record.\n",
        "        rng:       A private Random instance for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "        A list of 50 record dicts.\n",
        "    \"\"\"\n",
        "    records: List[Dict[str, Any]] = []\n",
        "    for _ in range(50):\n",
        "        obj_id = \"\".join(rng.choice(\"0123456789abcdef\") for _ in range(32))\n",
        "        value = rng.randint(0, 999)\n",
        "        records.append({\n",
        "            \"tenant_id\": tenant_id,\n",
        "            \"obj_id\":     obj_id,\n",
        "            \"value\":      value\n",
        "        })\n",
        "    return records\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUlcq7ycbHYw"
      },
      "outputs": [],
      "source": [
        "# tests\n",
        "\n",
        "import csv\n",
        "import os\n",
        "import tempfile\n",
        "import unittest\n",
        "\n",
        "from main import DateCleaner, CleaningConfig\n",
        "\n",
        "\n",
        "class TestDateCleaner(unittest.TestCase):\n",
        "    \"\"\"Unit tests covering all P/R requirements for DateCleaner.\"\"\"\n",
        "\n",
        "    def setUp(self):\n",
        "        \"\"\"Prepare a fresh DateCleaner and default date.\"\"\"\n",
        "        self.cleaner = DateCleaner()\n",
        "        self.default = \"1900-01-01\"\n",
        "        self.in_path = None\n",
        "        self.out_path = None\n",
        "\n",
        "    def tearDown(self):\n",
        "        \"\"\"Remove any temp files created.\"\"\"\n",
        "        for p in (self.in_path, self.out_path):\n",
        "            if p and os.path.exists(p):\n",
        "                os.remove(p)\n",
        "\n",
        "    def _write_csv(self, rows, cols):\n",
        "        \"\"\"Write rows to a temp CSV; return in/out file paths.\"\"\"\n",
        "        tf = tempfile.NamedTemporaryFile(\n",
        "            mode=\"w+\", newline=\"\", delete=False\n",
        "        )\n",
        "        writer = csv.DictWriter(tf, fieldnames=cols)\n",
        "        writer.writeheader()\n",
        "        writer.writerows(rows)\n",
        "        tf.flush(); tf.close()\n",
        "        self.in_path = tf.name\n",
        "        self.out_path = tf.name + \".out\"\n",
        "        return self.in_path, self.out_path\n",
        "\n",
        "    def _read_out(self):\n",
        "        \"\"\"Read output CSV into list of dicts.\"\"\"\n",
        "        with open(self.out_path, newline=\"\", encoding=\"utf-8\") as f:\n",
        "            return list(csv.DictReader(f))\n",
        "\n",
        "    def test_validate_supported_formats(self):\n",
        "        \"\"\"Ensure validate_date_column handles all required formats.\"\"\"\n",
        "        inputs = [\n",
        "            \"02/03/2023\", \"03/02/2023\", \"2023-03-02\",\n",
        "            \"02-Mar-2023\", \"Mar 02, 2023\", \"March 02, 2023\",\n",
        "            \"2023-03-02T12:34:56Z\",\n",
        "            \"2023-03-02T12:34:56.789Z\"\n",
        "        ]\n",
        "        cleaned = self.cleaner.validate_date_column(inputs)\n",
        "        for out, _ in cleaned:\n",
        "            self.assertEqual(out, \"2023-03-02\")\n",
        "\n",
        "    def test_ambiguous_resolution_and_swap(self):\n",
        "        \"\"\"02/03/2023 under US→Feb 3, Intl→Mar 2.\"\"\"\n",
        "        rows = [{\"c\": \"02/03/2023\"}]\n",
        "        in_p, out_p = self._write_csv(rows, [\"c\"])\n",
        "        # US\n",
        "        cfg_us = CleaningConfig([\"c\"], \"fill_default\", \"US\")\n",
        "        self.cleaner.process_file(in_p, out_p, cfg_us)\n",
        "        self.assertEqual(self._read_out()[0][\"c\"], \"2023-02-03\")\n",
        "        # Intl\n",
        "        cfg_i = CleaningConfig([\"c\"], \"fill_default\", \"International\")\n",
        "        self.cleaner.process_file(in_p, out_p, cfg_i)\n",
        "        self.assertEqual(self._read_out()[0][\"c\"], \"2023-03-02\")\n",
        "\n",
        "    def test_transposed_month_day_correction(self):\n",
        "        \"\"\"Handle '13/02/2023' correctly without counting a swap.\"\"\"\n",
        "        rows = [{\"c\": \"13/02/2023\"}]\n",
        "        in_p, out_p = self._write_csv(rows, [\"c\"])\n",
        "        cfg = CleaningConfig([\"c\"], \"fill_default\", \"International\")\n",
        "        report = self.cleaner.process_file(in_p, out_p, cfg)\n",
        "        self.assertEqual(self._read_out()[0][\"c\"], \"2023-02-13\")\n",
        "        self.assertEqual(report[\"corrections_made\"][\"month_day_swapped\"], 0)\n",
        "\n",
        "    def test_leap_year_edge_cases(self):\n",
        "        \"\"\"29-Feb valid in leap year; invalid in non‑leap → default.\"\"\"\n",
        "        rows = [{\"d\": \"02/29/2020\"}, {\"d\": \"02/29/2019\"}]\n",
        "        in_p, out_p = self._write_csv(rows, [\"d\"])\n",
        "        cfg = CleaningConfig([\"d\"], \"fill_default\", \"US\")\n",
        "        report = self.cleaner.process_file(in_p, out_p, cfg)\n",
        "        out = [r[\"d\"] for r in self._read_out()]\n",
        "        self.assertEqual(out[0], \"2020-02-29\")\n",
        "        self.assertEqual(out[1], self.default)\n",
        "        self.assertEqual(\n",
        "            report[\"corrections_made\"][\"invalid_dates_corrected\"], 1\n",
        "        )\n",
        "\n",
        "    def test_two_digit_years_inference(self):\n",
        "        \"\"\"Two‑digit years fall back to default_date per current impl.\"\"\"\n",
        "        cleaned = self.cleaner.validate_date_column([\"01/01/20\", \"01/01/85\"])\n",
        "        for out, _ in cleaned:\n",
        "            self.assertEqual(out, self.default)\n",
        "\n",
        "    def test_missing_value_strategies(self):\n",
        "        \"\"\"drop_row, fill_default, interpolate behave as specified.\"\"\"\n",
        "        rows = [{\"id\": \"1\", \"d\": \"03/01/2023\"}, {\"id\": \"2\", \"d\": \"\"}]\n",
        "        in_p, out_p = self._write_csv(rows, [\"id\", \"d\"])\n",
        "        # drop_row\n",
        "        cfg1 = CleaningConfig([\"d\"], \"drop_row\", \"US\")\n",
        "        self.cleaner.process_file(in_p, out_p, cfg1)\n",
        "        self.assertEqual(len(self._read_out()), 1)\n",
        "        # fill_default\n",
        "        cfg2 = CleaningConfig(\n",
        "            [\"d\"], \"fill_default\", \"US\", default_date=\"2000-01-01\"\n",
        "        )\n",
        "        self.cleaner.process_file(in_p, out_p, cfg2)\n",
        "        self.assertEqual(self._read_out()[1][\"d\"], \"2000-01-01\")\n",
        "        # interpolate\n",
        "        cfg3 = CleaningConfig([\"d\"], \"interpolate\", \"US\")\n",
        "        self.cleaner.process_file(in_p, out_p, cfg3)\n",
        "        self.assertEqual(self._read_out()[1][\"d\"], \"2023-03-01\")\n",
        "\n",
        "    def test_out_of_range_detection(self):\n",
        "        \"\"\"Dates outside 1900–2100 count as out_of_range_dates errors.\"\"\"\n",
        "        rows = [{\"x\": \"12/31/1899\"}, {\"x\": \"01/01/2101\"}]\n",
        "        in_p, out_p = self._write_csv(rows, [\"x\"])\n",
        "        cfg = CleaningConfig([\"x\"], \"fill_default\", \"US\")\n",
        "        report = self.cleaner.process_file(in_p, out_p, cfg)\n",
        "        self.assertEqual(report[\"errors_encountered\"][\"out_of_range_dates\"], 2)\n",
        "\n",
        "    def test_timezone_normalization(self):\n",
        "        \"\"\"ISO timestamps parse to correct date without time component.\"\"\"\n",
        "        rows = [\n",
        "            {\"t\": \"2023-12-31T23:59:59Z\"},\n",
        "            {\"t\": \"2023-12-31T23:59:59.123Z\"}\n",
        "        ]\n",
        "        in_p, out_p = self._write_csv(rows, [\"t\"])\n",
        "        cfg = CleaningConfig([\"t\"], \"fill_default\", \"US\")\n",
        "        self.cleaner.process_file(in_p, out_p, cfg)\n",
        "        out = [r[\"t\"] for r in self._read_out()]\n",
        "        self.assertTrue(all(o.startswith(\"2023-12-31\") for o in out))\n",
        "\n",
        "    def test_preserve_non_date_columns(self):\n",
        "        \"\"\"Non-date columns remain unchanged.\"\"\"\n",
        "        rows = [{\"id\": \"A\", \"d\": \"01/02/2023\", \"note\": \"foo,bar\"}]\n",
        "        in_p, out_p = self._write_csv(rows, [\"id\", \"d\", \"note\"])\n",
        "        cfg = CleaningConfig([\"d\"], \"fill_default\", \"US\")\n",
        "        self.cleaner.process_file(in_p, out_p, cfg)\n",
        "        out_row = self._read_out()[0]\n",
        "        self.assertEqual(out_row[\"note\"], \"foo,bar\")\n",
        "\n",
        "    def test_multiple_date_columns(self):\n",
        "        \"\"\"Clean multiple date columns in one pass.\"\"\"\n",
        "        rows = [{\n",
        "            \"a\": \"02/04/2023\",\n",
        "            \"b\": \"04/02/2023\",\n",
        "            \"other\": \"X\"\n",
        "        }]\n",
        "        in_p, out_p = self._write_csv(rows, [\"a\", \"b\", \"other\"])\n",
        "        cfg = CleaningConfig([\"a\", \"b\"], \"fill_default\", \"International\")\n",
        "        self.cleaner.process_file(in_p, out_p, cfg)\n",
        "        out = self._read_out()[0]\n",
        "        self.assertEqual(out[\"a\"], \"2023-04-02\")\n",
        "        self.assertEqual(out[\"b\"], \"2023-02-04\")\n",
        "        self.assertEqual(out[\"other\"], \"X\")\n",
        "\n",
        "    def test_corrupted_and_spaces(self):\n",
        "        \"\"\"Handle extra spaces, transposed fields, and malformed dates.\"\"\"\n",
        "        rows = [{\n",
        "            \"d\": \"  02 / 30 / 2023 \",\n",
        "        }, {\n",
        "            \"d\": \"NULL\",\n",
        "        }, {\n",
        "            \"d\": \"N/A\",\n",
        "        }]\n",
        "        in_p, out_p = self._write_csv(rows, [\"d\"])\n",
        "        cfg = CleaningConfig([\"d\"], \"fill_default\", \"US\")\n",
        "        report = self.cleaner.process_file(in_p, out_p, cfg)\n",
        "        out = [r[\"d\"] for r in self._read_out()]\n",
        "        # All three become default since 02/30 invalid, and NULL/N/A\n",
        "        self.assertEqual(out, [self.default] * 3)\n",
        "        # Count invalid correction once, missing_values_filled twice\n",
        "        self.assertEqual(report[\"corrections_made\"][\"invalid_dates_corrected\"], 1)\n",
        "        self.assertEqual(report[\"corrections_made\"][\"missing_values_filled\"], 2)\n",
        "\n",
        "    def test_deterministic_processing(self):\n",
        "        \"\"\"Repeated runs with same config produce identical reports.\"\"\"\n",
        "        rows = [{\"d\": \"03/03/2023\"} for _ in range(3)]\n",
        "        in_p, out_p = self._write_csv(rows, [\"d\"])\n",
        "        cfg = CleaningConfig([\"d\"], \"fill_default\", \"US\")\n",
        "        rep1 = self.cleaner.process_file(in_p, out_p, cfg)\n",
        "        out1 = self._read_out()\n",
        "        rep2 = self.cleaner.process_file(in_p, out_p, cfg)\n",
        "        out2 = self._read_out()\n",
        "        self.assertEqual(rep1, rep2)\n",
        "        self.assertEqual(out1, out2)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxZs2ehLYWkm"
      },
      "source": [
        "# Model Breaking Proof\n",
        "\n",
        "#### Model Breaking Task URL: https://rlhf-v3.turing.com/prompt/9aaf928e-beff-400f-8c23-418520e93be2\n",
        "\n",
        "#### Model code:\n",
        "\n",
        "```python\n",
        "import random\n",
        "\n",
        "# Private global data store (None indicates uninitialized)\n",
        "_data_store = None\n",
        "\n",
        "def bootstrap(tenants: list[str], rng_seed: int | None = None) -> None: # <Issue> Uses `list[str]` (PEP 585) and `int | None` (PEP 604) — invalid prior to Python 3.9/3.10, breaks CPython 3.8 compatibility.\n",
        "    \"\"\"\n",
        "    Initialize the multi-tenant data store, generating 50 records per tenant.\n",
        "    \n",
        "    Raises:\n",
        "        ValueError: If any duplicate tenant IDs are found.\n",
        "    \"\"\"\n",
        "    global _data_store\n",
        "    \n",
        "    # Detect duplicate tenants\n",
        "    if len(set(tenants)) < len(tenants):\n",
        "        raise ValueError(\"Duplicate tenant IDs are not allowed in bootstrap.\")\n",
        "    \n",
        "    # Re-initialize data store\n",
        "    _data_store = {}\n",
        "\n",
        "    # Use a dedicated random generator to avoid affecting global state\n",
        "    _rng = random.Random(rng_seed)\n",
        "    \n",
        "    for tenant_id in tenants:\n",
        "        _data_store[tenant_id] = _generate_tenant_data(tenant_id, _rng)\n",
        "\n",
        "def query(user_ctx: dict, *, value_min: int | None = None, value_max: int | None = None) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Query tenant-specific data with optional range filtering on the 'value' field.\n",
        "    \n",
        "    Raises:\n",
        "        KeyError: If tenant_id is missing in user_ctx or tenant_id is not recognized,\n",
        "                  or if the data store is uninitialized (None).\n",
        "        ValueError: If value_min > value_max or any other invalid parameters.\n",
        "    \"\"\"\n",
        "    global _data_store\n",
        "    \n",
        "    # Ensure we have been bootstrapped\n",
        "    if _data_store is None:\n",
        "        raise KeyError(\"No data store found. Call bootstrap before querying.\") # <Issue> Should raise RuntimeError on uninitialized access, not KeyError.\n",
        "    \n",
        "    # Validate tenant_id in user context\n",
        "    if \"tenant_id\" not in user_ctx:\n",
        "        raise KeyError(\"Missing 'tenant_id' in user context.\")\n",
        "    \n",
        "    tenant_id = user_ctx[\"tenant_id\"]\n",
        "    if tenant_id not in _data_store:\n",
        "        raise KeyError(f\"Unknown tenant_id: {tenant_id}\")\n",
        "    \n",
        "    # Validate range parameters\n",
        "    if value_min is not None and value_max is not None:\n",
        "        if value_min > value_max:\n",
        "            raise ValueError(\"value_min must be <= value_max.\")\n",
        "    \n",
        "    records = _data_store[tenant_id]\n",
        "    \n",
        "    # Apply numeric filters\n",
        "    filtered = []\n",
        "    for record in records:\n",
        "        val = record[\"value\"]\n",
        "        if value_min is not None and val < value_min:\n",
        "            continue\n",
        "        if value_max is not None and val > value_max:\n",
        "            continue\n",
        "        filtered.append(dict(record))  # Return a copy\n",
        "    \n",
        "    return filtered\n",
        "\n",
        "def _generate_tenant_data(tenant_id: str, rng: random.Random) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Generate 50 mock records for a given tenant.\n",
        "    \"\"\"\n",
        "    records = []\n",
        "    for _ in range(50):\n",
        "        obj_id = \"\".join(rng.choice(\"0123456789abcdef\") for _ in range(32))\n",
        "        value = rng.randint(0, 999)\n",
        "        records.append({\n",
        "            \"tenant_id\": tenant_id,\n",
        "            \"obj_id\": obj_id,\n",
        "            \"value\": value\n",
        "        })\n",
        "    return records\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}