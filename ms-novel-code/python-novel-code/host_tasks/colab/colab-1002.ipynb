{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKe-t1pIOo2f"
      },
      "source": [
        "# Metadata\n",
        "\n",
        "**L1 Taxonomy** - Backend Integration\n",
        "\n",
        "**L2 Taxonomy** - Webhooks\n",
        "\n",
        "**Subtopic** - Handling duplicate webhook deliveries idempotently (processing each event once)\n",
        "\n",
        "**Use Case** - Implement a Python script that simulates the receipt of webhook events. The script should handle duplicate webhook deliveries idempotently by storing a unique identifier for each event in a local file and checking this file before processing each event to ensure it is only processed once.\n",
        "\n",
        "**Programming Language** - Python\n",
        "\n",
        "**Target Model** - GPT-4o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS0xHSaZoEJO"
      },
      "source": [
        "# Setup\n",
        "\n",
        "```requirements.txt\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YToTLlRqOqmj"
      },
      "source": [
        "# Prompt\n",
        "Problem Statement:\n",
        "- Implement a Python script that simulates the receipt of webhook events.\n",
        "- The script should handle duplicate webhook deliveries idempotently by storing a unique identifier for each event in a local file and checking this file before processing each event to ensure it is only processed once.\n",
        "- Only events with unique IDs (not already stored) should be processed.\n",
        "\n",
        "Input Format:\n",
        "- A list of webhook event objects, each represented as a Python dictionary.\n",
        "- Each event must contain the following fields:\n",
        "  - `event_id`: A unique string identifier for the event (used to detect duplicates).\n",
        "  - `event_type`: A string describing the type of event (e.g., \"user.created\").\n",
        "  - `timestamp`: A string in ISO 8601 format indicating when the event occurred.\n",
        "  - `payload`: A dictionary containing additional event-specific data.\n",
        "\n",
        "Input Constraints:\n",
        "- The `event_id` must be a non-empty string with a maximum length of 100 characters.\n",
        "- The `event_type` must be a non-empty string.\n",
        "- The `timestamp` must be a valid ISO 8601 formatted string (e.g., \"2025-07-22T08:00:00Z\").\n",
        "- The `payload` must be a JSON object and may contain arbitrary key-value pairs.\n",
        "- A maximum of 10,000 events may be processed in one run.\n",
        "- If an event is missing required fields, it is considered malformed and may be skipped or raise an error depending on implementation preference.\n",
        "\n",
        "Output Format:\n",
        "- For each event:\n",
        "  - If the `event_id` has not been seen before:\n",
        "    - Process the event and log: \"Processed event: <event_id>\"\n",
        "  - If the `event_id` is a duplicate:\n",
        "    - Skip processing and log: \"Duplicate event skipped: <event_id>\"\n",
        "\n",
        "- All processed `event_id`s are stored in a local file (e.g., \"processed_events.txt\") to ensure idempotency across runs.\n",
        "\n",
        "Function Signature:\n",
        "```python\n",
        "def process_webhook_events(events: list, storage_file: str = \"processed_events.txt\") -> None:\n",
        "```\n",
        "\n",
        "Example:\n",
        "```python\n",
        "Input:\n",
        "[\n",
        "    {\n",
        "        \"event_id\": \"evt_001\",\n",
        "        \"event_type\": \"user.created\",\n",
        "        \"timestamp\": \"2025-07-22T08:00:00Z\",\n",
        "        \"payload\": {\"user_id\": 1, \"name\": \"Alice\"}\n",
        "    },\n",
        "    {\n",
        "        \"event_id\": \"evt_002\",\n",
        "        \"event_type\": \"user.updated\",\n",
        "        \"timestamp\": \"2025-07-22T08:05:00Z\",\n",
        "        \"payload\": {\"user_id\": 1, \"name\": \"Alice Smith\"}\n",
        "    },\n",
        "    {\n",
        "        \"event_id\": \"evt_001\",\n",
        "        \"event_type\": \"user.created\",\n",
        "        \"timestamp\": \"2025-07-22T08:10:00Z\",\n",
        "        \"payload\": {\"user_id\": 1, \"name\": \"Duplicate Alice\"}\n",
        "    }\n",
        "]\n",
        "\n",
        "Output:\n",
        "Processed event: evt_001  \n",
        "Processed event: evt_002  \n",
        "Duplicate event skipped: evt_001\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q79gFg5DOtlN"
      },
      "source": [
        "# Requirements\n",
        "Explicit Requirements:\n",
        "- Each event must be uniquely identified using its `event_id`.\n",
        "- Duplicate events (same `event_id`) must not be processed more than once.\n",
        "- All processed `event_id`s must be persisted in a local file for future deduplication.\n",
        "- The system must log whether an event is processed or skipped due to duplication.\n",
        "\n",
        "Implicit Requirements:\n",
        "- The solution must be idempotent across multiple runs by persisting state (via a file).\n",
        "- The solution should gracefully handle cases where the storage file does not exist initially.\n",
        "- The system should efficiently handle large input (up to 10,000 events) without reprocessing.\n",
        "- Event ordering should be preserved in output (processed/skipped logs appear in input order).\n",
        "\n",
        "Solution Expectations:\n",
        "- Read and parse the input list of webhook event dictionaries.\n",
        "- Check if each `event_id` has already been processed by referencing a local file.\n",
        "- If the `event_id` is new:\n",
        "  - Process the event (e.g., print or simulate processing).\n",
        "  - Append the `event_id` to the storage file to mark it as processed.\n",
        "  - Print: \"Processed event: <event_id>\"\n",
        "- If the `event_id` already exists in the file:\n",
        "  - Skip processing.\n",
        "  - Print: \"Duplicate event skipped: <event_id>\"\n",
        "- Ensure file read/write operations are safe and efficient.\n",
        "- Maintain a consistent and readable output format.\n",
        "- Perform input validation on all Inputs according to the Input Constraints.\n",
        "\n",
        "Edge Cases and Behavior:\n",
        "- Duplicate Events in the Same Input:\n",
        "  - Behavior: Only the first occurrence is processed; subsequent duplicates are skipped.\n",
        "\n",
        "- Duplicate Events Across Multiple Runs:\n",
        "  - Behavior: Events processed in previous runs (stored in the file) are not reprocessed.\n",
        "\n",
        "- Missing or Corrupted Storage File:\n",
        "  - Behavior: If the storage file does not exist, it is created. If it is empty or unreadable, processing continues from scratch.\n",
        "\n",
        "- Empty Input List:\n",
        "  - Behavior: No output is printed, and no changes are made to the storage file.\n",
        "\n",
        "- Malformed Events (e.g., missing `event_id`):\n",
        "  - Behavior: Such events should be skipped or raise a descriptive error depending on implementation preference.\n",
        "\n",
        "- Large Volume of Events (e.g., 10,000+):\n",
        "  - Behavior: Must be handled efficiently without memory or performance issues. Deduplication should remain accurate.\n",
        "\n",
        "- Non-UTF8 Characters or Special Characters in `event_id`:\n",
        "  - Behavior: Properly handled as long as the file supports UTF-8 encoding.\n",
        "\n",
        "- Reordered Events:\n",
        "  - Behavior: Deduplication is based on `event_id`, so order doesnâ€™t affect correctness but may affect output sequence.\n",
        "\n",
        "Solution Constraints:\n",
        "- Must use only standard Python libraries (no external dependencies).\n",
        "- Must support persistent deduplication across multiple script executions via a local file.\n",
        "- Must handle up to 10,000 events per run efficiently.\n",
        "- File I/O operations must be safe, using append or atomic write techniques to avoid corruption.\n",
        "- Must not modify the original input data.\n",
        "- Logging/output must preserve the input event order.\n",
        "- If the storage file is locked or cannot be written to, the system should fail gracefully with an informative error message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSg66GeEbDAT"
      },
      "outputs": [],
      "source": [
        "# code\n",
        "\"\"\"Process webhook events with deduplication and basic validation.\"\"\"\n",
        "\n",
        "import os\n",
        "import re\n",
        "\n",
        "\n",
        "def process_webhook_events(\n",
        "    events: list,\n",
        "    storage_file: str = \"processed_events.txt\"\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Process webhook events and store unique event IDs to avoid duplicates.\n",
        "\n",
        "    Args:\n",
        "        events: A list of webhook event objects (dictionaries).\n",
        "        storage_file: The name of the local file to store processed event IDs.\n",
        "    \"\"\"\n",
        "    processed_event_ids = set()\n",
        "\n",
        "    # Load already processed event IDs from the storage file\n",
        "    if os.path.exists(storage_file):\n",
        "        try:\n",
        "            with open(storage_file, 'r') as f:\n",
        "                for line in f:\n",
        "                    event_id = line.strip()\n",
        "                    if event_id:\n",
        "                        processed_event_ids.add(event_id)\n",
        "        except IOError as e:\n",
        "            print(f\"Error reading storage file '{storage_file}': {e}\")\n",
        "            return\n",
        "        except Exception as e:\n",
        "            print(\n",
        "                f\"An unexpected error occurred\"\n",
        "                f\" while reading '{storage_file}': {e}\"\n",
        "            )\n",
        "            return\n",
        "\n",
        "    # Open the storage file in append mode for writing new event IDs\n",
        "    try:\n",
        "        with (open(storage_file, 'a') as f_append):\n",
        "            for i, event in enumerate(events):\n",
        "                event_id = event.get(\"event_id\")\n",
        "                event_type = event.get(\"event_type\")\n",
        "                timestamp = event.get(\"timestamp\")\n",
        "                payload = event.get(\"payload\")\n",
        "\n",
        "                if not all([\n",
        "                    event_id, event_type, timestamp, payload is not None\n",
        "                ]):\n",
        "                    print(\n",
        "                        f\"Skipping malformed event at index {i}: \"\n",
        "                        f\"Missing required fields.\"\n",
        "                    )\n",
        "                    continue\n",
        "\n",
        "                if not isinstance(event_id, str\n",
        "                                  ) or not (1 <= len(event_id) <= 100):\n",
        "                    print(\n",
        "                        f\"Skipping malformed event at index {i}: \"\n",
        "                        f\"'event_id' is invalid.\"\n",
        "                    )\n",
        "                    continue\n",
        "\n",
        "                if not isinstance(event_type, str) or not event_type:\n",
        "                    print(\n",
        "                        f\"Skipping malformed event at index {i}: \"\n",
        "                        f\"'event_type' is invalid.\"\n",
        "                    )\n",
        "                    continue\n",
        "\n",
        "                # Basic ISO 8601 format check\n",
        "                if (\n",
        "                    not isinstance(timestamp, str) or\n",
        "                    not re.fullmatch(\n",
        "                        r'\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}Z',\n",
        "                        timestamp\n",
        "                    )\n",
        "                ):\n",
        "                    print(\n",
        "                        f\"Skipping malformed event at index {i}: \"\n",
        "                        f\"'timestamp' is not in valid ISO 8601 format \"\n",
        "                        f\"(YYYY-MM-DDTHH:MM:SSZ).\"\n",
        "                    )\n",
        "                    continue\n",
        "\n",
        "                if not isinstance(payload, dict):\n",
        "                    print(\n",
        "                        f\"Skipping malformed event at index {i}: \"\n",
        "                        f\"'payload' is not a dictionary.\"\n",
        "                    )\n",
        "                    continue\n",
        "\n",
        "                if event_id in processed_event_ids:\n",
        "                    print(f\"Duplicate event skipped: {event_id}\")\n",
        "                else:\n",
        "                    print(f\"Processed event: {event_id}\")\n",
        "                    processed_event_ids.add(event_id)\n",
        "                    f_append.write(f\"{event_id}\\n\")\n",
        "                    f_append.flush()\n",
        "\n",
        "    except IOError as e:\n",
        "        print(f\"Error writing to storage file '{storage_file}': {e}\")\n",
        "    except Exception as e:\n",
        "        print(\n",
        "            f\"An unexpected error occurred during event processing: {e}\"\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if os.path.exists(\"processed_events.txt\"):\n",
        "        os.remove(\"processed_events.txt\")\n",
        "        print(\"Cleaned up 'processed_events.txt' from previous run.\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    print(\"--- First Run ---\")\n",
        "    events_run1 = [\n",
        "        {\n",
        "            \"event_id\": \"evt_001\",\n",
        "            \"event_type\": \"user.created\",\n",
        "            \"timestamp\": \"2025-07-22T08:00:00Z\",\n",
        "            \"payload\": {\"user_id\": 1, \"name\": \"Alice\"}\n",
        "        },\n",
        "        {\n",
        "            \"event_id\": \"evt_002\",\n",
        "            \"event_type\": \"user.updated\",\n",
        "            \"timestamp\": \"2025-07-22T08:05:00Z\",\n",
        "            \"payload\": {\"user_id\": 1, \"name\": \"Alice Smith\"}\n",
        "        },\n",
        "        {\n",
        "            \"event_id\": \"evt_001\",\n",
        "            \"event_type\": \"user.created\",\n",
        "            \"timestamp\": \"2025-07-22T08:10:00Z\",\n",
        "            \"payload\": {\"user_id\": 1, \"name\": \"Duplicate Alice\"}\n",
        "        },\n",
        "        {\n",
        "            \"event_id\": \"evt_003\",\n",
        "            \"event_type\": \"product.added\",\n",
        "            \"timestamp\": \"2025-07-22T09:00:00Z\",\n",
        "            \"payload\": {\"product_id\": 101, \"name\": \"Laptop\"}\n",
        "        }\n",
        "    ]\n",
        "    process_webhook_events(events_run1)\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    print(\"--- Second Run ---\")\n",
        "    events_run2 = [\n",
        "        {\n",
        "            \"event_id\": \"evt_002\",\n",
        "            \"event_type\": \"user.deleted\",\n",
        "            \"timestamp\": \"2025-07-22T10:00:00Z\",\n",
        "            \"payload\": {\"user_id\": 1}\n",
        "        },\n",
        "        {\n",
        "            \"event_id\": \"evt_004\",\n",
        "            \"event_type\": \"order.placed\",\n",
        "            \"timestamp\": \"2025-07-22T10:15:00Z\",\n",
        "            \"payload\": {\"order_id\": 5001, \"amount\": 100.00}\n",
        "        },\n",
        "        {\n",
        "            \"event_id\": \"evt_001\",\n",
        "            \"event_type\": \"user.login\",\n",
        "            \"timestamp\": \"2025-07-22T10:20:00Z\",\n",
        "            \"payload\": {\"user_id\": 1}\n",
        "        }\n",
        "    ]\n",
        "    process_webhook_events(events_run2)\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    print(\"--- Malformed Events Test ---\")\n",
        "    malformed_events = [\n",
        "        {\n",
        "            \"event_type\": \"missing.id\",\n",
        "            \"timestamp\": \"2025-07-22T11:00:00Z\",\n",
        "            \"payload\": {}\n",
        "        },\n",
        "        {\n",
        "            \"event_id\": \"evt_005\",\n",
        "            \"event_type\": \"\",\n",
        "            \"timestamp\": \"2025-07-22T11:05:00Z\",\n",
        "            \"payload\": {}\n",
        "        },\n",
        "        {\n",
        "            \"event_id\": \"evt_006\",\n",
        "            \"event_type\": \"invalid.timestamp\",\n",
        "            \"timestamp\": \"2025/07/22 11:10:00\",\n",
        "            \"payload\": {}\n",
        "        },\n",
        "        {\n",
        "            \"event_id\": \"evt_007\",\n",
        "            \"event_type\": \"invalid.payload\",\n",
        "            \"timestamp\": \"2025-07-22T11:15:00Z\",\n",
        "            \"payload\": \"not_a_dict\"\n",
        "        },\n",
        "        {\n",
        "            \"event_id\": \"evt_008\" * 15,\n",
        "            \"event_type\": \"long.id.test\",\n",
        "            \"timestamp\": \"2025-07-22T11:20:00Z\",\n",
        "            \"payload\": {}\n",
        "        }\n",
        "    ]\n",
        "    process_webhook_events(malformed_events)\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    print(\"--- Empty Input Test ---\")\n",
        "    process_webhook_events([])\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    print(\"\\nContent of processed_events.txt:\")\n",
        "    if os.path.exists(\"processed_events.txt\"):\n",
        "        with open(\"processed_events.txt\", 'r') as f:\n",
        "            print(f.read().strip())\n",
        "    else:\n",
        "        print(\"File 'processed_events.txt' does not exist.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUlcq7ycbHYw"
      },
      "outputs": [],
      "source": [
        "# tests\n",
        "\n",
        "import io\n",
        "import os\n",
        "import tempfile\n",
        "import unittest\n",
        "from contextlib import redirect_stdout\n",
        "from pathlib import Path\n",
        "\n",
        "from main import process_webhook_events\n",
        "\n",
        "\n",
        "def _capture_output(func, *args, **kwargs):\n",
        "    \"\"\"Run *func* while capturing stdout; return list(stripped lines).\"\"\"\n",
        "    buf = io.StringIO()\n",
        "    with redirect_stdout(buf):\n",
        "        func(*args, **kwargs)\n",
        "    return [line.strip() for line in buf.getvalue().splitlines()]\n",
        "\n",
        "\n",
        "class TestProcessWebhookEvents(unittest.TestCase):\n",
        "    \"\"\"Unit-tests for `process_webhook_events` as specified in the prompt.\"\"\"\n",
        "\n",
        "    def setUp(self):\n",
        "        self.tmpdir = tempfile.TemporaryDirectory()\n",
        "        self.store_path = os.path.join(self.tmpdir.name, \"events.txt\")\n",
        "\n",
        "    def tearDown(self):\n",
        "        self.tmpdir.cleanup()\n",
        "\n",
        "    def test_unique_events_first_run(self):\n",
        "        events = [\n",
        "            {\"event_id\": \"evt_1\", \"event_type\": \"alpha\",\n",
        "                \"timestamp\": \"2025-07-22T08:00:00Z\", \"payload\": {}},\n",
        "            {\"event_id\": \"evt_2\", \"event_type\": \"beta\",\n",
        "                \"timestamp\": \"2025-07-22T08:01:00Z\", \"payload\": {}},\n",
        "        ]\n",
        "        out = _capture_output(process_webhook_events, events, self.store_path)\n",
        "        self.assertEqual(\n",
        "            out,\n",
        "            [\"Processed event: evt_1\", \"Processed event: evt_2\"],\n",
        "            \"Both unique events should be processed in order.\",\n",
        "        )\n",
        "        # verify IDs persisted\n",
        "        with open(self.store_path, encoding=\"utf-8\") as fh:\n",
        "            self.assertEqual(fh.read().splitlines(), [\"evt_1\", \"evt_2\"])\n",
        "\n",
        "    def test_duplicates_within_single_batch(self):\n",
        "        events = [\n",
        "            {\"event_id\": \"dup\", \"event_type\": \"x\",\n",
        "                \"timestamp\": \"2025-07-22T08:00:00Z\", \"payload\": {}},\n",
        "            {\"event_id\": \"dup\", \"event_type\": \"x\",\n",
        "                \"timestamp\": \"2025-07-22T08:02:00Z\", \"payload\": {}},\n",
        "        ]\n",
        "        out = _capture_output(process_webhook_events, events, self.store_path)\n",
        "        self.assertEqual(\n",
        "            out,\n",
        "            [\"Processed event: dup\", \"Duplicate event skipped: dup\"],\n",
        "            \"Second occurrence in same run should be skipped.\",\n",
        "        )\n",
        "\n",
        "    def test_idempotency_across_runs(self):\n",
        "        ev = {\"event_id\": \"evt_z\", \"event_type\": \"z\",\n",
        "              \"timestamp\": \"2025-07-22T08:00:00Z\", \"payload\": {}}\n",
        "        _capture_output(process_webhook_events, [ev], self.store_path)\n",
        "        out = _capture_output(process_webhook_events, [ev], self.store_path)\n",
        "        self.assertEqual(out, [\"Duplicate event skipped: evt_z\"])\n",
        "\n",
        "    def test_storage_file_created_when_missing(self):\n",
        "        self.assertFalse(Path(self.store_path).exists(),\n",
        "                         \"Pre-condition: storage file absent.\")\n",
        "        _capture_output(\n",
        "            process_webhook_events,\n",
        "            [{\"event_id\": \"a\", \"event_type\": \"t\",\n",
        "                \"timestamp\": \"2025-07-22T08:00:00Z\", \"payload\": {}}],\n",
        "            self.store_path,\n",
        "        )\n",
        "        self.assertTrue(Path(self.store_path).exists(),\n",
        "                        \"File should be created on first write.\")\n",
        "\n",
        "    def test_empty_input_no_processing(self):\n",
        "        out = _capture_output(process_webhook_events, [], self.store_path)\n",
        "        self.assertEqual(out, [], \"No events means no output.\")\n",
        "        if Path(self.store_path).exists():\n",
        "            self.assertEqual(\n",
        "                Path(self.store_path).read_text(encoding=\"utf-8\"), \"\")\n",
        "\n",
        "    def test_malformed_event_skipped(self):\n",
        "        \"\"\"Only the well-formed event is processed; malformed one is NOT.\"\"\"\n",
        "        events = [\n",
        "            {\"event_id\": \"good\", \"event_type\": \"x\",\n",
        "             \"timestamp\": \"2025-07-22T08:00:00Z\", \"payload\": {}},\n",
        "            {\"event_type\": \"missing_id\"},  # malformed\n",
        "        ]\n",
        "        out = _capture_output(process_webhook_events, events, self.store_path)\n",
        "\n",
        "        processed = [ln for ln in out if ln.startswith(\"Processed event:\")]\n",
        "        self.assertEqual(\n",
        "            processed,\n",
        "            [\"Processed event: good\"],\n",
        "            \"Malformed event must not be processed (no extra \"\n",
        "            \"'Processed event: â€¦' lines are allowed).\",\n",
        "        )\n",
        "\n",
        "    def test_non_ascii_event_id(self):\n",
        "        eid = \"%@#$42\"\n",
        "        events = [{\"event_id\": eid, \"event_type\": \"x\",\n",
        "                   \"timestamp\": \"2025-07-22T08:00:00Z\", \"payload\": {}}]\n",
        "        out = _capture_output(process_webhook_events, events, self.store_path)\n",
        "        self.assertEqual(out, [f\"Processed event: {eid}\"])\n",
        "\n",
        "    def test_max_length_event_id(self):\n",
        "        eid = \"x\" * 100\n",
        "        events = [{\"event_id\": eid, \"event_type\": \"t\",\n",
        "                   \"timestamp\": \"2025-07-22T08:00:00Z\", \"payload\": {}}]\n",
        "        out = _capture_output(process_webhook_events, events, self.store_path)\n",
        "        self.assertEqual(out, [f\"Processed event: {eid}\"])\n",
        "\n",
        "    def test_corrupted_storage_file_recovery(self):\n",
        "        # pre-corrupt with random bytes & blank lines\n",
        "        with open(self.store_path, \"wb\") as fh:\n",
        "            fh.write(b\"bad\\xffline\\n\\n\")\n",
        "        events = [{\"event_id\": \"fresh\", \"event_type\": \"a\",\n",
        "                   \"timestamp\": \"2025-07-22T08:00:00Z\", \"payload\": {}}]\n",
        "        out = _capture_output(process_webhook_events, events, self.store_path)\n",
        "        self.assertIn(\"Processed event: fresh\", out[0])\n",
        "\n",
        "    def test_large_batch_performance_integrity(self):\n",
        "        events = [\n",
        "            {\"event_id\": f\"id_{i}\", \"event_type\": \"bulk\",\n",
        "                \"timestamp\": \"2025-07-22T08:00:00Z\", \"payload\": {}}\n",
        "            for i in range(1_000)\n",
        "        ]\n",
        "        out = _capture_output(process_webhook_events, events, self.store_path)\n",
        "        # first and last lines sanity-check\n",
        "        self.assertEqual(out[0], \"Processed event: id_0\")\n",
        "        self.assertEqual(out[-1], \"Processed event: id_999\")\n",
        "        self.assertEqual(\n",
        "            len(out), 1_000, \"Every unique event should be processed once.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main(verbosity=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxZs2ehLYWkm"
      },
      "source": [
        "# Model Breaking Proof\n",
        "\n",
        "#### Model Breaking Task URL: https://rlhf-v3.turing.com/prompt/48b13eb1-45b7-4a3d-96a4-0f95d34e1b12?origin=https%3A%2F%2Flabeling-ms.turing.com&redirect_url=https%3A%2F%2Flabeling-ms.turing.com%2Fconversations%2F82885%2Fview\n",
        "\n",
        "#### Model code:\n",
        "\n",
        "```python\n",
        "# code generated by the model\n",
        "import os\n",
        "import json\n",
        "\n",
        "def process_webhook_events(events: list, storage_file: str = \"processed_events.txt\") -> None:\n",
        "    # Check if the storage file exists, if not create one.\n",
        "    if not os.path.exists(storage_file):\n",
        "        open(storage_file, 'w').close()\n",
        "        # <Issue>: No try-except when creating the file. Could fail if file permissions are restricted.\n",
        "    \n",
        "    # Read already processed event_ids from the storage file\n",
        "    with open(storage_file, 'r') as f:\n",
        "        # Reading line by line to avoid loading everything in memory at once for large files\n",
        "        processed_ids = set(line.strip() for line in f.readlines())\n",
        "        # <Issue>: `.readlines()` loads the entire file into memory, contrary to the comment above.\n",
        "        # <Issue>: No try-except around this read operation. Could raise IOError if file is corrupted or inaccessible.\n",
        "    \n",
        "    with open(storage_file, 'a') as f:\n",
        "        # <Issue>: No try-except for file writing. Could raise IOError if file is locked or read-only.\n",
        "        for event in events:\n",
        "            # Basic validation of the event\n",
        "            try:\n",
        "                event_id = event['event_id']\n",
        "                event_type = event['event_type']\n",
        "                timestamp = event['timestamp']\n",
        "                payload = event['payload']\n",
        "                \n",
        "                # Checking constraints\n",
        "                if not isinstance(event_id, str) or not event_id:\n",
        "                    raise ValueError(\"Invalid event_id\")\n",
        "                # <Issue>: Missing check for max length 100 for `event_id`.\n",
        "                if not isinstance(event_type, str) or not event_type:\n",
        "                    raise ValueError(\"Invalid event_type\")\n",
        "                 # <Issue>: No validation for ISO 8601 format of `timestamp`.\n",
        "                 # <Issue>: No validation that `payload` is a dictionary.\n",
        "                # Add other validation checks here if necessary\n",
        "\n",
        "            except (KeyError, ValueError) as e:  # Catch missing keys or validation issues\n",
        "                print(f\"Malformed event skipped: {e}\")\n",
        "                continue\n",
        "\n",
        "            # Process the event only if it hasn't been processed before\n",
        "            if event_id not in processed_ids:\n",
        "                # Simulate processing of the event\n",
        "                print(f\"Processed event: {event_id}\")\n",
        "                # Mark the event as processed by storing its id\n",
        "                f.write(event_id + '\\n')\n",
        "                # Add the processed_id to the current set to avoid processing it within the same run\n",
        "                processed_ids.add(event_id)\n",
        "            else:\n",
        "                # Duplicate event handling\n",
        "                print(f\"Duplicate event skipped: {event_id}\")\n",
        "\n",
        "# Example events for demonstration\n",
        "example_events = [\n",
        "    {\n",
        "        \"event_id\": \"evt_001\",\n",
        "        \"event_type\": \"user.created\",\n",
        "        \"timestamp\": \"2025-07-22T08:00:00Z\",\n",
        "        \"payload\": {\"user_id\": 1, \"name\": \"Alice\"}\n",
        "    },\n",
        "    {\n",
        "        \"event_id\": \"evt_002\",\n",
        "        \"event_type\": \"user.updated\",\n",
        "        \"timestamp\": \"2025-07-22T08:05:00Z\",\n",
        "        \"payload\": {\"user_id\": 1, \"name\": \"Alice Smith\"}\n",
        "    },\n",
        "    {\n",
        "        \"event_id\": \"evt_001\",\n",
        "        \"event_type\": \"user.created\",\n",
        "        \"timestamp\": \"2025-07-22T08:10:00Z\",\n",
        "        \"payload\": {\"user_id\": 1, \"name\": \"Duplicate Alice\"}\n",
        "    }\n",
        "]\n",
        "\n",
        "# Invoking the function\n",
        "process_webhook_events(example_events)\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}