{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKe-t1pIOo2f"
      },
      "source": [
        "# Metadata\n",
        "\n",
        "**L1 Taxonomy** - Backend Development\n",
        "\n",
        "**L2 Taxonomy** - API Development\n",
        "\n",
        "**Subtopic** - Building a RESTful API with Flask\n",
        "\n",
        "**Use Case** - Develop a Python module that reads a CSV file and transforms it into a JSON object\n",
        "\n",
        "**Programming Language** - Python\n",
        "\n",
        "**Target Model** - GPT-4o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS0xHSaZoEJO"
      },
      "source": [
        "# Setup\n",
        "\n",
        "```requirements.txt\n",
        "Flask==3.1.1\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YToTLlRqOqmj"
      },
      "source": [
        "# Prompt\n",
        "\n",
        "## Problem Overview\n",
        "\n",
        "You are tasked with building a **modular and extensible Python-based RESTful API** using **Flask** that can ingest a CSV file through an HTTP POST request, parse it, and return a JSON response. The transformation should support both **flat and nested JSON formats** based on user input. The system must ensure robustness, correctness, and scalability to support varying CSV schemas. The solution should be structured in a way that allows the CSV-to-JSON logic to function independently from the Flask interface and be easily tested or reused in other systems.\n",
        "\n",
        "\n",
        "## Input Format\n",
        "\n",
        "1. **HTTP POST Request** to endpoint `/transform`.\n",
        "2. The request must be in **multipart/form-data** format and contain:\n",
        "\n",
        "   * `file`: The uploaded CSV file.\n",
        "   * `format`: A string value either `\"flat\"` or `\"nested\"` specifying the transformation style.\n",
        "3. CSV headers can include dot notation (e.g., `person.name`, `person.age`) for nested output.\n",
        "\n",
        "Example request body:\n",
        "\n",
        "```\n",
        "POST /transform\n",
        "Content-Type: multipart/form-data\n",
        "- file: data.csv\n",
        "- format: nested\n",
        "```\n",
        "\n",
        "\n",
        "## Output Format\n",
        "\n",
        "A **JSON object** containing:\n",
        "\n",
        "* `success`: Boolean indicating success or failure.\n",
        "* `data`: The transformed JSON object (list of records).\n",
        "* `error`: Error message if any.\n",
        "\n",
        "**Successful response example**:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"success\": true,\n",
        "  \"data\": [\n",
        "    {\n",
        "      \"person\": {\n",
        "        \"name\": \"Alice\",\n",
        "        \"age\": \"30\"\n",
        "      },\n",
        "      \"city\": \"New York\"\n",
        "    },\n",
        "    {\n",
        "      \"person\": {\n",
        "        \"name\": \"Bob\",\n",
        "        \"age\": \"25\"\n",
        "      },\n",
        "      \"city\": \"Los Angeles\"\n",
        "    }\n",
        "  ],\n",
        "  \"error\": null\n",
        "}\n",
        "```\n",
        "\n",
        "**Failure response example**:\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"success\": false,\n",
        "  \"data\": null,\n",
        "  \"error\": \"Missing 'format' parameter or invalid CSV structure\"\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "## Examples\n",
        "\n",
        "### Input CSV (`flat` format):\n",
        "\n",
        "```\n",
        "name,age,city\n",
        "Alice,30,New York\n",
        "Bob,25,Los Angeles\n",
        "```\n",
        "\n",
        "### Output (`flat`):\n",
        "\n",
        "```json\n",
        "[\n",
        "  {\"name\": \"Alice\", \"age\": \"30\", \"city\": \"New York\"},\n",
        "  {\"name\": \"Bob\", \"age\": \"25\", \"city\": \"Los Angeles\"}\n",
        "]\n",
        "```\n",
        "\n",
        "### Input CSV (`nested` format with dot notation):\n",
        "\n",
        "```\n",
        "person.name,person.age,location.city\n",
        "Alice,30,New York\n",
        "Bob,25,Los Angeles\n",
        "```\n",
        "\n",
        "### Output (`nested`):\n",
        "\n",
        "```json\n",
        "[\n",
        "  {\n",
        "    \"person\": {\"name\": \"Alice\", \"age\": \"30\"},\n",
        "    \"location\": {\"city\": \"New York\"}\n",
        "  },\n",
        "  {\n",
        "    \"person\": {\"name\": \"Bob\", \"age\": \"25\"},\n",
        "    \"location\": {\"city\": \"Los Angeles\"}\n",
        "  }\n",
        "]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q79gFg5DOtlN"
      },
      "source": [
        "# Requirements\n",
        "\n",
        "## Explicit Requirements\n",
        "\n",
        "1. The Flask API must only accept `POST` requests with a file and format type.\n",
        "2. CSV rows must be converted to dictionaries.\n",
        "3. If the format is `nested`, headers with dot notation should form nested JSON objects.\n",
        "4. The module must handle:\n",
        "\n",
        "   * Empty files\n",
        "   * Inconsistent row lengths\n",
        "   * Invalid format values (`not in ['flat', 'nested']`)\n",
        "5. Error messages must be clear and returned in the response JSON.\n",
        "6. JSON keys should always be strings. Numeric values can be strings for consistency.\n",
        "\n",
        "\n",
        "## Implicit Requirements\n",
        "\n",
        "* The implementation must follow PEP8 and include docstrings for all functions and classes.\n",
        "* The module must be independently testable (i.e., functions should be decoupled from Flask).\n",
        "* File size should be limited to 2MB and validated in the Flask layer.\n",
        "* Rows should be processed in a memory-efficient manner using generators if possible.\n",
        "* Flask should return a proper 400 or 500 HTTP status code in case of errors.\n",
        "* Only standard Python libraries and Flask should be used.\n",
        "\n",
        "\n",
        "## Function Signature(s) and/or Class Definition(s)\n",
        "\n",
        "You are expected to implement the following module structure:\n",
        "\n",
        "```python\n",
        "def parse_csv(file_stream: IO[str], format_type: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Parse the uploaded CSV file and return a list of JSON records.\n",
        "\n",
        "    :param file_stream: A text stream of the CSV file.\n",
        "    :param format_type: 'flat' or 'nested' to specify transformation style.\n",
        "    :return: List of dictionaries representing JSON rows.\n",
        "    :raises ValueError: If the format_type is invalid or CSV parsing fails.\n",
        "    \"\"\"\n",
        "```\n",
        "\n",
        "**`app.py`**:\n",
        "\n",
        "```python\n",
        "@app.route(\"/transform\", methods=[\"POST\"])\n",
        "def transform():\n",
        "    \"\"\"\n",
        "    Flask route to accept a CSV file and return a JSON response.\n",
        "    Validates input, delegates to the transformer module, and structures response.\n",
        "    \"\"\"\n",
        "```\n",
        "\n",
        "\n",
        "## Edge Cases\n",
        "\n",
        "* A CSV with no rows (just headers).\n",
        "* A CSV with missing values in some cells.\n",
        "* Headers that conflict when transformed into nested keys (e.g., `a.b`, `a`).\n",
        "* Non-UTF-8 encoded CSV files.\n",
        "* Header with illegal or empty characters.\n",
        "* Rows with more columns than headers or fewer columns than headers.\n",
        "* Repeated headers (should raise an error).\n",
        "* Missing `format` field or invalid value.\n",
        "\n",
        "\n",
        "## Constraints\n",
        "\n",
        "* Python version must be 3.7 or above.\n",
        "* Use only Flask and standard Python libraries (`csv`, `io`, `typing`, etc).\n",
        "* Maximum file size: 2 MB.\n",
        "* CSV delimiter is always a comma (`,`).\n",
        "* No external CSV parsing libraries like `pandas` allowed.\n",
        "* Nested transformation must support arbitrary depth via dot notation.\n",
        "* Response must not exceed 2 seconds for files with up to 10,000 rows.\n",
        "\n",
        "Write the solution in a single module/file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSg66GeEbDAT"
      },
      "outputs": [],
      "source": [
        "# code\n",
        "\n",
        "\"\"\"\n",
        "Flask RESTful API to transform uploaded CSV files into flat or nested\n",
        "JSON format. The transformation logic is encapsulated in helper functions\n",
        "within this module to preserve modularity.\n",
        "\"\"\"\n",
        "\n",
        "from flask import Flask, request, jsonify\n",
        "import csv\n",
        "import io\n",
        "from typing import List, Dict, Any, IO\n",
        "from werkzeug.exceptions import RequestEntityTooLarge\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Enforce 2MB upload limit\n",
        "app.config['MAX_CONTENT_LENGTH'] = 2 * 1024 * 1024\n",
        "\n",
        "\n",
        "@app.route(\"/transform\", methods=[\"POST\"])\n",
        "def transform():\n",
        "    \"\"\"\n",
        "    Flask route to accept a CSV file and return a JSON response.\n",
        "    Validates input, delegates to transformation logic, and returns a\n",
        "    structured JSON result.\n",
        "    \"\"\"\n",
        "    if 'file' not in request.files or 'format' not in request.form:\n",
        "        return _error_response(\n",
        "            \"Missing 'file' or 'format' parameter.\", 400\n",
        "        )\n",
        "\n",
        "    uploaded_file = request.files['file']\n",
        "    format_type = request.form['format']\n",
        "\n",
        "    try:\n",
        "        file_stream = io.TextIOWrapper(\n",
        "            uploaded_file.stream, encoding='utf-8'\n",
        "        )\n",
        "        data = parse_csv(file_stream, format_type)\n",
        "        return jsonify(success=True, data=data, error=None)\n",
        "\n",
        "    except UnicodeDecodeError:\n",
        "        return _error_response(\"CSV file must be UTF-8 encoded.\", 400)\n",
        "\n",
        "    except ValueError as ve:\n",
        "        return _error_response(str(ve), 400)\n",
        "\n",
        "    except RequestEntityTooLarge:\n",
        "        return _error_response(\"File too large, must be under 2MB.\", 413)\n",
        "\n",
        "    except Exception:\n",
        "        return _error_response(\n",
        "            \"An unexpected error occurred while processing the file.\", 500\n",
        "        )\n",
        "\n",
        "\n",
        "def parse_csv(file_stream: IO[str], format_type: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Parse the uploaded CSV file and return a list of JSON records.\n",
        "\n",
        "    :param file_stream: A text stream of the CSV file.\n",
        "    :param format_type: 'flat' or 'nested' to specify transformation style.\n",
        "    :return: List of dictionaries representing JSON rows.\n",
        "    :raises ValueError: If the format_type is invalid or CSV parsing fails.\n",
        "    \"\"\"\n",
        "    if format_type not in ('flat', 'nested'):\n",
        "        raise ValueError(\"Invalid format_type. Must be 'flat' or 'nested'.\")\n",
        "\n",
        "    try:\n",
        "        reader = csv.DictReader(file_stream)\n",
        "\n",
        "        if not reader.fieldnames:\n",
        "            raise ValueError(\"CSV file is empty or missing headers.\")\n",
        "\n",
        "        if len(set(reader.fieldnames)) != len(reader.fieldnames):\n",
        "            raise ValueError(\"CSV headers contain duplicates.\")\n",
        "\n",
        "        _validate_headers(reader.fieldnames)\n",
        "\n",
        "        data = []\n",
        "\n",
        "        for row_num, row in enumerate(reader, start=1):\n",
        "            if None in row:\n",
        "                raise ValueError(\n",
        "                    f\"Row {row_num} has more columns than headers.\"\n",
        "                )\n",
        "\n",
        "            if len(row) != len(reader.fieldnames):\n",
        "                raise ValueError(\n",
        "                    f\"Inconsistent column count in row {row_num}.\"\n",
        "                )\n",
        "\n",
        "            str_row = {k: str(v) for k, v in row.items()}\n",
        "\n",
        "            if format_type == 'flat':\n",
        "                data.append(str_row)\n",
        "            else:\n",
        "                nested = _to_nested_dict(str_row)\n",
        "                data.append(nested)\n",
        "\n",
        "        return data\n",
        "\n",
        "    except csv.Error as e:\n",
        "        raise ValueError(f\"CSV parsing error: {str(e)}\") from e\n",
        "\n",
        "\n",
        "def _validate_headers(headers: List[str]) -> None:\n",
        "    \"\"\"\n",
        "    Validate that headers are non-empty, legal, and non-conflicting.\n",
        "\n",
        "    :param headers: List of header strings from the CSV file.\n",
        "    :raises ValueError: If headers are invalid or conflicting.\n",
        "    \"\"\"\n",
        "    seen = set()\n",
        "\n",
        "    for header in headers:\n",
        "        if not header or header.strip() == \"\":\n",
        "            raise ValueError(\"Empty header found.\")\n",
        "\n",
        "        parts = header.split('.')\n",
        "        path = []\n",
        "\n",
        "        for part in parts:\n",
        "            path.append(part)\n",
        "            path_str = '.'.join(path)\n",
        "\n",
        "            if path_str in seen:\n",
        "                continue\n",
        "\n",
        "            if any(h == path_str for h in headers if h != header):\n",
        "                raise ValueError(\n",
        "                    f\"Conflicting nested key: '{path_str}' conflicts with \"\n",
        "                    f\"another header.\"\n",
        "                )\n",
        "\n",
        "            seen.add(path_str)\n",
        "\n",
        "\n",
        "def _to_nested_dict(flat_dict: Dict[str, str]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Convert a flat dictionary with dotted keys into a nested dictionary.\n",
        "\n",
        "    :param flat_dict: Dictionary with possibly dotted keys.\n",
        "    :return: Nested dictionary.\n",
        "    :raises ValueError: If there are key conflicts in nesting.\n",
        "    \"\"\"\n",
        "    nested = {}\n",
        "\n",
        "    for key, value in flat_dict.items():\n",
        "        parts = key.split('.')\n",
        "        sub_dict = nested\n",
        "\n",
        "        for part in parts[:-1]:\n",
        "            if part not in sub_dict:\n",
        "                sub_dict[part] = {}\n",
        "            elif not isinstance(sub_dict[part], dict):\n",
        "                raise ValueError(\n",
        "                    f\"Key conflict while nesting: '{key}' cannot be created.\"\n",
        "                )\n",
        "            sub_dict = sub_dict[part]\n",
        "\n",
        "        if parts[-1] in sub_dict:\n",
        "            raise ValueError(\n",
        "                f\"Duplicate nested key detected: '{key}'\"\n",
        "            )\n",
        "\n",
        "        sub_dict[parts[-1]] = value\n",
        "\n",
        "    return nested\n",
        "\n",
        "\n",
        "def _error_response(message: str, status_code: int):\n",
        "    \"\"\"\n",
        "    Helper function to format error JSON responses.\n",
        "\n",
        "    :param message: Error message to return.\n",
        "    :param status_code: HTTP status code.\n",
        "    :return: Flask response with JSON body.\n",
        "    \"\"\"\n",
        "    return (\n",
        "        jsonify(success=False, data=None, error=message),\n",
        "        status_code\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUlcq7ycbHYw"
      },
      "outputs": [],
      "source": [
        "# tests\n",
        "\"\"\"Unit tests for the CSVTransformer class.\"\"\"\n",
        "\n",
        "import unittest\n",
        "import tempfile\n",
        "import os\n",
        "from datetime import datetime, timezone\n",
        "from main import CSVTransformer\n",
        "\n",
        "\n",
        "class TestCSVTransformer(unittest.TestCase):\n",
        "    \"\"\"Test suite for the CSVTransformer class.\"\"\"\n",
        "\n",
        "    def setUp(self):\n",
        "        \"\"\"Prepare common schema and sample CSV content for tests.\"\"\"\n",
        "        self.schema = {\n",
        "            \"id\": {\"target_field\": \"user.id\", \"type\": int},\n",
        "            \"name\": {\"target_field\": \"user.name\", \"type\": str},\n",
        "            \"active\": {\"target_field\": \"user.active\", \"type\": bool},\n",
        "            \"created\": {\"target_field\": \"meta.created\", \"type\": datetime},\n",
        "        }\n",
        "\n",
        "        self.valid_csv = (\n",
        "            \"id,name,active,created\\n\"\n",
        "            \"1,Alice,true,2023-01-01T10:00:00Z\\n\"\n",
        "            \"2,Bob,false,2023-02-01T11:30:00Z\\n\"\n",
        "        )\n",
        "\n",
        "    def _write_temp_file(self, content):\n",
        "        \"\"\"Write content to a temporary file and return the file path.\"\"\"\n",
        "        f = tempfile.NamedTemporaryFile(mode=\"w+\", delete=False)\n",
        "        f.write(content)\n",
        "        f.close()\n",
        "        return f.name\n",
        "\n",
        "    def tearDown(self):\n",
        "        \"\"\"Clean up temporary files created during the tests.\"\"\"\n",
        "        for f in getattr(self, \"_files_to_cleanup\", []):\n",
        "            os.unlink(f)\n",
        "\n",
        "    def _cleanup_file(self, path):\n",
        "        \"\"\"Track a file for cleanup after the test.\"\"\"\n",
        "        if not hasattr(self, \"_files_to_cleanup\"):\n",
        "            self._files_to_cleanup = []\n",
        "        self._files_to_cleanup.append(path)\n",
        "\n",
        "    def test_flat_mode_transform(self):\n",
        "        \"\"\"Test transformation in 'flat' mode.\"\"\"\n",
        "        path = self._write_temp_file(self.valid_csv)\n",
        "        self._cleanup_file(path)\n",
        "        transformer = CSVTransformer(self.schema, mode=\"flat\")\n",
        "        result = transformer.transform(path)\n",
        "        self.assertEqual(len(result), 2)\n",
        "        self.assertIn(\"user.id\", result[0])\n",
        "        self.assertIn(\"meta.created\", result[0])\n",
        "\n",
        "    def test_nested_mode_transform(self):\n",
        "        \"\"\"Test transformation in 'nested' mode.\"\"\"\n",
        "        path = self._write_temp_file(self.valid_csv)\n",
        "        self._cleanup_file(path)\n",
        "        transformer = CSVTransformer(self.schema, mode=\"nested\")\n",
        "        result = transformer.transform(path)\n",
        "        self.assertEqual(result[0][\"user\"][\"id\"], 1)\n",
        "\n",
        "    def test_grouped_mode(self):\n",
        "        \"\"\"Test grouping rows by a column in 'grouped' mode.\"\"\"\n",
        "        schema = {\n",
        "            \"name\": {\"target_field\": \"name\", \"type\": str},\n",
        "            \"id\": {\"target_field\": \"id\", \"type\": int},\n",
        "        }\n",
        "        csv = \"id,name\\n1,Alice\\n2,Alice\\n3,Bob\\n\"\n",
        "        path = self._write_temp_file(csv)\n",
        "        self._cleanup_file(path)\n",
        "        transformer = CSVTransformer(schema, mode=\"grouped\", group_by=\"name\")\n",
        "        result = transformer.transform(path)\n",
        "        self.assertEqual(len(result[\"Alice\"]), 2)\n",
        "        self.assertEqual(len(result[\"Bob\"]), 1)\n",
        "\n",
        "    def test_invalid_mode(self):\n",
        "        \"\"\"Test error raised for invalid mode.\"\"\"\n",
        "        with self.assertRaises(ValueError):\n",
        "            CSVTransformer(self.schema, mode=\"invalid\")\n",
        "\n",
        "    def test_invalid_on_error(self):\n",
        "        \"\"\"Test error raised for invalid on_error mode.\"\"\"\n",
        "        with self.assertRaises(ValueError):\n",
        "            CSVTransformer(self.schema, on_error=\"warn\")\n",
        "\n",
        "    def test_grouped_missing_group_by(self):\n",
        "        \"\"\"Test error raised when 'group_by' is missing in grouped mode.\"\"\"\n",
        "        with self.assertRaises(ValueError):\n",
        "            CSVTransformer(self.schema, mode=\"grouped\")\n",
        "\n",
        "    def test_missing_required_column_raises(self):\n",
        "        \"\"\"Test that missing columns raise an error with on_error='raise'.\"\"\"\n",
        "        csv = \"id,created\\n1,2022-01-01T00:00:00Z\\n\"\n",
        "        path = self._write_temp_file(csv)\n",
        "        self._cleanup_file(path)\n",
        "        transformer = CSVTransformer(self.schema, on_error=\"raise\")\n",
        "        with self.assertRaises(ValueError):\n",
        "            transformer.transform(path)\n",
        "\n",
        "    def test_missing_required_column_skip(self):\n",
        "        \"\"\"Test that missing columns are skipped with on_error='skip'.\"\"\"\n",
        "        csv = \"id,created\\n1,2022-01-01T00:00:00Z\\n\"\n",
        "        path = self._write_temp_file(csv)\n",
        "        self._cleanup_file(path)\n",
        "        transformer = CSVTransformer(self.schema, on_error=\"skip\")\n",
        "        result = transformer.transform(path)\n",
        "        self.assertEqual(len(result), 1)\n",
        "\n",
        "    def test_casting_error_raise(self):\n",
        "        \"\"\"Test that casting errors raise an exception.\"\"\"\n",
        "        csv = \"id,name,active,created\\nX,Alice,true,invalid\\n\"\n",
        "        path = self._write_temp_file(csv)\n",
        "        self._cleanup_file(path)\n",
        "        transformer = CSVTransformer(self.schema, on_error=\"raise\")\n",
        "        with self.assertRaises(Exception):\n",
        "            transformer.transform(path)\n",
        "\n",
        "    def test_casting_error_skip(self):\n",
        "        \"\"\"Test that casting errors are skipped with on_error='skip'.\"\"\"\n",
        "        csv = \"id,name,active,created\\n1,Alice,,2023-01-01T00:00:00Z\\n\"\n",
        "        path = self._write_temp_file(csv)\n",
        "        self._cleanup_file(path)\n",
        "        transformer = CSVTransformer(self.schema, mode=\"flat\", on_error=\"skip\")\n",
        "        result = transformer.transform(path)\n",
        "        expected = [{\n",
        "            \"user.id\": 1,\n",
        "            \"user.name\": \"Alice\",\n",
        "            \"meta.created\": datetime(2023, 1, 1, 0, 0, tzinfo=timezone.utc),\n",
        "        }]\n",
        "        self.assertEqual(result, expected)\n",
        "\n",
        "    def test_empty_csv_returns_empty_list(self):\n",
        "        \"\"\"Test that an empty CSV returns an empty list.\"\"\"\n",
        "        path = self._write_temp_file(\"\")\n",
        "        self._cleanup_file(path)\n",
        "        transformer = CSVTransformer(self.schema, mode=\"flat\")\n",
        "        result = transformer.transform(path)\n",
        "        self.assertEqual(result, [])\n",
        "\n",
        "    def test_exceed_max_columns(self):\n",
        "        \"\"\"Test error raised when column count exceeds maximum allowed.\"\"\"\n",
        "        header = \",\".join([f\"col{i}\" for i in range(101)])\n",
        "        path = self._write_temp_file(header + \"\\n\")\n",
        "        self._cleanup_file(path)\n",
        "        transformer = CSVTransformer({}, mode=\"flat\")\n",
        "        with self.assertRaises(ValueError):\n",
        "            transformer.transform(path)\n",
        "\n",
        "    def test_schema_duplicate_target_field(self):\n",
        "        \"\"\"Test schema validation for duplicate target_field entries.\"\"\"\n",
        "        schema = {\n",
        "            \"id\": {\"target_field\": \"user.id\", \"type\": int},\n",
        "            \"uid\": {\"target_field\": \"user.id\", \"type\": int},\n",
        "        }\n",
        "        with self.assertRaises(ValueError):\n",
        "            CSVTransformer(schema)\n",
        "\n",
        "    def test_schema_exceeds_nesting(self):\n",
        "        \"\"\"Test error for schema field path exceeding nesting limit.\"\"\"\n",
        "        schema = {\n",
        "            \"id\": {\"target_field\": \"a.b.c.d.e.f\", \"type\": int},\n",
        "        }\n",
        "        with self.assertRaises(ValueError):\n",
        "            CSVTransformer(schema)\n",
        "\n",
        "    def test_cast_type_bool(self):\n",
        "        \"\"\"Test casting string values to bool.\"\"\"\n",
        "        t = CSVTransformer({})\n",
        "        self.assertTrue(t._cast_type(\"True\", bool))\n",
        "        self.assertFalse(t._cast_type(\"0\", bool))\n",
        "\n",
        "    def test_cast_type_invalid_bool(self):\n",
        "        \"\"\"Test casting invalid bool string raises error.\"\"\"\n",
        "        t = CSVTransformer({}, on_error=\"raise\")\n",
        "        with self.assertRaises(ValueError):\n",
        "            t._cast_type(\"maybe\", bool)\n",
        "\n",
        "    def test_cast_type_int(self):\n",
        "        \"\"\"Test casting string to integer.\"\"\"\n",
        "        t = CSVTransformer({})\n",
        "        self.assertEqual(t._cast_type(\"123\", int), 123)\n",
        "\n",
        "    def test_cast_type_datetime(self):\n",
        "        \"\"\"Test parsing ISO 8601 datetime string.\"\"\"\n",
        "        t = CSVTransformer({})\n",
        "        dt = t._cast_type(\"2023-01-01T10:00:00Z\", datetime)\n",
        "        self.assertIsInstance(dt, datetime)\n",
        "\n",
        "    def test_cast_type_unsupported_type(self):\n",
        "        \"\"\"Test error raised for unsupported type casting.\"\"\"\n",
        "        t = CSVTransformer({}, on_error=\"raise\")\n",
        "        with self.assertRaises(TypeError):\n",
        "            t._cast_type(\"{}\", dict)\n",
        "\n",
        "    def test_flatten_dict(self):\n",
        "        \"\"\"Test flattening of a nested dictionary.\"\"\"\n",
        "        t = CSVTransformer({})\n",
        "        nested = {\"a\": {\"b\": {\"c\": 1}}}\n",
        "        flat = t._flatten_dict(nested)\n",
        "        self.assertEqual(flat[\"a.b.c\"], 1)\n",
        "\n",
        "    def test_construct_mapped_field(self):\n",
        "        \"\"\"Test nested field assignment using dot-separated path.\"\"\"\n",
        "        t = CSVTransformer({})\n",
        "        d = {}\n",
        "        t._construct_mapped_field(d, \"a.b.c\", 42)\n",
        "        self.assertEqual(d[\"a\"][\"b\"][\"c\"], 42)\n",
        "\n",
        "    def test_extract_group_key(self):\n",
        "        \"\"\"Test extraction of group_by key from nested structure.\"\"\"\n",
        "        t = CSVTransformer({}, mode=\"grouped\", group_by=\"a.b\")\n",
        "        row = {\"a\": {\"b\": \"x\"}}\n",
        "        self.assertEqual(t._extract_group_key(row), \"x\")\n",
        "\n",
        "    def test_extract_group_key_missing(self):\n",
        "        \"\"\"Test missing group_by key returns None.\"\"\"\n",
        "        t = CSVTransformer({}, mode=\"grouped\", group_by=\"a.b\")\n",
        "        row = {\"a\": {\"c\": \"x\"}}\n",
        "        self.assertIsNone(t._extract_group_key(row))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxZs2ehLYWkm"
      },
      "source": [
        "# Model Breaking Proof\n",
        "\n",
        "#### Model Breaking Task URL: https://rlhf-v3.turing.com/prompt/3d6ef542-d380-4505-a09d-5ef6bedf10d2?origin=https%3A%2F%2Flabeling-ms.turing.com&redirect_url=https%3A%2F%2Flabeling-ms.turing.com%2Fconversations%2F74836%2Fview\n",
        "\n",
        "#### Model code:\n",
        "\n",
        "```python\n",
        "from flask import Flask, request, jsonify\n",
        "import csv\n",
        "import io\n",
        "from typing import List, Dict, Any, IO\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "def parse_csv(file_stream: IO[str], format_type: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Parse the uploaded CSV file and return a list of JSON records.\n",
        "\n",
        "    :param file_stream: A text stream of the CSV file.\n",
        "    :param format_type: 'flat' or 'nested' to specify transformation style.\n",
        "    :return: List of dictionaries representing JSON rows.\n",
        "    :raises ValueError: If the format_type is invalid or CSV parsing fails.\n",
        "    \"\"\"\n",
        "    # Read the CSV file into a DictReader\n",
        "    try:\n",
        "        reader = csv.DictReader(file_stream) # <Issue>: The code does not validate reader.fieldnames for duplicates, which may lead to overwriting or undefined behavior.\n",
        "        if format_type not in ('flat', 'nested'):\n",
        "            raise ValueError(f\"Invalid format_type {format_type}. Must be 'flat' or 'nested'.\")\n",
        "        \n",
        "        data = []\n",
        "        \n",
        "        for row in reader: # <Issue>: csv.DictReader silently pads/truncates rows. The code fails to detect rows with more or fewer values than headers.\n",
        "            if format_type == 'flat':\n",
        "                data.append(row) # <Issue>: Values from csv.DictReader may remain as native types (e.g., integers, floats). This violates the requirement that all values in the JSON must be strings.\n",
        "            else:  # format_type == 'nested'\n",
        "                nested_row = {}\n",
        "                for key, value in row.items():\n",
        "                    keys = key.split('.')\n",
        "                    sub_dict = nested_row\n",
        "                    for part in keys[:-1]:\n",
        "                        if part not in sub_dict:\n",
        "                            sub_dict[part] = {}\n",
        "                        sub_dict = sub_dict[part] # <Issue>: The code blindly inserts into nested structures without verifying if the current key already exists as a non-dict value, which could lead to incorrect nesting or crashes.\n",
        "                    sub_dict[keys[-1]] = value\n",
        "                data.append(nested_row)\n",
        "\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Error processing CSV: {str(e)}\")\n",
        "\n",
        "@app.route(\"/transform\", methods=[\"POST\"])\n",
        "def transform():\n",
        "    \"\"\"\n",
        "    Flask route to accept a CSV file and return a JSON response.\n",
        "    Validates input, delegates to the transformer module, and structures response.\n",
        "    \"\"\"\n",
        "    if 'file' not in request.files or 'format' not in request.form:\n",
        "        return jsonify(success=False, data=None, error=\"Missing file or format parameter\"), 400\n",
        "\n",
        "    file = request.files['file']\n",
        "    format_type = request.form['format']\n",
        "\n",
        "    # Limit file size to 2MB\n",
        "    file.stream.seek(0, io.SEEK_END)\n",
        "    file_length = file.stream.tell()\n",
        "    file.stream.seek(0)\n",
        "    if file_length > 2 * 1024 * 1024:\n",
        "        return jsonify(success=False, data=None, error=\"File too large, must be less than 2MB\"), 400\n",
        "\n",
        "    try:\n",
        "        file_stream = io.TextIOWrapper(file.stream, encoding='utf-8') # <Issue>: If decoding fails, the generic except Exception block will catch it and return \"An unexpected error occurred\", violating the requirement for clear error messages.\n",
        "        data = parse_csv(file_stream, format_type)\n",
        "        return jsonify(success=True, data=data, error=None)\n",
        "    except ValueError as e:\n",
        "        return jsonify(success=False, data=None, error=str(e)), 400\n",
        "    except Exception as e:\n",
        "        return jsonify(success=False, data=None, error=\"An unexpected error occurred\"), 500\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(debug=True)\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}