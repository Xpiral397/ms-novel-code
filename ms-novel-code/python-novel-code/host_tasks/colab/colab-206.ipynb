{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKe-t1pIOo2f"
      },
      "source": [
        "# Metadata\n",
        "\n",
        "**L1 Taxonomy** - Data Analysis and Engineering\n",
        "\n",
        "**L2 Taxonomy** - Pandas for DataFrames\n",
        "\n",
        "**Subtopic** - Handling missing data by imputation or dropping in DataFrames\n",
        "\n",
        "**Use Case** - Create a Python module that uses Pandas to read a CSV file into a DataFrame. Identify columns with missing data and apply imputation techniques to fill the missing values. The imputation method could be mean, median, mode, or a constant value. In addition, provide an option to drop the rows or columns with missing data based on a threshold of missing values set by the user.\n",
        "\n",
        "**Programming Language** - Python\n",
        "\n",
        "**Target Model** - GPT-4o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS0xHSaZoEJO"
      },
      "source": [
        "# Setup\n",
        "\n",
        "```requirements.txt\n",
        "pandas\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YToTLlRqOqmj"
      },
      "source": [
        "# Prompt\n",
        "I need to create a Python module that uses Pandas to read a CSV file into a DataFrame. My module must first identify all columns that contain missing data. Then, apply different imputation techniques per column. Log every imputation and drop step in detail.\n",
        "\n",
        "**Input Format**\n",
        "\n",
        "- A string file path to CSV file.\n",
        "- A dictionary called imputation_rules where keys are column names and values are imputation strategies (\"mean\", \"median\", \"mode\").\n",
        "- A float threshold representing the maximum allowable fraction of missing values in any column or row.\n",
        "\n",
        "**Output Format**\n",
        "\n",
        "- A cleaned pandas DataFrame.\n",
        "- A list of strings as a log detailing each imputation or drop action in order.\n",
        "- DataFrame saved to CSV with name cleaned_<original_filename>.csv\n",
        "\n",
        "**Examples**\n",
        "\n",
        "```\n",
        "Input:\n",
        "filepath = \"data.csv\"\n",
        "imputation_rules = {\n",
        "    \"age\": \"mean\",\n",
        "    \"income\": \"median\",\n",
        "    \"gender\": \"mode\"\n",
        "}\n",
        "threshold = 0.25\n",
        "\n",
        "Output:\n",
        "(DataFrame object with missing values handled)\n",
        "[\n",
        "    \"Column age: filled 10 missing values with mean=36.4\",\n",
        "    \"Column income: filled 5 missing values with median=70500.0\",\n",
        "    \"Column gender: filled 3 missing values with mode='F'\",\n",
        "    \"Dropped column 'device_id' due to >90% uniqueness\",\n",
        "    \"Dropped row 42 due to missing data > threshold=0.25\",\n",
        "    \"Saved cleaned DataFrame to 'cleaned_data.csv'\"\n",
        "]\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q79gFg5DOtlN"
      },
      "source": [
        "# Requirements\n",
        "\n",
        "**Explicit and Implicit Points**\n",
        "\n",
        "- Use pandas to read and process CSV data.\n",
        "- Apply imputation on columns with missing data.\n",
        "- Support only \"mean\", \"median\", \"mode\"\n",
        "- Drop rows or columns after imputation if their NaN ratio exceeds threshold.\n",
        "\n",
        "\n",
        "**Solution Expectations**\n",
        "\n",
        "- Ensure type-safe imputation: no mean/median on strings, no mode on high-cardinality columns.\n",
        "- All columns with high-cardinality should be dropped.\n",
        "- After all operations the final data will be saved to CSV with name cleaned_<original_filename>.csv\n",
        "\n",
        "\n",
        "\n",
        "**Function Signatures**\n",
        "\n",
        "```python\n",
        "def clean_and_impute(filepath: str, imputation_rules: dict, threshold: float) -> tuple[pd.DataFrame, list[str]]:\n",
        "    pass\n",
        "```\n",
        "\n",
        "**Edge Case Behavior**\n",
        "\n",
        "- If file is empty or only contains headers, return empty DataFrame and empty log.\n",
        "- If column is entirely NaN, drop it and log it.\n",
        "- If a rule points to a non-existent column, skip and log warning.\n",
        "- If no missing values are found, return and log that nothing was changed.\n",
        "- If threshold is out of bound return and log that incorrect threshold was\n",
        "invalid.\n",
        "- If dropping a high-cardinality column log it.\n",
        "\n",
        "**Constraints**\n",
        "\n",
        "- Use only pandas as external library.\n",
        "- If more than 90% of values in column are unique that will be considered high-cardinality column.\n",
        "- Threshold is a float between 0 and 1 inclusive.\n",
        "- Do not modify the original CSV file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSg66GeEbDAT",
        "outputId": "2793f12e-df5e-4f0d-aaeb-53a426e1977c"
      },
      "outputs": [],
      "source": [
        "# code\n",
        "\n",
        "\"\"\"Module for cleaning and imputing missing data in a CSV file.\"\"\"\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def clean_and_impute(\n",
        "    filepath: str,\n",
        "    imputation_rules: dict,\n",
        "    threshold: float\n",
        ") -> tuple[pd.DataFrame, list[str]]:\n",
        "    \"\"\"\n",
        "    Clean and impute missing values in the given CSV file.\n",
        "\n",
        "    Args:\n",
        "        filepath (str): Path to the CSV file.\n",
        "        imputation_rules (dict): Dictionary mapping column names to\n",
        "            imputation strategies (\"mean\", \"median\", or \"mode\").\n",
        "        threshold (float): Max allowed missing ratio for rows/columns.\n",
        "\n",
        "    Returns:\n",
        "        tuple[pd.DataFrame, list[str]]: Cleaned DataFrame and a log list.\n",
        "    \"\"\"\n",
        "    log = []\n",
        "\n",
        "    if threshold < 0 or threshold > 1:\n",
        "        log.append(\n",
        "            \"Invalid threshold value: must be between 0 and 1 inclusive.\"\n",
        "        )\n",
        "        return pd.DataFrame(), log\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(filepath)\n",
        "    except Exception as e:\n",
        "        log.append(f\"Failed to read CSV: {e}\")\n",
        "        return pd.DataFrame(), log\n",
        "\n",
        "    if df.empty:\n",
        "        log.append(\"CSV is empty or contains only headers.\")\n",
        "        return df, log\n",
        "\n",
        "    changes_made = False\n",
        "\n",
        "    for col, strategy in imputation_rules.items():\n",
        "        if col not in df.columns:\n",
        "            log.append(\n",
        "                f\"Skipped non-existent column '{col}' for imputation\"\n",
        "            )\n",
        "            continue\n",
        "\n",
        "        if df[col].isnull().sum() == 0:\n",
        "            continue\n",
        "\n",
        "        if strategy not in [\"mean\", \"median\", \"mode\"]:\n",
        "            log.append(\n",
        "                f\"Invalid imputation strategy '{strategy}' for column '{col}'\"\n",
        "            )\n",
        "            continue\n",
        "\n",
        "        if df[col].isnull().all():\n",
        "            continue\n",
        "\n",
        "        if (\n",
        "            not pd.api.types.is_numeric_dtype(df[col])\n",
        "            and strategy in [\"mean\", \"median\"]\n",
        "        ):\n",
        "            log.append(\n",
        "                f\"Cannot apply '{strategy}' imputation on \"\n",
        "                f\"non-numeric column '{col}'\"\n",
        "            )\n",
        "            continue\n",
        "\n",
        "        filled_count = df[col].isnull().sum()\n",
        "\n",
        "        if strategy == \"mean\":\n",
        "            mean_value = df[col].mean()\n",
        "            df[col].fillna(mean_value, inplace=True)\n",
        "            log.append(\n",
        "                f\"Column {col}: filled {filled_count} missing values \"\n",
        "                f\"with mean={mean_value}\"\n",
        "            )\n",
        "            changes_made = True\n",
        "\n",
        "        elif strategy == \"median\":\n",
        "            median_value = df[col].median()\n",
        "            df[col].fillna(median_value, inplace=True)\n",
        "            log.append(\n",
        "                f\"Column {col}: filled {filled_count} missing values \"\n",
        "                f\"with median={median_value}\"\n",
        "            )\n",
        "            changes_made = True\n",
        "\n",
        "        elif strategy == \"mode\":\n",
        "            if df[col].nunique() / len(df) > 0.9:\n",
        "                log.append(\n",
        "                    f\"Cannot apply mode imputation on high-cardinality \"\n",
        "                    f\"column '{col}'\"\n",
        "                )\n",
        "                continue\n",
        "\n",
        "            if not df[col].mode().empty:\n",
        "                mode_value = df[col].mode().iloc[0]\n",
        "                df[col].fillna(mode_value, inplace=True)\n",
        "                log.append(\n",
        "                    f\"Column {col}: filled {filled_count} missing values \"\n",
        "                    f\"with mode='{mode_value}'\"\n",
        "                )\n",
        "                changes_made = True\n",
        "\n",
        "    cols_to_drop = []\n",
        "    for col in df.columns:\n",
        "        if df[col].nunique() / len(df) > 0.9:\n",
        "            cols_to_drop.append(col)\n",
        "            log.append(f\"Dropped column '{col}' due to >90% uniqueness\")\n",
        "            changes_made = True\n",
        "\n",
        "    df.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "    entirely_nan_cols = df.columns[df.isnull().all()]\n",
        "    for col in entirely_nan_cols:\n",
        "        df.drop(columns=[col], inplace=True)\n",
        "        log.append(f\"Dropped column '{col}' due to being entirely NaN\")\n",
        "        changes_made = True\n",
        "\n",
        "    for col in df.columns[df.isnull().mean() > threshold]:\n",
        "        df.drop(columns=[col], inplace=True)\n",
        "        log.append(\n",
        "            f\"Dropped column '{col}' due to missing data > threshold=\"\n",
        "            f\"{threshold}\"\n",
        "        )\n",
        "        changes_made = True\n",
        "\n",
        "    rows_to_drop = df.index[df.isnull().mean(axis=1) > threshold]\n",
        "    for index in rows_to_drop:\n",
        "        df.drop(index=index, inplace=True)\n",
        "        log.append(\n",
        "            f\"Dropped row {index} due to missing data > threshold=\"\n",
        "            f\"{threshold}\"\n",
        "        )\n",
        "        changes_made = True\n",
        "\n",
        "    if not changes_made and not any(df.isnull().sum() > 0):\n",
        "        log.append(\"No missing values found, nothing was changed\")\n",
        "\n",
        "    base_filename = os.path.basename(filepath)\n",
        "    new_filename = f\"cleaned_{base_filename}\"\n",
        "    df.to_csv(new_filename, index=False)\n",
        "    log.append(f\"Saved cleaned DataFrame to '{new_filename}'\")\n",
        "\n",
        "    return df, log\n",
        "\n",
        "\n",
        "imputation_rules = {\n",
        "    \"age\": \"mean\",\n",
        "    \"income\": \"median\",\n",
        "    \"gender\": \"mode\",\n",
        "    \"city\": \"mode\",\n",
        "    \"empty_col\": \"mode\"\n",
        "}\n",
        "\n",
        "clean_and_impute('test.csv', imputation_rules, 0.3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KUlcq7ycbHYw",
        "outputId": "4fa7ce0e-57a3-4576-a76f-8c11b5ea4b53"
      },
      "outputs": [],
      "source": [
        "# tests\n",
        "\n",
        "\"\"\"\n",
        "Unit tests for the clean_and_impute function.\n",
        "\"\"\"\n",
        "import os\n",
        "import tempfile\n",
        "import unittest\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from main import clean_and_impute\n",
        "\n",
        "\n",
        "class TestCleanAndImpute(unittest.TestCase):\n",
        "    \"\"\"\n",
        "    Test suite for the clean_and_impute function.\n",
        "    \"\"\"\n",
        "\n",
        "    def setUp(self) -> None:\n",
        "        \"\"\"\n",
        "        Create a temporary directory and switch to it before each test.\n",
        "        \"\"\"\n",
        "        self.tempdir = tempfile.TemporaryDirectory()\n",
        "        self.original_cwd = os.getcwd()\n",
        "        os.chdir(self.tempdir.name)\n",
        "\n",
        "    def tearDown(self) -> None:\n",
        "        \"\"\"\n",
        "        Restore the original directory, clean temporary dir after each test.\n",
        "        \"\"\"\n",
        "        os.chdir(self.original_cwd)\n",
        "        self.tempdir.cleanup()\n",
        "\n",
        "    def write_csv(self, filename: str, df: pd.DataFrame) -> None:\n",
        "        \"\"\"\n",
        "        Write the given DataFrame to a CSV file without an index.\n",
        "\n",
        "        Args:\n",
        "            filename: The name of the CSV file to write.\n",
        "            df: The DataFrame to save.\n",
        "        \"\"\"\n",
        "        df.to_csv(filename, index=False)\n",
        "\n",
        "    def test_threshold_out_of_bounds(self) -> None:\n",
        "        \"\"\"\n",
        "        Threshold outside [0, 1] must return an empty DataFrame & log an error.\n",
        "        \"\"\"\n",
        "        df, log = clean_and_impute('dummy.csv', {}, -0.1)\n",
        "        self.assertTrue(df.empty)\n",
        "        self.assertIn('Invalid threshold value', log[0])\n",
        "\n",
        "        df, log = clean_and_impute('dummy.csv', {}, 1.5)\n",
        "        self.assertTrue(df.empty)\n",
        "        self.assertIn('Invalid threshold value', log[0])\n",
        "\n",
        "    def test_file_not_found(self) -> None:\n",
        "        \"\"\"\n",
        "        A missing file should return an empty DataFrame and log a read failure.\n",
        "        \"\"\"\n",
        "        df, log = clean_and_impute('nofile.csv', {'col': 'mean'}, 0.5)\n",
        "        self.assertTrue(df.empty)\n",
        "        self.assertIn('Failed to read CSV', log[0])\n",
        "\n",
        "    def test_empty_csv(self) -> None:\n",
        "        \"\"\"\n",
        "        CSV with only headers should return empty DataFrame & log the condition.\n",
        "        \"\"\"\n",
        "        df_input = pd.DataFrame(columns=['a', 'b'])\n",
        "        self.write_csv('empty.csv', df_input)\n",
        "        df, log = clean_and_impute('empty.csv', {}, 0.5)\n",
        "        self.assertTrue(df.empty)\n",
        "        self.assertIn('CSV is empty or contains only headers', log[0])\n",
        "\n",
        "    def test_skip_nonexistent_column_rule(self) -> None:\n",
        "        \"\"\"\n",
        "          Absent of imputation rules for columns in CSV must be skipped with a\n",
        "          warning log.\n",
        "        \"\"\"\n",
        "        df_input = pd.DataFrame({'x': [1, None, 3]})\n",
        "        self.write_csv('data.csv', df_input)\n",
        "        df, log = clean_and_impute('data.csv', {'y': 'mean'}, 0.5)\n",
        "        self.assertIn(\"Skipped non-existent column 'y'\", log[0])\n",
        "\n",
        "    def test_mean_and_median_imputation(self) -> None:\n",
        "        \"\"\"\n",
        "        Numeric columns should be correctly imputed using mean and median\n",
        "        strategies.\n",
        "        \"\"\"\n",
        "        df_input = pd.DataFrame({\n",
        "            'a': [1, None, 3],\n",
        "            'b': [None, 2, 4]\n",
        "        })\n",
        "        self.write_csv('in.csv', df_input)\n",
        "        df, log = clean_and_impute('in.csv', {'a': 'mean', 'b': 'median'}, 1.0)\n",
        "        self.assertAlmostEqual(df.loc[1, 'a'], 2)\n",
        "        self.assertAlmostEqual(df.loc[0, 'b'], 2)\n",
        "        self.assertIn('Column a: filled 1 missing values with mean=', log[0])\n",
        "        self.assertIn('Column b: filled 1 missing values with median=', log[1])\n",
        "\n",
        "    def test_mode_imputation_and_high_cardinality(self) -> None:\n",
        "        \"\"\"\n",
        "        Mode imputation should fill missing values, and high-cardinality columns\n",
        "         should be dropped.\n",
        "        \"\"\"\n",
        "        df_input = pd.DataFrame({\n",
        "            'c': ['x', None, 'x', 'y', 'z',\n",
        "                  'w', 'v', 'u', 't', 's']\n",
        "        })\n",
        "        self.write_csv('mode.csv', df_input)\n",
        "        df, log = clean_and_impute('mode.csv', {'c': 'mode'}, 1.0)\n",
        "        self.assertEqual(df.loc[1, 'c'], 'x')\n",
        "        self.assertTrue(any('Dropped column' in entry for entry in log))\n",
        "\n",
        "    def test_drop_entirely_nan_column(self) -> None:\n",
        "        \"\"\"\n",
        "        Columns that are entirely NaN should be dropped and logged.\n",
        "        \"\"\"\n",
        "        df_input = pd.DataFrame({\n",
        "            'd': [None, None, None],\n",
        "            'e': [1, 2, 3]\n",
        "        })\n",
        "        self.write_csv('nan.csv', df_input)\n",
        "        df, log = clean_and_impute('nan.csv', {'d': 'mode', 'e': 'mean'}, 1.0)\n",
        "        self.assertNotIn('d', df.columns)\n",
        "        self.assertIn(\"Dropped column 'd' due to being entirely NaN\", log)\n",
        "\n",
        "    def test_drop_columns_over_threshold(self) -> None:\n",
        "        \"\"\"\n",
        "        Columns with missing-value fraction above the threshold should be dropped.\n",
        "        \"\"\"\n",
        "        df_input = pd.DataFrame({\n",
        "            'f': [1, None, None, 4],\n",
        "            'g': [1, 2, 3, 4]\n",
        "        })\n",
        "        self.write_csv('thr.csv', df_input)\n",
        "        df, log = clean_and_impute('thr.csv', {'f': 'mean', 'g': 'mean'}, 0.25)\n",
        "        self.assertNotIn('f', df.columns)\n",
        "        self.assertIn(\n",
        "            \"Dropped column 'f' due to missing data > threshold=0.25\",\n",
        "            log\n",
        "        )\n",
        "\n",
        "    def test_drop_rows_over_threshold(self) -> None:\n",
        "        \"\"\"\n",
        "        Rows with missing-value fraction above the threshold should be dropped.\n",
        "        \"\"\"\n",
        "        df_input = pd.DataFrame({\n",
        "            'h': [1, None, 3],\n",
        "            'i': [None, None, None]\n",
        "        })\n",
        "        self.write_csv('rows.csv', df_input)\n",
        "        df, log = clean_and_impute('rows.csv', {'h': 'mean', 'i': 'mode'}, 0.5)\n",
        "        self.assertNotIn(1, df.index)\n",
        "        self.assertIn(\n",
        "            'Dropped row 1 due to missing data > threshold=0.5',\n",
        "            log\n",
        "        )\n",
        "\n",
        "    def test_no_missing_values(self) -> None:\n",
        "        \"\"\"\n",
        "        When no missing values exist, log that nothing was changed.\n",
        "        \"\"\"\n",
        "        df_input = pd.DataFrame({\n",
        "            'j': [1, 2, 3],\n",
        "            'k': ['a', 'b', 'c']\n",
        "        })\n",
        "        self.write_csv('nomiss.csv', df_input)\n",
        "        df, log = clean_and_impute('nomiss.csv', {}, 0.5)\n",
        "        self.assertIn(\n",
        "            'No missing values found, nothing was changed',\n",
        "            log\n",
        "        )\n",
        "\n",
        "    def test_output_file_created(self) -> None:\n",
        "        \"\"\"\n",
        "        Ensure the cleaned CSV file is saved and the final log entry is correct.\n",
        "        \"\"\"\n",
        "        df_input = pd.DataFrame({\n",
        "            'l': [1, None],\n",
        "            'm': ['x', None]\n",
        "        })\n",
        "        self.write_csv('out.csv', df_input)\n",
        "        _, log = clean_and_impute('out.csv', {'l': 'mean', 'm': 'mode'}, 1.0)\n",
        "        self.assertTrue(os.path.exists('cleaned_out.csv'))\n",
        "        self.assertIn(\n",
        "            \"Saved cleaned DataFrame to 'cleaned_out.csv'\",\n",
        "            log[-1]\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    unittest.main(argv=[''])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxZs2ehLYWkm"
      },
      "source": [
        "# Model Breaking Proof\n",
        "\n",
        "#### Model Breaking Task URL: <https://rlhf-v3.turing.com/prompt/a930830d-b9cf-45c1-be4d-c24736d990f5?origin=https%3A%2F%2Flabeling-ms.turing.com&redirect_url=https%3A%2F%2Flabeling-ms.turing.com%2Fconversations%2F74355%2Fview>\n",
        "\n",
        "#### Model code:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "# <Issue>: <Using numpy, which is not allowed>\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def clean_and_impute(filepath: str, imputation_rules: dict, threshold: float) -> tuple[pd.DataFrame, list[str]]:\n",
        "    log = []\n",
        "    \n",
        "    # Validate threshold\n",
        "    if threshold < 0 or threshold > 1:\n",
        "        log.append(\"Invalid threshold value: must be between 0 and 1 inclusive.\")\n",
        "        return pd.DataFrame(), log\n",
        "    \n",
        "    # Read CSV into DataFrame\n",
        "    try:\n",
        "        df = pd.read_csv(filepath)\n",
        "    except Exception as e:\n",
        "        log.append(f\"Failed to read CSV: {e}\")\n",
        "        return pd.DataFrame(), log\n",
        "    # <Issue>: <Edge case not handled for \"If no missing values are found, return and log that nothing was changed\">\n",
        "\n",
        "    # Edge case: Empty file or only headers\n",
        "    if df.empty:\n",
        "        log.append(\"CSV is empty or contains only headers.\")\n",
        "        return df, log\n",
        "    \n",
        "    # Identify high-cardinality columns to drop\n",
        "    cols_to_drop = []\n",
        "    for col in df.columns:\n",
        "        if df[col].nunique() / len(df) > 0.9:\n",
        "            cols_to_drop.append(col)\n",
        "            log.append(f\"Dropped column '{col}' due to >90% uniqueness\")\n",
        "    \n",
        "    # Drop high-cardinality columns\n",
        "    df.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "    # Handle imputation\n",
        "    for col, strategy in imputation_rules.items():\n",
        "        if col not in df.columns:\n",
        "            log.append(f\"Skipped non-existent column '{col}' for imputation\")\n",
        "            continue\n",
        "\n",
        "            # <Issue>: <Missing check and handling for fully NaN columns>\n",
        "\n",
        "        if df[col].isnull().sum() == 0:\n",
        "            continue  # No missing data in this column\n",
        "\n",
        "        if strategy not in [\"mean\", \"median\", \"mode\"]:\n",
        "            log.append(f\"Invalid imputation strategy '{strategy}' for column '{col}'\")\n",
        "            continue\n",
        "\n",
        "        if df[col].dtype not in [np.float64, np.int64] and strategy in [\"mean\", \"median\"]:\n",
        "            log.append(f\"Cannot apply '{strategy}' imputation on non-numeric column '{col}'\")\n",
        "            continue\n",
        "        \n",
        "        # <Issue>: <filled_count is calculated after fillna(),which is not correct, it should be before to log correctly>\n",
        "        \n",
        "        # Impute based on strategy\n",
        "        if strategy == \"mean\":\n",
        "            mean_value = df[col].mean()\n",
        "            filled_count = df[col].isnull().sum()\n",
        "            df[col].fillna(mean_value, inplace=True)\n",
        "            log.append(f\"Column {col}: filled {filled_count} missing values with mean={mean_value}\")\n",
        "        elif strategy == \"median\":\n",
        "            median_value = df[col].median()\n",
        "            filled_count = df[col].isnull().sum()\n",
        "            df[col].fillna(median_value, inplace=True)\n",
        "            log.append(f\"Column {col}: filled {filled_count} missing values with median={median_value}\")\n",
        "        elif strategy == \"mode\":\n",
        "            mode_value = df[col].mode().iloc[0]\n",
        "            filled_count = df[col].isnull().sum()\n",
        "            df[col].fillna(mode_value, inplace=True)\n",
        "            log.append(f\"Column {col}: filled {filled_count} missing values with mode='{mode_value}'\")\n",
        "\n",
        "    # <Issue>: <Missing log if no changes were made>\n",
        "\n",
        "    # Drop any columns or rows with remaining high NaN ratios\n",
        "    for col in df.columns[df.isnull().mean() > threshold]:\n",
        "        df.drop(columns=[col], inplace=True)\n",
        "        log.append(f\"Dropped column '{col}' due to missing data > threshold={threshold}\")\n",
        "\n",
        "    for index in df.index[df.isnull().mean(axis=1) > threshold]:\n",
        "        df.drop(index=index, inplace=True)\n",
        "        log.append(f\"Dropped row {index} due to missing data > threshold={threshold}\")\n",
        "\n",
        "    # Save cleaned DataFrame to a new CSV\n",
        "    base_filename = os.path.basename(filepath)\n",
        "    new_filename = f\"cleaned_{base_filename}\"\n",
        "    df.to_csv(new_filename, index=False)\n",
        "    log.append(f\"Saved cleaned DataFrame to '{new_filename}'\")\n",
        "\n",
        "    return df, log\n",
        "\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
