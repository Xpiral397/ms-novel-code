{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKe-t1pIOo2f"
      },
      "source": [
        "# Metadata\n",
        "\n",
        "**L1 Taxonomy** - Computing Paradigms\n",
        "\n",
        "**L2 Taxonomy** - Procedural Programming\n",
        "\n",
        "**Subtopic** - LLM-inspired kernel weighted regression algorithm\n",
        "\n",
        "**Use Case** - Design a Python module to implement a kernel-based weighted regression algorithm inspired by research examples. Develop a procedural function that computes a weight matrix using kernel functions and iteratively updates regression coefficients via numpy’s vectorized operations. Integrate error handling and efficient data processing to facilitate economic modeling and predictive analytics in real-world settings fileciteturn0file19.\n",
        "\n",
        "**Programming Language** - Python\n",
        "\n",
        "**Target Model** - GPT-4o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ1goTnQsWPe"
      },
      "source": [
        "# Model Breaking Hints\n",
        "\n",
        "\n",
        "1) **What is the initial use case?**\n",
        "\n",
        "The initial problem involves designing a Python module to implement a kernel-based weighted regression algorithm. It requires developing a procedural function that computes a weight matrix using kernel functions and iteratively updates regression coefficients with NumPy's vectorized operations. The goal is to facilitate economic modeling and predictive analytics in real-world settings by integrating error handling and efficient data processing.\n",
        "\n",
        "2) **Why is the initial use case easy?**\n",
        "\n",
        "The initial problem is relatively straightforward because it involves standard techniques in regression analysis and kernel methods using well-established libraries like NumPy. The tasks of computing a weight matrix with kernel functions and iteratively updating coefficients are common in statistical programming. Additionally, integrating error handling and efficient data processing are standard best practices and do not introduce significant complexity or unconventional challenges.\n",
        "\n",
        "3) **How could we make it harder?**\n",
        "\n",
        "To significantly increase the complexity, we can integrate several advanced concepts:\n",
        "\n",
        "- **Graph Algorithms**: Model data points as nodes in a graph, computing weights using centrality measures like PageRank, which introduces complexity in understanding and implementing graph structures and algorithms.\n",
        "  \n",
        "- **Multi-Objective Optimization**: Introduce conflicting constraints in optimizing regression coefficients, requiring complex optimization techniques like Min-Cost Max-Flow, adding layers of mathematical and computational difficulty.\n",
        "  \n",
        "- **Distributed Computing and Consensus Algorithms**: Implement the regression algorithm in a distributed environment, necessitating synchronization across nodes using consensus algorithms like Raft, which involves understanding distributed systems and fault tolerance.\n",
        "  \n",
        "- **Advanced Data Structures**: Use KD-Trees for efficient nearest-neighbor searches in high-dimensional, sparse data, increasing the complexity of data handling and kernel computations.\n",
        "  \n",
        "- **Dynamic Programming over Tree Decompositions**: Apply dynamic programming techniques over tree structures to efficiently update regression coefficients, adding algorithmic complexity and requiring multi-step logical reasoning.\n",
        "\n",
        "By combining these elements, we create a problem that requires deep understanding across multiple advanced topics in computer science and mathematics.\n",
        "\n",
        "4) **Which parameters can we change?**\n",
        "\n",
        "- **Data Representation**: Transform the data into a dynamically evolving graph structure instead of a flat dataset.\n",
        "  \n",
        "- **Weight Computation**: Replace simple kernel functions with graph-based centrality measures, such as those from PageRank algorithms.\n",
        "  \n",
        "- **Optimization Constraints**: Introduce multi-objective optimization with conflicting constraints, requiring advanced algorithms like Min-Cost Max-Flow.\n",
        "  \n",
        "- **Computing Environment**: Shift from a single-machine implementation to a distributed system, requiring the use of consensus algorithms like Raft to manage state and ensure data consistency.\n",
        "  \n",
        "- **Data Structures**: Utilize advanced structures like KD-Trees to handle high-dimensional, sparse data efficiently.\n",
        "  \n",
        "- **Algorithmic Techniques**: Incorporate dynamic programming over tree decompositions to optimize the updating of regression coefficients.\n",
        "\n",
        "By altering these parameters, we increase the problem's complexity, requiring knowledge of advanced algorithms, data structures, distributed computing, and multi-step reasoning.\n",
        "\n",
        "5) **What can be a final hard prompt?**\n",
        "\n",
        "\"Develop a Python module that performs distributed kernel-based weighted regression over a dynamically evolving graph of high-dimensional, sparse data points. Compute the weight matrix using centrality measures derived from PageRank algorithms on the graph, and optimize regression coefficients through multi-objective optimization satisfying conflicting constraints using Min-Cost Max-Flow techniques. Ensure data consistency and synchronization across distributed nodes using consensus algorithms like Raft, and utilize advanced data structures like KD-Trees for efficient nearest-neighbor searches and dynamic programming over tree decompositions to update regression coefficients efficiently.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS0xHSaZoEJO"
      },
      "source": [
        "# Setup\n",
        "\n",
        "```requirements.txt\n",
        "numpy==1.26.4\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YToTLlRqOqmj"
      },
      "source": [
        "# Prompt\n",
        "\n",
        "I want to build a Python module that performs kernel based weighted regression using a procedural programming approach. The module should run an iterative regression process where weights are computed using a specified kernel function and the regression coefficients are updated at each step using only NumPy's vectorized operations. The implementation must not use any object oriented programming style.\n",
        "\n",
        "**Input Format**\n",
        "\n",
        "The function must accept six inputs in this order:\n",
        "\n",
        "- X: a NumPy array with shape (number_of_rows, number_of_columns) representing the input data\n",
        "- y: a NumPy array with shape (number_of_rows,) representing the target outputs\n",
        "- kernel: a string that must be either gaussian, laplacian, or linear\n",
        "- bandwidth: a float greater than 0 that controls the kernel spread\n",
        "- max_iter: an integer greater than 0 for maximum number of update steps\n",
        "- tolerance: a float greater than or equal to 0 that decides the stopping threshold\n",
        "\n",
        "**Output Format**\n",
        "\n",
        "The function must return a tuple of two values:\n",
        "\n",
        "- A NumPy array of shape (number_of_columns,) containing the final regression coefficients\n",
        "- A NumPy array of shape (number_of_rows, number_of_rows) representing the symmetric weight matrix\n",
        "\n",
        "\n",
        "**Example**\n",
        "\n",
        "Input:\n",
        "\n",
        "```python\n",
        "X = np.array([[1], [2], [3]], dtype=float)\n",
        "y = np.array([2, 4, 6], dtype=float)\n",
        "kernel = \"gaussian\"\n",
        "bandwidth = 1.0\n",
        "max_iter = 5\n",
        "tolerance = 0.001\n",
        "```\n",
        "\n",
        "Expected Output:\n",
        "\n",
        "A tuple:\n",
        "\n",
        "- A NumPy array approximately equal to [2.0]\n",
        "- A 3 by 3 symmetric matrix with weights based on the gaussian kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q79gFg5DOtlN"
      },
      "source": [
        "# Requirements\n",
        "\n",
        "**Implicit and explicit instructions**\n",
        "\n",
        "- You must use only NumPy for all operations\n",
        "- You must compute a new weight matrix at each iteration using the current coefficients\n",
        "- You must stop early if the difference between two coefficient vectors is below the tolerance based on L2 norm\n",
        "- You must select and apply the correct kernel type using only if elif else blocks\n",
        "\n",
        "**Function Signature**\n",
        "\n",
        "```python\n",
        "def kernel_weighted_regression(X, y, kernel, bandwidth, max_iter, tolerance):\n",
        "```\n",
        "\n",
        "**Edge Case Behavior**\n",
        "\n",
        "1. If X is empty or has zero rows or columns, raise ValueError with message: \"X must not be empty\"\n",
        "2. If y contains any NaN, raise ValueError with message: \"y must not contain NaN\"\n",
        "3. If kernel is not one of the allowed values, raise ValueError with message: \"Unsupported kernel type\"\n",
        "4. If bandwidth is less than or equal to 0, raise ValueError with message: \"Bandwidth must be positive\"\n",
        "5. If max_iter is less than or equal to 0, raise ValueError with message: \"max_iter must be a positive integer\"\n",
        "6. If tolerance is less than 0, raise ValueError with message: \"tolerance must be non negative\"\n",
        "\n",
        "**Constraints**\n",
        "\n",
        "1. You must not use any loop such as for or while anywhere in the implementation.\n",
        "2. You must not use any external library other than numpy.\n",
        "3. You must not use any numpy function that performs matrix solve automatically like lstsq or pinv.\n",
        "4. You must not return until you verify that the final weight matrix is symmetric using np.allclose.\n",
        "5. You must raise ValueError with message \"Weight matrix must be symmetric\" if the matrix is not symmetric.\n",
        "6. You must round the final coefficient values to 4 decimal places using np.round before returning.\n",
        "7. You must check that X is 2D and y is 1D using assert and raise AssertionError with a custom message if not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSg66GeEbDAT"
      },
      "outputs": [],
      "source": [
        "# code\n",
        "\n",
        "\"\"\"This module performs kernel-based weighted regression using NumPy only.\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def kernel_weighted_regression(\n",
        "    X, y, kernel, bandwidth, max_iter, tolerance\n",
        "):\n",
        "    \"\"\"\n",
        "    Perform kernel-based weighted regression.\n",
        "\n",
        "    Args:\n",
        "        X (np.ndarray): 2D input array of shape (n_samples, n_features)\n",
        "        y (np.ndarray): 1D target array of shape (n_samples,)\n",
        "        kernel (str): One of 'gaussian', 'laplacian', or 'linear'\n",
        "        bandwidth (float): Positive kernel bandwidth\n",
        "        max_iter (int): Maximum number of iterations\n",
        "        tolerance (float): Non-negative L2 threshold for convergence\n",
        "\n",
        "    Returns:\n",
        "        tuple[np.ndarray, np.ndarray]:\n",
        "            - Final coefficients (rounded to 4 decimals)\n",
        "            - Final symmetric weight matrix\n",
        "    \"\"\"\n",
        "    assert X.ndim == 2, \"x must be a 2D array\"\n",
        "    assert y.ndim == 1, \"y must be a 1D array\"\n",
        "\n",
        "    if X.size == 0 or X.shape[0] == 0 or X.shape[1] == 0:\n",
        "        raise ValueError(\"X must not be empty\")\n",
        "    if np.any(np.isnan(X)):\n",
        "        raise ValueError(\"X must not contain NaN\")\n",
        "    if np.any(np.isnan(y)):\n",
        "        raise ValueError(\"y must not contain NaN\")\n",
        "    if kernel not in (\"gaussian\", \"laplacian\", \"linear\"):\n",
        "        raise ValueError(\"Unsupported kernel type\")\n",
        "    if bandwidth <= 0:\n",
        "        raise ValueError(\"Bandwidth must be positive\")\n",
        "    if max_iter <= 0:\n",
        "        raise ValueError(\"max_iter must be a positive integer\")\n",
        "    if tolerance < 0:\n",
        "        raise ValueError(\"tolerance must be non negative\")\n",
        "    if X.shape[0] != y.shape[0]:\n",
        "        raise ValueError(\"X and y must have the same number of rows\")\n",
        "\n",
        "    n_features = X.shape[1]\n",
        "    initial_coeffs = np.zeros(n_features)\n",
        "\n",
        "    coeffs, weight_matrix = _iterative_update(\n",
        "        X, y, initial_coeffs, kernel, bandwidth, max_iter, tolerance, 0\n",
        "    )\n",
        "\n",
        "    if not np.allclose(weight_matrix, weight_matrix.T):\n",
        "        raise ValueError(\"Weight matrix must be symmetric\")\n",
        "\n",
        "    coeffs = np.round(coeffs, 4)\n",
        "    return coeffs, weight_matrix\n",
        "\n",
        "\n",
        "def _iterative_update(\n",
        "    X, y, coeffs, kernel, bandwidth, max_iter, tolerance, step\n",
        "):\n",
        "    if step >= max_iter:\n",
        "        w = _compute_weight_matrix(X, kernel, bandwidth)\n",
        "        return coeffs, w\n",
        "\n",
        "    w = _compute_weight_matrix(X, kernel, bandwidth)\n",
        "    new_coeffs = _compute_weighted_coefficients(X, y, w)\n",
        "\n",
        "    if np.linalg.norm(new_coeffs - coeffs) < tolerance:\n",
        "        return new_coeffs, w\n",
        "\n",
        "    return _iterative_update(\n",
        "        X, y, new_coeffs, kernel, bandwidth, max_iter, tolerance, step + 1\n",
        "    )\n",
        "\n",
        "\n",
        "def _compute_weight_matrix(X, kernel, h):\n",
        "    diffs = X[:, None, :] - X[None, :, :]\n",
        "    dist = np.linalg.norm(diffs, axis=2)\n",
        "\n",
        "    if kernel == \"gaussian\":\n",
        "        w = np.exp(-dist**2 / (2 * h**2))\n",
        "    elif kernel == \"laplacian\":\n",
        "        w = np.exp(-dist / h)\n",
        "    else:  # linear\n",
        "        w = np.maximum(0, 1 - dist / h)\n",
        "\n",
        "    return (w + w.T) / 2\n",
        "\n",
        "\n",
        "def _compute_weighted_coefficients(X, y, w):\n",
        "    xtw = X.T @ w\n",
        "    xtwx = xtw @ X\n",
        "    xtwy = xtw @ y\n",
        "\n",
        "    reg = 1e-10 * np.eye(X.shape[1])\n",
        "    inv = np.linalg.inv(xtwx + reg)\n",
        "\n",
        "    coeffs = inv @ xtwy\n",
        "    return coeffs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUlcq7ycbHYw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e176b37-631f-4096-c99c-bd5682d6b1c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "..........................\n",
            "----------------------------------------------------------------------\n",
            "Ran 26 tests in 0.038s\n",
            "\n",
            "OK\n"
          ]
        }
      ],
      "source": [
        "# tests\n",
        "\n",
        "import unittest\n",
        "import numpy as np\n",
        "from main import kernel_weighted_regression\n",
        "\n",
        "\n",
        "class TestKernelWeightedRegression(unittest.TestCase):\n",
        "\n",
        "    def setUp(self):\n",
        "        self.X_basic = np.array([[1], [2], [3]])\n",
        "        self.y_basic = np.array([1, 2, 3])\n",
        "\n",
        "    def test_gaussian_basic(self):\n",
        "        coeffs, w = kernel_weighted_regression(self.X_basic, self.y_basic, \"gaussian\", 1.0, 100, 1e-6)\n",
        "        self.assertEqual(coeffs.shape, (1,))\n",
        "        self.assertTrue(np.allclose(w, w.T))\n",
        "\n",
        "    def test_laplacian_basic(self):\n",
        "        coeffs, w = kernel_weighted_regression(self.X_basic, self.y_basic, \"laplacian\", 1.0, 100, 1e-6)\n",
        "        self.assertEqual(coeffs.shape, (1,))\n",
        "        self.assertTrue(np.allclose(w, w.T))\n",
        "\n",
        "    def test_linear_basic(self):\n",
        "        coeffs, w = kernel_weighted_regression(self.X_basic, self.y_basic, \"linear\", 1.0, 100, 1e-6)\n",
        "        self.assertEqual(coeffs.shape, (1,))\n",
        "        self.assertTrue(np.allclose(w, w.T))\n",
        "\n",
        "    def test_zero_bandwidth(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            kernel_weighted_regression(self.X_basic, self.y_basic, \"gaussian\", 0, 100, 1e-6)\n",
        "\n",
        "    def test_negative_bandwidth(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            kernel_weighted_regression(self.X_basic, self.y_basic, \"gaussian\", -1, 100, 1e-6)\n",
        "\n",
        "    def test_invalid_kernel(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            kernel_weighted_regression(self.X_basic, self.y_basic, \"cosine\", 1.0, 100, 1e-6)\n",
        "\n",
        "    def test_nan_input_X(self):\n",
        "        X = self.X_basic.astype(float)\n",
        "        X[0, 0] = np.nan\n",
        "        with self.assertRaises(ValueError):\n",
        "            kernel_weighted_regression(X, self.y_basic, \"gaussian\", 1.0, 100, 1e-6)\n",
        "\n",
        "    def test_nan_input_y(self):\n",
        "        y = self.y_basic.astype(float)\n",
        "        y[1] = np.nan\n",
        "        with self.assertRaises(ValueError):\n",
        "            kernel_weighted_regression(self.X_basic, y, \"gaussian\", 1.0, 100, 1e-6)\n",
        "\n",
        "    def test_empty_X(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            kernel_weighted_regression(np.array([]).reshape(0, 0), np.array([]), \"gaussian\", 1.0, 100, 1e-6)\n",
        "\n",
        "    def test_incorrect_dim_X(self):\n",
        "        with self.assertRaises(AssertionError):\n",
        "            kernel_weighted_regression(np.array([1, 2, 3]), self.y_basic, \"gaussian\", 1.0, 100, 1e-6)\n",
        "\n",
        "    def test_incorrect_dim_y(self):\n",
        "        with self.assertRaises(AssertionError):\n",
        "            kernel_weighted_regression(self.X_basic, self.y_basic.reshape(-1, 1), \"gaussian\", 1.0, 100, 1e-6)\n",
        "\n",
        "    def test_mismatched_dimensions(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            kernel_weighted_regression(self.X_basic, np.array([1, 2]), \"gaussian\", 1.0, 100, 1e-6)\n",
        "\n",
        "    def test_tolerance_negative(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            kernel_weighted_regression(self.X_basic, self.y_basic, \"gaussian\", 1.0, 100, -0.1)\n",
        "\n",
        "    def test_zero_max_iter(self):\n",
        "        with self.assertRaises(ValueError):\n",
        "            kernel_weighted_regression(self.X_basic, self.y_basic, \"gaussian\", 1.0, 0, 1e-6)\n",
        "\n",
        "    def test_coeffs_rounding(self):\n",
        "        coeffs, _ = kernel_weighted_regression(self.X_basic, self.y_basic, \"gaussian\", 0.5, 100, 1e-6)\n",
        "        decimals = np.abs(coeffs * 10000 - np.round(coeffs * 10000))\n",
        "        self.assertTrue(np.all(decimals == 0))\n",
        "\n",
        "    def test_multiple_features(self):\n",
        "        X = np.array([[1, 2], [2, 3], [3, 4]])\n",
        "        y = np.array([1, 2, 3])\n",
        "        coeffs, _ = kernel_weighted_regression(X, y, \"laplacian\", 1.0, 100, 1e-6)\n",
        "        self.assertEqual(coeffs.shape, (2,))\n",
        "\n",
        "    def test_large_bandwidth(self):\n",
        "        coeffs, _ = kernel_weighted_regression(self.X_basic, self.y_basic, \"gaussian\", 100.0, 100, 1e-6)\n",
        "        self.assertEqual(coeffs.shape, (1,))\n",
        "\n",
        "    def test_small_bandwidth(self):\n",
        "        coeffs, _ = kernel_weighted_regression(self.X_basic, self.y_basic, \"laplacian\", 1e-6, 100, 1e-6)\n",
        "        self.assertEqual(coeffs.shape, (1,))\n",
        "\n",
        "    def test_convergence(self):\n",
        "        coeffs1, _ = kernel_weighted_regression(self.X_basic, self.y_basic, \"gaussian\", 1.0, 1, 1e-6)\n",
        "        coeffs2, _ = kernel_weighted_regression(self.X_basic, self.y_basic, \"gaussian\", 1.0, 100, 1e-6)\n",
        "        self.assertTrue(np.allclose(coeffs2, coeffs2))\n",
        "\n",
        "    def test_weight_matrix_shape(self):\n",
        "        _, w = kernel_weighted_regression(self.X_basic, self.y_basic, \"gaussian\", 1.0, 100, 1e-6)\n",
        "        self.assertEqual(w.shape, (3, 3))\n",
        "\n",
        "    def test_weight_matrix_symmetry(self):\n",
        "        _, w = kernel_weighted_regression(self.X_basic, self.y_basic, \"gaussian\", 1.0, 100, 1e-6)\n",
        "        self.assertTrue(np.allclose(w, w.T))\n",
        "\n",
        "    def test_output_values_stability(self):\n",
        "        coeffs1, _ = kernel_weighted_regression(self.X_basic, self.y_basic, \"linear\", 1.0, 100, 1e-6)\n",
        "        coeffs2, _ = kernel_weighted_regression(self.X_basic, self.y_basic, \"linear\", 1.0, 100, 1e-6)\n",
        "        self.assertTrue(np.allclose(coeffs1, coeffs2))\n",
        "\n",
        "    def test_high_dimensional_data(self):\n",
        "        X = np.random.rand(10, 5)\n",
        "        y = np.random.rand(10)\n",
        "        coeffs, w = kernel_weighted_regression(X, y, \"gaussian\", 1.0, 50, 1e-6)\n",
        "        self.assertEqual(coeffs.shape, (5,))\n",
        "        self.assertEqual(w.shape, (10, 10))\n",
        "\n",
        "    def test_large_dataset(self):\n",
        "        np.random.seed(0)\n",
        "        X = np.random.rand(50, 3)\n",
        "        y = np.random.rand(50)\n",
        "        coeffs, _ = kernel_weighted_regression(X, y, \"laplacian\", 0.8, 30, 1e-4)\n",
        "        self.assertEqual(len(coeffs), 3)\n",
        "\n",
        "    def test_all_same_input(self):\n",
        "        X = np.ones((5, 2))\n",
        "        y = np.ones(5)\n",
        "        coeffs, _ = kernel_weighted_regression(X, y, \"gaussian\", 1.0, 100, 1e-6)\n",
        "        self.assertTrue(np.allclose(coeffs, coeffs[0]))\n",
        "\n",
        "    def test_weight_matrix_nonzero(self):\n",
        "        _, w = kernel_weighted_regression(self.X_basic, self.y_basic, \"linear\", 0.1, 50, 1e-6)\n",
        "        self.assertFalse(np.all(w == 0))\n",
        "\n",
        "    def test_iterative_stops(self):\n",
        "        coeffs, _ = kernel_weighted_regression(self.X_basic, self.y_basic, \"gaussian\", 0.1, 1, 1e-6)\n",
        "        self.assertEqual(coeffs.shape, (1,))\n",
        "\n",
        "    def test_extreme_y(self):\n",
        "        y = np.array([1e10, -1e10, 1e10])\n",
        "        coeffs, _ = kernel_weighted_regression(self.X_basic, y, \"laplacian\", 1.0, 100, 1e-6)\n",
        "        self.assertTrue(np.isfinite(coeffs).all())\n",
        "\n",
        "    def test_weight_matrix_structure(self):\n",
        "        _, w = kernel_weighted_regression(self.X_basic, self.y_basic, \"gaussian\", 1.0, 10, 1e-4)\n",
        "        self.assertTrue(np.all(w >= 0) and np.all(w <= 1))\n",
        "\n",
        "    def test_return_type(self):\n",
        "        result = kernel_weighted_regression(self.X_basic, self.y_basic, \"linear\", 1.0, 10, 1e-6)\n",
        "        self.assertIsInstance(result, tuple)\n",
        "        self.assertIsInstance(result[0], np.ndarray)\n",
        "        self.assertIsInstance(result[1], np.ndarray)\n",
        "\n",
        "    def test_stress_random(self):\n",
        "        np.random.seed(42)\n",
        "        X = np.random.rand(20, 4)\n",
        "        y = np.random.rand(20)\n",
        "        coeffs, _ = kernel_weighted_regression(X, y, \"laplacian\", 0.5, 40, 1e-6)\n",
        "        self.assertEqual(coeffs.shape[0], 4)\n",
        "\n",
        "    def test_consistency_multiple_runs(self):\n",
        "        coeffs1, _ = kernel_weighted_regression(self.X_basic, self.y_basic, \"linear\", 0.5, 30, 1e-6)\n",
        "        coeffs2, _ = kernel_weighted_regression(self.X_basic, self.y_basic, \"linear\", 0.5, 30, 1e-6)\n",
        "        self.assertTrue(np.allclose(coeffs1, coeffs2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxZs2ehLYWkm"
      },
      "source": [
        "# Model Breaking Proof\n",
        "\n",
        "#### Model Breaking Task URL: <https://rlhf-v3.turing.com/prompt/9686f47a-c688-4e5f-85cf-969853000ae1?origin=https%3A%2F%2Flabeling-ms.turing.com&redirect_url=https%3A%2F%2Flabeling-ms.turing.com%2Fconversations%2F45629%2Fview>\n",
        "\n",
        "#### Model code:\n",
        "\n",
        "```python\n",
        "import numpy as np\n",
        "\n",
        "def kernel_weighted_regression(X, y, kernel, bandwidth, max_iter, tolerance):\n",
        "    # Input validation\n",
        "    if X.size == 0:\n",
        "        raise ValueError(\"X must not be empty\")\n",
        "    if np.isnan(y).any():\n",
        "        raise ValueError(\"y must not contain NaN\")\n",
        "    if kernel not in ['gaussian', 'laplacian', 'linear']:\n",
        "        raise ValueError(\"Unsupported kernel type\")\n",
        "    if bandwidth <= 0:\n",
        "        raise ValueError(\"Bandwidth must be positive\")\n",
        "    if max_iter <= 0:\n",
        "        raise ValueError(\"max_iter must be a positive integer\")\n",
        "    if tolerance < 0:\n",
        "        raise ValueError(\"tolerance must be non negative\")\n",
        "    \n",
        "    # Assert shape for correctness\n",
        "    assert X.ndim == 2, \"X must be a 2D array\"\n",
        "    assert y.ndim == 1, \"y must be a 1D array\"\n",
        "  \n",
        "    n_samples, n_features = X.shape\n",
        "    \n",
        "    # Initialize coefficients\n",
        "    beta = np.zeros(n_features)\n",
        "    \n",
        "    # Iterate to update weights and coefficients\n",
        "\n",
        "     # <Issue>: <Loop is used despite prompt stating must not use any loop such as for or while>\n",
        "    for iteration in range(max_iter):\n",
        "        # Compute prediction\n",
        "        y_pred = X @ beta\n",
        "        \n",
        "        # Compute residuals\n",
        "        residuals = y - y_pred\n",
        "        \n",
        "        # Compute the distance matrix\n",
        "        diff_matrix = residuals[:, None]\n",
        "        distances = np.abs(diff_matrix - diff_matrix.T)\n",
        "        \n",
        "        # Compute weights based on kernel type\n",
        "        if kernel == 'gaussian':\n",
        "            weights = np.exp(-(distances**2) / (2 * bandwidth**2))\n",
        "        elif kernel == 'laplacian':\n",
        "            weights = np.exp(-distances / bandwidth)\n",
        "        elif kernel == 'linear':\n",
        "            weights = np.maximum(0, 1 - distances / bandwidth)\n",
        "        \n",
        "        # Prepare the diagonal weight matrix\n",
        "        W = np.diag(np.sum(weights, axis=1))\n",
        "        \n",
        "        # Update coefficients\n",
        "        XtWX = X.T @ W @ X\n",
        "        XtWy = X.T @ W @ y\n",
        "\n",
        "        # <Issue>: <np.linalg.solve is explicitly disallowed in prompt, 'must not use lstsq or solve'>\n",
        "        \n",
        "        # Solve for new beta using vectorized operation\n",
        "        new_beta = np.linalg.solve(XtWX, XtWy)\n",
        "        \n",
        "        # Check for convergence using L2 norm\n",
        "        if np.linalg.norm(new_beta - beta, ord=2) < tolerance:\n",
        "            beta = new_beta\n",
        "            break\n",
        "        \n",
        "        # Update beta\n",
        "        beta = new_beta\n",
        "    \n",
        "    # Check for symmetry of the final weight matrix\n",
        "    if not np.allclose(weights, weights.T):\n",
        "        raise ValueError(\"Weight matrix must be symmetric\")\n",
        "    \n",
        "    # Round beta to 4 decimal places\n",
        "    beta = np.round(beta, 4)\n",
        "    \n",
        "    return beta, weights\n",
        "\n",
        "# Example usage:\n",
        "X = np.array([[1], [2], [3]], dtype=float)\n",
        "y = np.array([2, 4, 6], dtype=float)\n",
        "kernel = \"gaussian\"\n",
        "bandwidth = 1.0\n",
        "max_iter = 5\n",
        "tolerance = 0.001\n",
        "\n",
        "beta, weights = kernel_weighted_regression(X, y, kernel, bandwidth, max_iter, tolerance)\n",
        "print(f\"Coefficients: {beta}\")\n",
        "print(f\"Weight Matrix: {weights}\")\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}