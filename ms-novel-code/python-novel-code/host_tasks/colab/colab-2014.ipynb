{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKe-t1pIOo2f"
      },
      "source": [
        "# Metadata\n",
        "\n",
        "**L1 Taxonomy** - Backend Development\n",
        "\n",
        "**L2 Taxonomy** - API Development\n",
        "\n",
        "**Subtopic** - Building a RESTful API with Flask\n",
        "\n",
        "**Use Case** - Develop a Python module that reads a CSV file and transforms it into a JSON object\n",
        "\n",
        "**Programming Language** - Python\n",
        "\n",
        "**Target Model** - GPT-4o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS0xHSaZoEJO"
      },
      "source": [
        "# Setup\n",
        "\n",
        "```requirements.txt\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YToTLlRqOqmj"
      },
      "source": [
        "# Prompt\n",
        "\n",
        "## **Problem Overview**\n",
        "\n",
        "You are tasked with building a robust and extensible Python module named `CSVTransformer` that reads a CSV file and transforms its content into a structured JSON object. This transformation must support:\n",
        "\n",
        "1. **Schema Mapping**: Ability to convert field names using a provided mapping.\n",
        "2. **Data Typing**: Enforce type conversion based on the mapped schema.\n",
        "3. **Mode Handling**: Support different transformation modes like:\n",
        "\n",
        "   * `flat`: Each CSV row becomes a standalone JSON object.\n",
        "   * `grouped`: Rows are grouped by a specified field.\n",
        "   * `nested`: Nested objects are created based on column prefixes.\n",
        "4. **Error Handling**: Gracefully handle bad rows, missing columns, and type mismatches with options to skip, log, or raise.\n",
        "\n",
        "This module should work as a standalone component that can be easily integrated into larger data pipelines.\n",
        "\n",
        "\n",
        "## **Input Format**\n",
        "\n",
        "* A **CSV file** with headers in the first row. The values may be strings, numbers, booleans, or ISO8601-formatted timestamps.\n",
        "* A **schema mapping dictionary** (`Dict[str, Dict]`) specifying:\n",
        "\n",
        "  * `target_field`: the new name of the column\n",
        "  * `type`: the expected Python type (`str`, `int`, `float`, `bool`, `datetime`)\n",
        "* A **mode string** (`str`) from: `\"flat\"`, `\"grouped\"`, `\"nested\"`\n",
        "* Optional parameters:\n",
        "\n",
        "  * `group_by`: required only in `grouped` mode\n",
        "  * `delimiter`: defaults to `,`\n",
        "  * `on_error`: one of `\"raise\"`, `\"skip\"`, or `\"log\"`\n",
        "\n",
        "\n",
        "## **Output Format**\n",
        "\n",
        "* A **JSON object** (`Dict` or `List[Dict]`) representing the transformed data.\n",
        "\n",
        "  * For `flat` mode: a list of JSON objects\n",
        "  * For `grouped` mode: a dictionary mapping `group_by` values to lists of objects\n",
        "  * For `nested` mode: a list of JSON objects with nested structures based on prefixes in headers (e.g., `address.street` becomes nested under `address`)\n",
        "\n",
        "\n",
        "## **Examples**\n",
        "\n",
        "### Input CSV (`users.csv`):\n",
        "\n",
        "```\n",
        "id,name,age,address.street,address.city,active\n",
        "1,Alice,30,123 Main St,Metropolis,true\n",
        "2,Bob,25,456 Elm St,Gotham,false\n",
        "```\n",
        "\n",
        "### Schema Mapping:\n",
        "\n",
        "```python\n",
        "{\n",
        "  \"id\": {\"target_field\": \"userId\", \"type\": int},\n",
        "  \"name\": {\"target_field\": \"fullName\", \"type\": str},\n",
        "  \"age\": {\"target_field\": \"age\", \"type\": int},\n",
        "  \"address.street\": {\"target_field\": \"address.street\", \"type\": str},\n",
        "  \"address.city\": {\"target_field\": \"address.city\", \"type\": str},\n",
        "  \"active\": {\"target_field\": \"isActive\", \"type\": bool}\n",
        "}\n",
        "```\n",
        "\n",
        "### Mode: `\"nested\"`\n",
        "\n",
        "### Output:\n",
        "\n",
        "```json\n",
        "[\n",
        "  {\n",
        "    \"userId\": 1,\n",
        "    \"fullName\": \"Alice\",\n",
        "    \"age\": 30,\n",
        "    \"address\": {\n",
        "      \"street\": \"123 Main St\",\n",
        "      \"city\": \"Metropolis\"\n",
        "    },\n",
        "    \"isActive\": true\n",
        "  },\n",
        "  {\n",
        "    \"userId\": 2,\n",
        "    \"fullName\": \"Bob\",\n",
        "    \"age\": 25,\n",
        "    \"address\": {\n",
        "      \"street\": \"456 Elm St\",\n",
        "      \"city\": \"Gotham\"\n",
        "    },\n",
        "    \"isActive\": false\n",
        "  }\n",
        "]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q79gFg5DOtlN"
      },
      "source": [
        "# Requirements\n",
        "\n",
        "\n",
        "## **Explicit Requirements**\n",
        "\n",
        "* The module must expose a public interface to:\n",
        "\n",
        "  * Load CSV content from a file path\n",
        "  * Accept and validate schema mapping\n",
        "  * Choose transformation mode (`flat`, `grouped`, or `nested`)\n",
        "  * Handle errors based on `on_error` setting\n",
        "* Output should be a Python object (not written to file)\n",
        "* Columns not in the schema must be ignored\n",
        "\n",
        "\n",
        "## **Implicit Requirements**\n",
        "\n",
        "* The module should be written in a way that is testable and reusable.\n",
        "* Use `datetime.fromisoformat()` or `dateutil.parser.parse()` to handle datetime parsing.\n",
        "* Booleans must recognize `\"true\"`, `\"false\"`, `\"1\"`, `\"0\"` (case-insensitive).\n",
        "* Trim whitespace from CSV field values before type conversion.\n",
        "* Must support Unicode characters in values and field names.\n",
        "\n",
        "\n",
        "## **Function Signature(s) and/or Class Definition(s)**\n",
        "\n",
        "```python\n",
        "class CSVTransformer:\n",
        "    def __init__(self, schema: Dict[str, Dict], mode: str = \"flat\", group_by: Optional[str] = None,\n",
        "                 delimiter: str = \",\", on_error: str = \"raise\"):\n",
        "        ...\n",
        "\n",
        "    def transform(self, csv_path: str) -> Union[List[Dict], Dict[str, List[Dict]]]:\n",
        "        \"\"\"Transforms the CSV at the given path into JSON based on the configuration.\"\"\"\n",
        "        ...\n",
        "```\n",
        "\n",
        "\n",
        "## **Edge Cases**\n",
        "\n",
        "* Empty CSV: Return empty list or empty dict depending on mode\n",
        "* Missing required columns: Raise or log based on `on_error`\n",
        "* Extra columns not in schema: Ignore them silently\n",
        "* Duplicate group\\_by values in `grouped` mode: Append to the group list\n",
        "* Nested field names like `meta.info.level` should produce multi-level nesting\n",
        "\n",
        "\n",
        "## **Constraints**\n",
        "\n",
        "* Must be implemented using only Python 3.7+ standard library (no Pandas)\n",
        "* Should not read the entire file into memory for large CSVs; process line-by-line\n",
        "* Schema must be valid before transformation begins\n",
        "* Maximum CSV size supported: 100 MB\n",
        "* Maximum number of columns: 100\n",
        "* Nested fields should not exceed depth of 5 levels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSg66GeEbDAT"
      },
      "outputs": [],
      "source": [
        "# code\n",
        "\n",
        "\"\"\"CSVTransformer: A Python class for transforming CSV files into JSON objects\n",
        "with schema mapping, type enforcement, and mode-based output formatting.\"\"\"\n",
        "\n",
        "import csv\n",
        "import logging\n",
        "import os\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Optional, Union\n",
        "\n",
        "try:\n",
        "    from dateutil.parser import parse as iso_parse\n",
        "except ImportError:\n",
        "    iso_parse = None\n",
        "\n",
        "\n",
        "class CSVTransformer:\n",
        "    \"\"\"\n",
        "    A class to transform CSV files into JSON objects with schema mapping,\n",
        "    type enforcement, and mode-based output formatting.\n",
        "\n",
        "    Supports 'flat', 'grouped', and 'nested' modes with error handling\n",
        "    strategies and constraints on input size and structure.\n",
        "    \"\"\"\n",
        "\n",
        "    MAX_COLUMNS = 100\n",
        "    MAX_FILE_SIZE_BYTES = 100 * 1024 * 1024  # 100 MB\n",
        "    MAX_NESTING_DEPTH = 5\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        schema: Dict[str, Dict],\n",
        "        mode: str = \"flat\",\n",
        "        group_by: Optional[str] = None,\n",
        "        delimiter: str = \",\",\n",
        "        on_error: str = \"raise\",\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the transformer.\n",
        "\n",
        "        Args:\n",
        "            schema (Dict[str, Dict]): Mapping of CSV fields to output fields\n",
        "                and expected types.\n",
        "            mode (str): One of \"flat\", \"grouped\", \"nested\".\n",
        "            group_by (Optional[str]): Field name to group by (for 'grouped' mode).\n",
        "            delimiter (str): CSV delimiter character.\n",
        "            on_error (str): Error handling mode: \"raise\", \"skip\", or \"log\".\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If `mode` or `on_error` is invalid, or if required\n",
        "                parameters are missing.\n",
        "        \"\"\"\n",
        "        valid_modes = {\"flat\", \"grouped\", \"nested\"}\n",
        "        if mode not in valid_modes:\n",
        "            raise ValueError(f\"Invalid mode '{mode}'. Must be one of {valid_modes}.\")\n",
        "\n",
        "        if on_error not in {\"raise\", \"skip\", \"log\"}:\n",
        "            raise ValueError(\"on_error must be one of 'raise', 'skip', or 'log'.\")\n",
        "\n",
        "        if mode == \"grouped\" and not group_by:\n",
        "            raise ValueError(\"`group_by` must be provided when mode is 'grouped'.\")\n",
        "\n",
        "        self.schema = schema\n",
        "        self.mode = mode\n",
        "        self.group_by = group_by\n",
        "        self.delimiter = delimiter\n",
        "        self.on_error = on_error\n",
        "\n",
        "        self._validate_schema()\n",
        "\n",
        "    def _validate_schema(self):\n",
        "        \"\"\"\n",
        "        Validate the schema before processing.\n",
        "\n",
        "        Ensures target_field and type presence, nesting depth limits,\n",
        "        and no duplicate target_field paths.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If schema is invalid.\n",
        "        \"\"\"\n",
        "        target_paths = set()\n",
        "        for field, mapping in self.schema.items():\n",
        "            if \"target_field\" not in mapping or \"type\" not in mapping:\n",
        "                raise ValueError(\n",
        "                    f\"Schema mapping for '{field}' must include \"\n",
        "                    \"'target_field' and 'type'.\"\n",
        "                )\n",
        "            path = mapping[\"target_field\"].split(\".\")\n",
        "            if len(path) > self.MAX_NESTING_DEPTH:\n",
        "                raise ValueError(\n",
        "                    f\"Nested target_field '{mapping['target_field']}' exceeds \"\n",
        "                    f\"max depth {self.MAX_NESTING_DEPTH}.\"\n",
        "                )\n",
        "            if mapping[\"target_field\"] in target_paths:\n",
        "                raise ValueError(\n",
        "                    f\"Duplicate target_field '{mapping['target_field']}' in schema.\"\n",
        "                )\n",
        "            target_paths.add(mapping[\"target_field\"])\n",
        "\n",
        "    def _check_file_constraints(self, file_path: str):\n",
        "        \"\"\"\n",
        "        Check file size and column count constraints.\n",
        "\n",
        "        Args:\n",
        "            file_path (str): Path to the CSV file.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If file size or column count exceeds limits.\n",
        "        \"\"\"\n",
        "        file_size = os.path.getsize(file_path)\n",
        "        if file_size > self.MAX_FILE_SIZE_BYTES:\n",
        "            raise ValueError(\n",
        "                f\"CSV file size {file_size} exceeds maximum allowed \"\n",
        "                f\"{self.MAX_FILE_SIZE_BYTES} bytes.\"\n",
        "            )\n",
        "\n",
        "        with open(file_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "            reader = csv.reader(csvfile, delimiter=self.delimiter)\n",
        "            try:\n",
        "                header = next(reader)\n",
        "            except StopIteration:\n",
        "                # Empty file: no columns\n",
        "                return\n",
        "            if len(header) > self.MAX_COLUMNS:\n",
        "                raise ValueError(\n",
        "                    f\"CSV has {len(header)} columns which exceeds the max \"\n",
        "                    f\"allowed {self.MAX_COLUMNS}.\"\n",
        "                )\n",
        "\n",
        "    def transform(self, csv_path: str) -> Union[List[Dict], Dict[str, List[Dict]]]:\n",
        "        \"\"\"\n",
        "        Transform the CSV file content to JSON object(s) per configuration.\n",
        "\n",
        "        Returns flattened dicts for 'flat' mode,\n",
        "        nested dicts for 'nested' mode,\n",
        "        and grouped dict for 'grouped' mode.\n",
        "\n",
        "        Args:\n",
        "            csv_path (str): Path to the CSV file.\n",
        "\n",
        "        Returns:\n",
        "            Union[List[Dict], Dict[str, List[Dict]]]: Transformed JSON data.\n",
        "\n",
        "        Raises:\n",
        "            FileNotFoundError: If the CSV file does not exist.\n",
        "            ValueError: If file constraints or schema validation fail.\n",
        "        \"\"\"\n",
        "        self._check_file_constraints(csv_path)\n",
        "\n",
        "        json_output = []\n",
        "\n",
        "        try:\n",
        "            with open(csv_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n",
        "                csv_reader = csv.DictReader(csvfile, delimiter=self.delimiter)\n",
        "                if csv_reader.fieldnames is None:\n",
        "                    return [] if self.mode != \"grouped\" else {}\n",
        "\n",
        "                missing_cols = [\n",
        "                    col for col in self.schema if col not in csv_reader.fieldnames\n",
        "                ]\n",
        "                if missing_cols:\n",
        "                    msg = (\n",
        "                        f\"CSV file is missing required columns: {missing_cols}.\"\n",
        "                    )\n",
        "                    if self.on_error == \"raise\":\n",
        "                        raise ValueError(msg)\n",
        "                    elif self.on_error == \"log\":\n",
        "                        logging.error(msg)\n",
        "\n",
        "                for row in csv_reader:\n",
        "                    try:\n",
        "                        transformed_row = self._process_row(row)\n",
        "                        if transformed_row is not None:\n",
        "                            if self.mode == \"flat\":\n",
        "                                transformed_row = self._flatten_dict(transformed_row)\n",
        "                            json_output.append(transformed_row)\n",
        "                    except Exception as e:\n",
        "                        if self.on_error == \"raise\":\n",
        "                            raise\n",
        "                        elif self.on_error == \"log\":\n",
        "                            logging.error(f\"Error processing row {row}: {e}\")\n",
        "                        if self.on_error in {\"skip\", \"log\"}:\n",
        "                            continue\n",
        "\n",
        "            if self.mode == \"grouped\":\n",
        "                return self._group_rows(json_output)\n",
        "            else:\n",
        "                return json_output\n",
        "\n",
        "        except FileNotFoundError as e:\n",
        "            raise FileNotFoundError(f\"CSV file not found at path '{csv_path}'.\") from e\n",
        "\n",
        "    def _process_row(self, row: Dict[str, str]) -> Optional[Dict]:\n",
        "        \"\"\"\n",
        "        Process a single CSV row and apply schema mapping and type casting.\n",
        "\n",
        "        Args:\n",
        "            row (Dict[str, str]): A CSV row dictionary.\n",
        "\n",
        "        Returns:\n",
        "            Optional[Dict]: Transformed row or None if invalid/skipped.\n",
        "\n",
        "        Raises:\n",
        "            KeyError: If required field is missing and on_error='raise'.\n",
        "            ValueError, TypeError: If type casting fails and on_error='raise'.\n",
        "        \"\"\"\n",
        "        transformed_row = {}\n",
        "\n",
        "        for field_name, mapping in self.schema.items():\n",
        "            raw_value = row.get(field_name)\n",
        "\n",
        "            if raw_value is None or raw_value.strip() == \"\":\n",
        "                if self.on_error == \"raise\":\n",
        "                    raise KeyError(f\"Missing or empty column '{field_name}' in row.\")\n",
        "                else:\n",
        "                    continue\n",
        "\n",
        "            raw_value = raw_value.strip()\n",
        "            try:\n",
        "                casted_value = self._cast_type(raw_value, mapping[\"type\"])\n",
        "                self._construct_mapped_field(\n",
        "                    transformed_row, mapping[\"target_field\"], casted_value\n",
        "                )\n",
        "            except (ValueError, TypeError) as e:\n",
        "                if self.on_error == \"raise\":\n",
        "                    raise\n",
        "                elif self.on_error == \"log\":\n",
        "                    logging.error(f\"Type casting error for row {row}: {e}\")\n",
        "                continue\n",
        "\n",
        "        if not transformed_row:\n",
        "            return None\n",
        "        return transformed_row\n",
        "\n",
        "    def _construct_mapped_field(self, base: Dict, field_path: str, value):\n",
        "        \"\"\"\n",
        "        Construct nested dictionaries for dot-separated field paths.\n",
        "\n",
        "        Args:\n",
        "            base (Dict): Base dictionary to insert into.\n",
        "            field_path (str): Dot-separated target field path.\n",
        "            value: Value to set at the nested path.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If nesting depth exceeds limits.\n",
        "        \"\"\"\n",
        "        parts = field_path.split(\".\")\n",
        "        if len(parts) > self.MAX_NESTING_DEPTH:\n",
        "            raise ValueError(\n",
        "                f\"Field path '{field_path}' exceeds max nesting depth \"\n",
        "                f\"of {self.MAX_NESTING_DEPTH}.\"\n",
        "            )\n",
        "        current = base\n",
        "        for part in parts[:-1]:\n",
        "            if part not in current or not isinstance(current[part], dict):\n",
        "                current[part] = {}\n",
        "            current = current[part]\n",
        "        current[parts[-1]] = value\n",
        "\n",
        "    def _cast_type(self, value: str, target_type: type):\n",
        "        \"\"\"\n",
        "        Cast a string value to a target type with robust parsing.\n",
        "\n",
        "        Args:\n",
        "            value (str): The input string value.\n",
        "            target_type (type): Target Python type (int, float, bool, datetime, str).\n",
        "\n",
        "        Returns:\n",
        "            The value cast to the target type.\n",
        "\n",
        "        Raises:\n",
        "            ValueError, TypeError: If conversion fails.\n",
        "        \"\"\"\n",
        "        if target_type == int:\n",
        "            return int(value)\n",
        "        elif target_type == float:\n",
        "            return float(value)\n",
        "        elif target_type == bool:\n",
        "            lower_val = value.lower()\n",
        "            if lower_val in {\"true\", \"1\"}:\n",
        "                return True\n",
        "            elif lower_val in {\"false\", \"0\"}:\n",
        "                return False\n",
        "            else:\n",
        "                raise ValueError(f\"Invalid boolean value '{value}'.\")\n",
        "        elif target_type == datetime:\n",
        "            if iso_parse:\n",
        "                try:\n",
        "                    return iso_parse(value)\n",
        "                except Exception as e:\n",
        "                    raise ValueError(\n",
        "                        f\"Invalid ISO8601 datetime value '{value}': {e}\"\n",
        "                    ) from e\n",
        "            else:\n",
        "                try:\n",
        "                    return datetime.fromisoformat(value)\n",
        "                except Exception as e:\n",
        "                    raise ValueError(\n",
        "                        f\"Invalid ISO8601 datetime value '{value}': {e}\"\n",
        "                    ) from e\n",
        "        elif target_type == str:\n",
        "            return value\n",
        "        else:\n",
        "            raise TypeError(f\"Unsupported target type '{target_type}'.\")\n",
        "\n",
        "    def _group_rows(self, rows: List[Dict]) -> Dict[str, List[Dict]]:\n",
        "        \"\"\"\n",
        "        Group rows by the group_by field.\n",
        "\n",
        "        Args:\n",
        "            rows (List[Dict]): List of transformed rows.\n",
        "\n",
        "        Returns:\n",
        "            Dict[str, List[Dict]]: Grouped dictionary.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If group_by field is missing in any row and on_error='raise'.\n",
        "        \"\"\"\n",
        "        grouped = {}\n",
        "        for row in rows:\n",
        "            key = self._extract_group_key(row)\n",
        "            if key is None:\n",
        "                msg = f\"Missing group_by key '{self.group_by}' in row {row}.\"\n",
        "                if self.on_error == \"raise\":\n",
        "                    raise ValueError(msg)\n",
        "                elif self.on_error == \"log\":\n",
        "                    logging.error(msg)\n",
        "                continue\n",
        "            grouped.setdefault(key, []).append(row)\n",
        "        return grouped\n",
        "\n",
        "    def _extract_group_key(self, row: Dict) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Extract the group_by key from a possibly nested row.\n",
        "\n",
        "        Args:\n",
        "            row (Dict): Transformed row.\n",
        "\n",
        "        Returns:\n",
        "            Optional[str]: Group key or None if not found.\n",
        "        \"\"\"\n",
        "        parts = self.group_by.split(\".\")\n",
        "        current = row\n",
        "        for part in parts:\n",
        "            if not isinstance(current, dict) or part not in current:\n",
        "                return None\n",
        "            current = current[part]\n",
        "        return str(current) if current is not None else None\n",
        "\n",
        "    def _flatten_dict(self, d: Dict, parent_key: str = \"\", sep: str = \".\") -> Dict:\n",
        "        \"\"\"\n",
        "        Flatten a nested dictionary.\n",
        "\n",
        "        Args:\n",
        "            d (Dict): Dictionary to flatten.\n",
        "            parent_key (str): Prefix key for recursion.\n",
        "            sep (str): Separator between keys.\n",
        "\n",
        "        Returns:\n",
        "            Dict: Flattened dictionary with dot-separated keys.\n",
        "        \"\"\"\n",
        "        items = []\n",
        "        for k, v in d.items():\n",
        "            new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
        "            if isinstance(v, dict):\n",
        "                items.extend(self._flatten_dict(v, new_key, sep=sep).items())\n",
        "            else:\n",
        "                items.append((new_key, v))\n",
        "        return dict(items)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUlcq7ycbHYw"
      },
      "outputs": [],
      "source": [
        "# tests\n",
        "\"\"\"Unittests for CSVTransformer: validate modes, casting, schema and errors.\"\"\"\n",
        "\n",
        "import unittest\n",
        "import tempfile\n",
        "import os\n",
        "from datetime import datetime\n",
        "from main import CSVTransformer\n",
        "\n",
        "\n",
        "class TestCSVTransformer(unittest.TestCase):\n",
        "    \"\"\"Unittests for the CSVTransformer class.\"\"\"\n",
        "\n",
        "    def setUp(self):\n",
        "        \"\"\"Set up temporary CSV files and schema for tests.\"\"\"\n",
        "        self.schema = {\n",
        "            \"id\": {\"target_field\": \"user.id\", \"type\": int},\n",
        "            \"name\": {\"target_field\": \"user.name\", \"type\": str},\n",
        "            \"active\": {\"target_field\": \"status.active\", \"type\": bool},\n",
        "            \"created\": {\"target_field\": \"meta.created\", \"type\": datetime},\n",
        "        }\n",
        "\n",
        "        self.csv_data = (\n",
        "            \"id,name,active,created\\n\"\n",
        "            \"1,Alice,true,2023-01-01T10:00:00Z\\n\"\n",
        "            \"2,Bob,false,2023-01-02T12:30:00Z\\n\"\n",
        "        )\n",
        "\n",
        "        self.temp_file = tempfile.NamedTemporaryFile(mode=\"w+\", delete=False)\n",
        "        self.temp_file.write(self.csv_data)\n",
        "        self.temp_file.close()\n",
        "\n",
        "    def tearDown(self):\n",
        "        \"\"\"Clean up the temporary file.\"\"\"\n",
        "        os.unlink(self.temp_file.name)\n",
        "\n",
        "    def test_flat_mode(self):\n",
        "        \"\"\"Test transformation in flat mode.\"\"\"\n",
        "        transformer = CSVTransformer(self.schema, mode=\"flat\")\n",
        "        result = transformer.transform(self.temp_file.name)\n",
        "        self.assertEqual(len(result), 2)\n",
        "        self.assertIn(\"user.id\", result[0])\n",
        "\n",
        "    def test_nested_mode(self):\n",
        "        \"\"\"Test transformation in nested mode.\"\"\"\n",
        "        transformer = CSVTransformer(self.schema, mode=\"nested\")\n",
        "        result = transformer.transform(self.temp_file.name)\n",
        "        self.assertEqual(result[0][\"user\"][\"id\"], 1)\n",
        "\n",
        "    def test_grouped_mode(self):\n",
        "        \"\"\"Test transformation in grouped mode by name.\"\"\"\n",
        "        schema = {\n",
        "            \"name\": {\"target_field\": \"name\", \"type\": str},\n",
        "            \"id\": {\"target_field\": \"id\", \"type\": int},\n",
        "        }\n",
        "        csv_data = \"id,name\\n1,Alice\\n2,Alice\\n3,Bob\\n\"\n",
        "        file = tempfile.NamedTemporaryFile(mode=\"w+\", delete=False)\n",
        "        file.write(csv_data)\n",
        "        file.close()\n",
        "\n",
        "        transformer = CSVTransformer(schema, mode=\"grouped\", group_by=\"name\")\n",
        "        result = transformer.transform(file.name)\n",
        "        os.unlink(file.name)\n",
        "        self.assertEqual(len(result[\"Alice\"]), 2)\n",
        "\n",
        "    def test_invalid_mode(self):\n",
        "        \"\"\"Test initialization with an invalid mode.\"\"\"\n",
        "        with self.assertRaises(ValueError):\n",
        "            CSVTransformer(self.schema, mode=\"invalid\")\n",
        "\n",
        "    def test_invalid_on_error(self):\n",
        "        \"\"\"Test initialization with invalid on_error strategy.\"\"\"\n",
        "        with self.assertRaises(ValueError):\n",
        "            CSVTransformer(self.schema, on_error=\"fail\")\n",
        "\n",
        "    def test_grouped_without_group_by(self):\n",
        "        \"\"\"Test grouped mode without group_by.\"\"\"\n",
        "        with self.assertRaises(ValueError):\n",
        "            CSVTransformer(self.schema, mode=\"grouped\")\n",
        "\n",
        "    def test_exceed_max_columns(self):\n",
        "        \"\"\"Test column limit enforcement.\"\"\"\n",
        "        csv_data = \",\".join([f\"col{i}\" for i in range(101)]) + \"\\n\"\n",
        "        file = tempfile.NamedTemporaryFile(mode=\"w+\", delete=False)\n",
        "        file.write(csv_data)\n",
        "        file.close()\n",
        "        transformer = CSVTransformer({}, mode=\"flat\")\n",
        "\n",
        "        with self.assertRaises(ValueError):\n",
        "            transformer.transform(file.name)\n",
        "        os.unlink(file.name)\n",
        "\n",
        "    def test_empty_file(self):\n",
        "        \"\"\"Test handling of empty file.\"\"\"\n",
        "        file = tempfile.NamedTemporaryFile(mode=\"w+\", delete=False)\n",
        "        file.close()\n",
        "        transformer = CSVTransformer(self.schema, mode=\"flat\")\n",
        "        result = transformer.transform(file.name)\n",
        "        os.unlink(file.name)\n",
        "        self.assertEqual(result, [])\n",
        "\n",
        "    def test_missing_required_column(self):\n",
        "        \"\"\"Test missing column error behavior.\"\"\"\n",
        "        csv_data = \"id,created\\n1,2022-01-01T00:00:00Z\\n\"\n",
        "        file = tempfile.NamedTemporaryFile(mode=\"w+\", delete=False)\n",
        "        file.write(csv_data)\n",
        "        file.close()\n",
        "        transformer = CSVTransformer(\n",
        "            self.schema, mode=\"flat\", on_error=\"skip\"\n",
        "        )\n",
        "        result = transformer.transform(file.name)\n",
        "        os.unlink(file.name)\n",
        "        self.assertEqual(len(result), 1)\n",
        "\n",
        "    def test_casting_error_skip(self):\n",
        "        \"\"\"Test skipping rows with type casting errors.\"\"\"\n",
        "        csv_data = \"id,name,active,created\\nX,Alice,true,invalid\\n\"\n",
        "        file = tempfile.NamedTemporaryFile(mode=\"w+\", delete=False)\n",
        "        file.write(csv_data)\n",
        "        file.close()\n",
        "        transformer = CSVTransformer(\n",
        "            self.schema, mode=\"flat\", on_error=\"skip\"\n",
        "        )\n",
        "        result = transformer.transform(file.name)\n",
        "        os.unlink(file.name)\n",
        "        self.assertEqual(\n",
        "            result,\n",
        "            [{'user.name': 'Alice', 'status.active': True}]\n",
        "        )\n",
        "\n",
        "    def test_casting_error_raise(self):\n",
        "        \"\"\"Test raising exception on casting errors.\"\"\"\n",
        "        csv_data = \"id,name,active,created\\nX,Alice,true,invalid\\n\"\n",
        "        file = tempfile.NamedTemporaryFile(mode=\"w+\", delete=False)\n",
        "        file.write(csv_data)\n",
        "        file.close()\n",
        "        transformer = CSVTransformer(\n",
        "            self.schema, mode=\"flat\", on_error=\"raise\"\n",
        "        )\n",
        "\n",
        "        with self.assertRaises(Exception):\n",
        "            transformer.transform(file.name)\n",
        "        os.unlink(file.name)\n",
        "\n",
        "    def test_boolean_true_casting(self):\n",
        "        \"\"\"Test valid boolean string conversion.\"\"\"\n",
        "        transformer = CSVTransformer(\n",
        "            {\"active\": {\"target_field\": \"a\", \"type\": bool}}\n",
        "        )\n",
        "        row = {\"active\": \"true\"}\n",
        "        result = transformer._process_row(row)\n",
        "        self.assertTrue(result[\"a\"])\n",
        "\n",
        "    def test_boolean_invalid_casting(self):\n",
        "        \"\"\"Test invalid boolean string conversion raises.\"\"\"\n",
        "        transformer = CSVTransformer(\n",
        "            {\"active\": {\"target_field\": \"a\", \"type\": bool}},\n",
        "            on_error=\"raise\"\n",
        "        )\n",
        "        row = {\"active\": \"yes\"}\n",
        "        with self.assertRaises(ValueError):\n",
        "            transformer._process_row(row)\n",
        "\n",
        "    def test_datetime_casting(self):\n",
        "        \"\"\"Test valid datetime parsing.\"\"\"\n",
        "        transformer = CSVTransformer(\n",
        "            {\"created\": {\"target_field\": \"meta.created\", \"type\": datetime}}\n",
        "        )\n",
        "        row = {\"created\": \"2024-01-01T10:00:00Z\"}\n",
        "        result = transformer._process_row(row)\n",
        "        self.assertIsInstance(result[\"meta\"][\"created\"], datetime)\n",
        "\n",
        "    def test_unsupported_type_cast(self):\n",
        "        \"\"\"Test unsupported type raises TypeError.\"\"\"\n",
        "        transformer = CSVTransformer(\n",
        "            {\"x\": {\"target_field\": \"x\", \"type\": list}}\n",
        "        )\n",
        "        row = {\"x\": \"[1,2]\"}\n",
        "        with self.assertRaises(TypeError):\n",
        "            transformer._process_row(row)\n",
        "\n",
        "    def test_flatten_nested_dict(self):\n",
        "        \"\"\"Test flattening a nested dictionary.\"\"\"\n",
        "        transformer = CSVTransformer(self.schema)\n",
        "        nested = {\"a\": {\"b\": {\"c\": 1}}}\n",
        "        flat = transformer._flatten_dict(nested)\n",
        "        self.assertEqual(flat[\"a.b.c\"], 1)\n",
        "\n",
        "    def test_missing_field_skipped(self):\n",
        "        \"\"\"Test skipping missing field when on_error is skip.\"\"\"\n",
        "        transformer = CSVTransformer(self.schema, on_error=\"skip\")\n",
        "        row = {\"id\": \"1\", \"name\": \"\"}\n",
        "        result = transformer._process_row(row)\n",
        "        self.assertEqual(result.get(\"user\", {}).get(\"id\"), 1)\n",
        "\n",
        "    def test_group_key_extraction(self):\n",
        "        \"\"\"Test group key extraction from nested path.\"\"\"\n",
        "        transformer = CSVTransformer(\n",
        "            self.schema, mode=\"grouped\", group_by=\"user.name\"\n",
        "        )\n",
        "        row = {\"user\": {\"name\": \"Alice\"}}\n",
        "        key = transformer._extract_group_key(row)\n",
        "        self.assertEqual(key, \"Alice\")\n",
        "\n",
        "    def test_construct_mapped_field(self):\n",
        "        \"\"\"Test building nested path.\"\"\"\n",
        "        transformer = CSVTransformer(self.schema)\n",
        "        base = {}\n",
        "        transformer._construct_mapped_field(base, \"a.b.c\", 123)\n",
        "        self.assertEqual(base[\"a\"][\"b\"][\"c\"], 123)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxZs2ehLYWkm"
      },
      "source": [
        "# Model Breaking Proof\n",
        "\n",
        "#### Model Breaking Task URL: https://rlhf-v3.turing.com/prompt/e48a6f45-1a99-4b32-ab39-0f4fbc163e5f?origin=https%3A%2F%2Flabeling-ms.turing.com&redirect_url=https%3A%2F%2Flabeling-ms.turing.com%2Fconversations%2F74834%2Fview\n",
        "\n",
        "#### Model code:\n",
        "\n",
        "```python\n",
        "import csv\n",
        "import json\n",
        "from typing import List, Dict, Union, Optional\n",
        "from datetime import datetime\n",
        "import logging\n",
        "\n",
        "class CSVTransformer:\n",
        "    def __init__(self, schema: Dict[str, Dict], mode: str = \"flat\", group_by: Optional[str] = None,\n",
        "                 delimiter: str = \",\", on_error: str = \"raise\"):\n",
        "        self.schema = schema\n",
        "        self.mode = mode\n",
        "        self.group_by = group_by\n",
        "        self.delimiter = delimiter\n",
        "        if on_error not in [\"raise\", \"skip\", \"log\"]:\n",
        "            raise ValueError(\"on_error must be one of 'raise', 'skip', or 'log'\")\n",
        "        self.on_error = on_error\n",
        "\n",
        "    def transform(self, csv_path: str) -> Union[List[Dict], Dict[str, List[Dict]]]:\n",
        "        json_output = []\n",
        "\n",
        "        try: # <Issue>: There is no upfront validation of the schema to check if required columns exist or if the schema itself is well-formed, risking runtime errors.\n",
        "            with open(csv_path, newline='', encoding='utf-8') as csvfile:\n",
        "                csv_reader = csv.DictReader(csvfile, delimiter=self.delimiter)\n",
        "                for row in csv_reader:\n",
        "                    try:\n",
        "                        transformed_row = self.process_row(row)\n",
        "                        if transformed_row:\n",
        "                            json_output.append(transformed_row)\n",
        "                    except Exception as e:\n",
        "                        self.handle_error(e, row)\n",
        "\n",
        "                if self.mode == \"grouped\" and self.group_by: # <Issue>: The code does not handle mode == \"nested\" differently; it simply returns a flat or grouped list. The problem specifies nested mode should create nested objects based on prefixes distinctly.\n",
        "                    return self.group_rows(json_output)\n",
        "                return json_output\n",
        "        except FileNotFoundError:\n",
        "            raise FileNotFoundError(f\"CSV file at path '{csv_path}' not found.\")\n",
        "\n",
        "    def process_row(self, row: Dict[str, str]) -> Optional[Dict]:\n",
        "        transformed_row = {}\n",
        "        \n",
        "        for field_name, mapping in self.schema.items():\n",
        "            original_value = row.get(field_name, None)\n",
        "            \n",
        "            if original_value is None and self.on_error == \"raise\":\n",
        "                raise KeyError(f\"Missing required column '{field_name}' in row.\")\n",
        "            if original_value is None:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                transformed_value = self.cast_type(original_value.strip(), mapping['type'])\n",
        "                self.construct_mapped_field(transformed_row, mapping['target_field'], transformed_value)\n",
        "            except (ValueError, TypeError) as e:\n",
        "                self.handle_error(e, row)\n",
        "        \n",
        "        return transformed_row\n",
        "\n",
        "    def construct_mapped_field(self, base: Dict, field_path: str, value): # <Issue>: There is no check or limitation on the nesting depth, allowing arbitrarily deep nested structures which violate the constraint.\n",
        "        parts = field_path.split('.')\n",
        "        for part in parts[:-1]:\n",
        "            base = base.setdefault(part, {})\n",
        "        base[parts[-1]] = value\n",
        "\n",
        "    def cast_type(self, value: str, target_type: type):\n",
        "        if target_type == int:\n",
        "            return int(value)\n",
        "        elif target_type == float:\n",
        "            return float(value)\n",
        "        elif target_type == bool:\n",
        "            return value.lower() in [\"true\", \"1\"]\n",
        "        elif target_type == datetime:\n",
        "            return datetime.fromisoformat(value)\n",
        "        elif target_type == str:\n",
        "            return value\n",
        "        else:\n",
        "            raise TypeError(f\"Unsupported type specified: {target_type}\")\n",
        "\n",
        "    def group_rows(self, rows: List[Dict]) -> Dict[str, List[Dict]]:\n",
        "        grouped_data = {}\n",
        "        for row in rows:\n",
        "            group_key = row.get(self.group_by)\n",
        "            if group_key is not None:\n",
        "                if group_key not in grouped_data:\n",
        "                    grouped_data[group_key] = []\n",
        "                grouped_data[group_key].append(row)\n",
        "        return grouped_data\n",
        "\n",
        "    def handle_error(self, error: Exception, row: Dict):\n",
        "        if self.on_error == \"raise\":\n",
        "            raise error\n",
        "        elif self.on_error == \"log\":\n",
        "            logging.error(f\"Error processing row {row}: {error}\")\n",
        "\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}