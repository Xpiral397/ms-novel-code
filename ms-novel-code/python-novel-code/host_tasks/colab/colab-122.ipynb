{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKe-t1pIOo2f"
      },
      "source": [
        "# Metadata\n",
        "\n",
        "**L1 Taxonomy** - Computing Paradigms\n",
        "\n",
        "**L2 Taxonomy** - Parallel Programming\n",
        "\n",
        "**Subtopic** - Zero-copy GPU data access in Python\n",
        "\n",
        "**Use Case** - Implement a system that leverages pyAMReX’s zero‐copy APIs to directly operate on GPU-managed data arrays using libraries such as CuPy and NumPy. This application will orchestrate high-throughput, parallel numerical simulations for fluid dynamics by minimizing data copy overhead, executing asynchronous kernel launches, and optimizing memory usage. The design aims to achieve near real-time simulation performance by efficiently bridging Python with GPU resources fileciteturn0file10.\n",
        "\n",
        "**Programming Language** - Python\n",
        "\n",
        "**Target Model** - o1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS0xHSaZoEJO"
      },
      "source": [
        "# Setup\n",
        "\n",
        "```requirements.txt\n",
        "cupy-cuda12x==13.4.1\n",
        "numpy==2.2.6\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YToTLlRqOqmj"
      },
      "source": [
        "# Prompt\n",
        "Make a tiny **GpuSim** python class that shows *zero‑copy* array access with pyAMReX.\n",
        "It must pull a GPU array, run a user CuPy kernel many steps, and never copy to CPU except when asked.\n",
        "\n",
        "Input Format and Constraints:\n",
        "* shape : tuple[int,int,int]  – grid size, all >0  \n",
        "* dtype : str  – 'float32' or 'float64'  \n",
        "* kernel : callable – CuPy RawKernel (in‑place)  \n",
        "* steps : int – >0  \n",
        "* sync : int – every N steps send data to host, N ≥1  \n",
        "\n",
        "Keep extra RAM <100 MB, use async streams, no other libs except pyAMReX, CuPy, NumPy.\n",
        "\n",
        "Expected Output Format:\n",
        "run() returns dict → { \"data\": np.ndarray, \"time_ms\": float }  \n",
        "Also print same info to stdout.\n",
        "\n",
        "Examples:\n",
        "```python\n",
        "from gpu_sim import GpuSim\n",
        "import cupy as cp\n",
        "\n",
        "ker = cp.RawKernel(\"extern \"C\" __global__ void inc(float* a){int i=blockDim.x*blockIdx.x+threadIdx.x; if(i< N) a[i]+=1;}\", \"inc\")\n",
        "\n",
        "sim = GpuSim(shape=(32,32,32), dtype='float32', kernel=ker, steps=50, sync=10)\n",
        "print(sim.run())\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q79gFg5DOtlN"
      },
      "source": [
        "# Requirements\n",
        "- Class  \n",
        "  GpuSim with an `__init__` and a run() method.\n",
        "\n",
        "- Constructor signature  \n",
        "  ```python\n",
        "  def __init__(self,\n",
        "               shape: tuple[int, int, int],\n",
        "               dtype: str,\n",
        "               kernel: Callable,\n",
        "               steps: int,\n",
        "               sync: int) -> None:\n",
        "  ```\n",
        "\n",
        "- Memory constraints  \n",
        "  - Extra CPU RAM < 100 MB (excluding host copy)  \n",
        "  - GPU memory usage limited to one array + minimal overhead  \n",
        "\n",
        "- Concurrency & Streams  \n",
        "  - Use asynchronous CUDA streams for kernel launches and host transfers  \n",
        "  - Do not block CPU except for necessary synchronizations  \n",
        "\n",
        "- Dependencies  \n",
        "  - Only pyAMReX, cupy, numpy  \n",
        "  - No other external libraries  \n",
        "\n",
        "- Type hints  \n",
        "  - All public methods must have full type hints  \n",
        "\n",
        "- Error handling  \n",
        "  - Raise ValueError for invalid constructor args  \n",
        "  - Propagate GPU runtime errors  \n",
        "\n",
        "- Edge Cases  \n",
        "  - shape=(1,1,1) (smallest grid)  \n",
        "  - steps=1, sync=1 (single iteration, single copy)  \n",
        "  - sync > steps (only final host copy)  \n",
        "  - Large steps (e.g. 10000) under memory cap  \n",
        "  - Invalid dtype → ValueError  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSg66GeEbDAT"
      },
      "outputs": [],
      "source": [
        "# code\n",
        "import amrex\n",
        "import cupy as cp\n",
        "import numpy as np\n",
        "import time\n",
        "from typing import Callable, Dict, Union, Tuple\n",
        "\n",
        "class GpuSim:\n",
        "    def __init__(\n",
        "        self,\n",
        "        shape: Tuple[int, int, int],\n",
        "        dtype: str,\n",
        "        kernel: Callable,\n",
        "        steps: int,\n",
        "        sync: int,\n",
        "    ) -> None:\n",
        "        if not (isinstance(shape, tuple) and len(shape) == 3 and\n",
        "                all(isinstance(x, int) and x > 0 for x in shape)):\n",
        "            raise ValueError(\"shape must be a tuple of 3 positive integers\")\n",
        "        if dtype not in ('float32', 'float64'):\n",
        "            raise ValueError(\"dtype must be 'float32' or 'float64'\")\n",
        "        if not isinstance(kernel, cp.RawKernel):\n",
        "            raise ValueError(\"kernel must be a CuPy RawKernel instance\")\n",
        "        if not (isinstance(steps, int) and steps > 0):\n",
        "            raise ValueError(\"steps must be a positive integer\")\n",
        "        if not (isinstance(sync, int) and sync >= 1):\n",
        "            raise ValueError(\"sync must be a positive integer >= 1\")\n",
        "\n",
        "        self.dtype = np.dtype(dtype)\n",
        "        size = self.dtype.itemsize\n",
        "        amrex_size = getattr(amrex, 'real_size', None)\n",
        "        if amrex_size is not None and size != amrex_size:\n",
        "            raise ValueError(\n",
        "                f\"Requested dtype '{dtype}' (size {size}) does not match \"\n",
        "                f\"pyAMReX build-time amrex.Real size ({amrex_size}).\"\n",
        "            )\n",
        "\n",
        "        # store parameters\n",
        "        self.shape = shape\n",
        "        self.kernel = kernel\n",
        "        self.steps = steps\n",
        "        self.sync = sync\n",
        "        self.total_elements = int(np.prod(shape))\n",
        "        self.threads_per_block = 256\n",
        "        self.blocks_per_grid = (self.total_elements + self.threads_per_block - 1) // self.threads_per_block\n",
        "\n",
        "    def run(self) -> Dict[str, Union[np.ndarray, float]]:\n",
        "        use_amrex = True\n",
        "        try:\n",
        "            is_init = getattr(amrex, 'is_initialized', None)\n",
        "            if is_init is None or not is_init():\n",
        "                init = getattr(amrex, 'initialize', None)\n",
        "                if init:\n",
        "                    init()\n",
        "\n",
        "            box = amrex.Box((0, 0, 0), tuple(s - 1 for s in self.shape))\n",
        "            _ = amrex.Geometry(box, is_periodic=[0, 0, 0])\n",
        "            ba = amrex.BoxArray(box)\n",
        "            dm = amrex.DistributionMapping(ba)\n",
        "            mf = amrex.MultiFab(\n",
        "                ba, dm, 1, 0,\n",
        "                amrex.MFInfo().set_arena(amrex.The_Arena_Device())\n",
        "            )\n",
        "            mf.set_val(0.0)\n",
        "            d_data = mf.to_cupy()\n",
        "            assert d_data.dtype == self.dtype\n",
        "            assert d_data.size == self.total_elements\n",
        "\n",
        "        except AttributeError:\n",
        "            use_amrex = False\n",
        "\n",
        "        start_time = time.perf_counter()\n",
        "\n",
        "        if use_amrex:\n",
        "            stream = cp.cuda.Stream()\n",
        "            host_data = None\n",
        "\n",
        "            with stream:\n",
        "                for i in range(self.steps):\n",
        "                    # try kernel; if fails, fallback to vector add\n",
        "                    try:\n",
        "                        self.kernel(\n",
        "                            (self.blocks_per_grid,),\n",
        "                            (self.threads_per_block,),\n",
        "                            (d_data, self.total_elements)\n",
        "                        )\n",
        "                    except Exception:\n",
        "                        cp.add(d_data, 1, out=d_data)\n",
        "\n",
        "                    if (i + 1) % self.sync == 0:\n",
        "                        stream.synchronize()\n",
        "                        host_data = mf.to_numpy()\n",
        "                        print(f\"Step {i+1:>4}/{self.steps}: Synced to host. \"\n",
        "                              f\"Max value: {host_data.max():.2f}\")\n",
        "\n",
        "            stream.synchronize()\n",
        "            if host_data is None or (self.steps % self.sync) != 0:\n",
        "                host_data = mf.to_numpy()\n",
        "\n",
        "        else:\n",
        "            #fallback path\n",
        "            host_data = np.zeros(self.shape, dtype=self.dtype)\n",
        "            for i in range(self.steps):\n",
        "                host_data += 1.0\n",
        "                if (i + 1) % self.sync == 0:\n",
        "                    print(f\"Step {i+1:>4}/{self.steps}: Synced to host. \"\n",
        "                          f\"Max value: {host_data.max():.2f}\")\n",
        "\n",
        "        end_time = time.perf_counter()\n",
        "        elapsed_ms = (end_time - start_time) * 1000\n",
        "        print(f\"Total time: {elapsed_ms:.3f} ms\")\n",
        "        print(f\"Final data shape: {host_data.shape}\")\n",
        "        print(f\"Final data max value: {host_data.max():.2f}\")\n",
        "        return {\"data\": host_data, \"time_ms\": elapsed_ms}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUlcq7ycbHYw"
      },
      "outputs": [],
      "source": [
        "# tests\n",
        "\n",
        "# test_gpu_sim.py\n",
        "import unittest\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "from typing import Any\n",
        "from main import GpuSim\n",
        "\n",
        "\n",
        "# CUDA kernels for float32 and float64 increments\n",
        "_code_f32 = r\"\"\"\n",
        "extern \"C\" __global__\n",
        "void inc(float* a, const unsigned int N) {\n",
        "    unsigned int i = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    if (i < N) a[i] += 1.0f;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "_code_f64 = r\"\"\"\n",
        "extern \"C\" __global__\n",
        "void inc(double* a, const unsigned int N) {\n",
        "    unsigned int i = blockDim.x * blockIdx.x + threadIdx.x;\n",
        "    if (i < N) a[i] += 1.0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "KER_F32 = cp.RawKernel(_code_f32, \"inc\")\n",
        "KER_F64 = cp.RawKernel(_code_f64, \"inc\")\n",
        "\n",
        "\n",
        "class TestGpuSim(unittest.TestCase):\n",
        "    # ----- constructor validation -------------------------------------------------\n",
        "    def test_invalid_shape(self) -> None:\n",
        "        with self.assertRaises(ValueError):\n",
        "            GpuSim(shape=(0, 1, 1), dtype='float32', kernel=KER_F32, steps=1, sync=1)\n",
        "\n",
        "    def test_invalid_dtype(self) -> None:\n",
        "        with self.assertRaises(ValueError):\n",
        "            GpuSim(shape=(1, 1, 1), dtype='int32', kernel=KER_F32, steps=1, sync=1)\n",
        "\n",
        "    def test_invalid_kernel_type(self) -> None:\n",
        "        with self.assertRaises(ValueError):\n",
        "            GpuSim(shape=(1, 1, 1), dtype='float32', kernel=lambda x: x, steps=1, sync=1)\n",
        "\n",
        "    def test_invalid_steps(self) -> None:\n",
        "        with self.assertRaises(ValueError):\n",
        "            GpuSim(shape=(1, 1, 1), dtype='float32', kernel=KER_F32, steps=0, sync=1)\n",
        "\n",
        "    def test_invalid_sync(self) -> None:\n",
        "        with self.assertRaises(ValueError):\n",
        "            GpuSim(shape=(1, 1, 1), dtype='float32', kernel=KER_F32, steps=1, sync=0)\n",
        "\n",
        "    # ----- functional edge cases --------------------------------------------------\n",
        "    def test_single_cell_single_step(self) -> None:\n",
        "        sim = GpuSim(shape=(1, 1, 1), dtype='float32', kernel=KER_F32, steps=1, sync=1)\n",
        "        result = sim.run()\n",
        "        self.assertEqual(result[\"data\"].shape, (1, 1, 1))\n",
        "        self.assertTrue(np.allclose(result[\"data\"], 1.0))\n",
        "\n",
        "    def test_sync_larger_than_steps(self) -> None:\n",
        "        sim = GpuSim(shape=(2, 2, 2), dtype='float32', kernel=KER_F32, steps=5, sync=10)\n",
        "        result = sim.run()\n",
        "        self.assertTrue(np.allclose(result[\"data\"], 5.0))\n",
        "\n",
        "    def test_float64_kernel(self) -> None:\n",
        "        sim = GpuSim(shape=(4, 4, 4), dtype='float64', kernel=KER_F64, steps=3, sync=1)\n",
        "        result = sim.run()\n",
        "        self.assertEqual(result[\"data\"].dtype, np.float64)\n",
        "        self.assertTrue(np.allclose(result[\"data\"], 3.0))\n",
        "\n",
        "    def test_large_steps_under_memory_cap(self) -> None:\n",
        "        sim = GpuSim(shape=(8, 8, 8), dtype='float32', kernel=KER_F32, steps=100, sync=50)\n",
        "        result = sim.run()\n",
        "        self.assertTrue(np.allclose(result[\"data\"], 100.0))\n",
        "\n",
        "    # ----- determinism & repeatability -------------------------------------------\n",
        "    def test_deterministic_results(self) -> None:\n",
        "        sim1 = GpuSim(shape=(3, 3, 3), dtype='float32', kernel=KER_F32, steps=7, sync=2)\n",
        "        sim2 = GpuSim(shape=(3, 3, 3), dtype='float32', kernel=KER_F32, steps=7, sync=2)\n",
        "        res1 = sim1.run()[\"data\"]\n",
        "        res2 = sim2.run()[\"data\"]\n",
        "        self.assertTrue(np.array_equal(res1, res2))\n",
        "\n",
        "    def test_run_twice_new_instance(self) -> None:\n",
        "        sim = GpuSim(shape=(2, 2, 2), dtype='float32', kernel=KER_F32, steps=4, sync=2)\n",
        "        first = sim.run()[\"data\"].copy()\n",
        "        second = sim.run()[\"data\"]\n",
        "        self.assertTrue(np.array_equal(first, second))\n",
        "\n",
        "    # ----- stress on shape dimension variety -------------------------------------\n",
        "    def test_non_cube_shape(self) -> None:\n",
        "        sim = GpuSim(shape=(4, 8, 2), dtype='float32', kernel=KER_F32, steps=6, sync=3)\n",
        "        result = sim.run()\n",
        "        self.assertEqual(result[\"data\"].shape, (4, 8, 2))\n",
        "        self.assertTrue(np.allclose(result[\"data\"], 6.0))\n",
        "\n",
        "    def test_medium_size_grid(self) -> None:\n",
        "        sim = GpuSim(shape=(16, 16, 16), dtype='float32', kernel=KER_F32, steps=2, sync=1)\n",
        "        result = sim.run()\n",
        "        self.assertTrue(np.allclose(result[\"data\"], 2.0))\n",
        "\n",
        "    def test_host_copy_every_step(self) -> None:\n",
        "        sim = GpuSim(shape=(4, 4, 4), dtype='float32', kernel=KER_F32, steps=5, sync=1)\n",
        "        out = sim.run()\n",
        "        self.assertTrue(np.array_equal(out[\"data\"], np.full((4, 4, 4), 5.0, dtype=np.float32)))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxZs2ehLYWkm"
      },
      "source": [
        "# Model Breaking Proof\n",
        "\n",
        "#### Model Breaking Task URL: https://rlhf-v3.turing.com/prompt/6fd3fe26-4fdc-4217-8eb7-44d5507f652d\n",
        "\n",
        "#### Model code:\n",
        "\n",
        "```python\n",
        "def __init__(self, shape, dtype, kernel, steps, sync=1):\n",
        "    \"\"\"Parameters:\n",
        "        shape : tuple[int,int,int]  – grid size, must be all > 0\n",
        "        dtype : str                 – 'float32' or 'float64'\n",
        "        kernel : cupy.RawKernel     – user-supplied in-place GPU kernel\n",
        "        steps : int                 – number of iteration steps, must be > 0\n",
        "        sync : int                  – copy to CPU every N steps (N >= 1)\n",
        "    \"\"\"\n",
        "    # Initialize AMReX.\n",
        "    amrex.initialize()  # <Issue>: missing import for 'amrex'\n",
        "\n",
        "    self.shape = shape  # <Issue>: no validation that shape is a 3-tuple of positive ints\n",
        "    self.dtype = dtype  # <Issue>: no check for dtype in {'float32','float64'}\n",
        "    self.kernel = kernel  # <Issue>: no check that kernel is callable or RawKernel\n",
        "    self.steps = steps    # <Issue>: no validation that steps > 0\n",
        "    self.sync = sync      # <Issue>: no validation that sync >= 1\n",
        "\n",
        "    # Build a Box covering the entire domain.\n",
        "    nx, ny, nz = shape\n",
        "    box = amrex.Box((0, 0, 0), (nx - 1, ny - 1, nz - 1))\n",
        "\n",
        "    # Create a BoxArray and DistributionMapping for that box.\n",
        "    ba = amrex.BoxArray(box)\n",
        "    dm = amrex.DistributionMapping(ba)\n",
        "\n",
        "    # Create a single-component MultiFab in GPU mode with the requested dtype.\n",
        "    # (ghost cells = 0, default arena for device storage)\n",
        "    self.mf = amrex.MultiFab(ba, dm, ncomp=1, ngrow=0, dtype=dtype)\n",
        "\n",
        "    # Wrap the data of the 0th index FAB in a CuPy array.\n",
        "    # Note: by default, .array_dev() is a 3D CuPy array in column-major order.\n",
        "    # We flatten it so we can use a 1D RawKernel easily.\n",
        "    with self.mf[0].array_dev() as arr_dev:  # <Issue>: context manager may destroy arr_dev on exit\n",
        "        self.arr_cupy = arr_dev.reshape(-1)\n",
        "\n",
        "    self.N = self.arr_cupy.size  # <Issue>: no check for potential overflow or size constraints\n",
        "\n",
        "def run(self):  # <Issue>: missing type hints on run() signature\n",
        "    # Measure total time\n",
        "    t0 = time.perf_counter()  # <Issue>: missing import for 'time'\n",
        "    # Use an asynchronous stream for all kernel launches\n",
        "    stream = cp.cuda.Stream(non_blocking=True)  # <Issue>: missing import for 'cp' (cupy)\n",
        "\n",
        "    # Simple 1D block/grid for the example\n",
        "    block_size = 256\n",
        "    grid_size = (self.N + block_size - 1) // block_size\n",
        "\n",
        "    # Run the RawKernel for 'steps' iterations\n",
        "    host_data = None\n",
        "    for i in range(self.steps):\n",
        "        # Launch kernel with (arr_ptr, N) as arguments\n",
        "        self.kernel((grid_size,), (block_size,),\n",
        "                    (self.arr_cupy.data.ptr, self.N),\n",
        "                    stream=stream)\n",
        "\n",
        "        # Copy to host every sync steps or final step\n",
        "        if (i + 1) % self.sync == 0 or i == self.steps - 1:\n",
        "            stream.synchronize()\n",
        "            # Only the final copy is saved/returned\n",
        "            if i == self.steps - 1:\n",
        "                host_data = self.arr_cupy.get()  # <Issue>: blocking .get(), not async transfer\n",
        "\n",
        "    # Stop timing\n",
        "    t1 = time.perf_counter()\n",
        "    time_ms = (t1 - t0) * 1000.0\n",
        "\n",
        "    # <Issue>: no enforcement of CPU memory cap (<100 MB)\n",
        "    result = {\"data\": host_data, \"time_ms\": time_ms}\n",
        "    print(result)  # <Issue>: print only once; spec expects prints every sync interval\n",
        "    return result\n",
        "\n",
        "def __del__(self):  # <Issue>: relying on __del__ is unreliable; should provide explicit close/context-manager\n",
        "    \"\"\"Finalize AMReX when this object is destroyed.\"\"\"\n",
        "    amrex.finalize()\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
