{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKe-t1pIOo2f"
      },
      "source": [
        "# Metadata\n",
        "\n",
        "**L1 Taxonomy** - Computing Paradigms\n",
        "\n",
        "**L2 Taxonomy** - Functional Programming\n",
        "\n",
        "**Subtopic** - Higher-Order Functions and their Applications\n",
        "\n",
        "**Use Case** - Create a Python module that uses higher-order functions to process and transform data from a CSV file. The module should include functions for filtering rows based on certain criteria, mapping values to transform the data, and reducing the data to aggregate results.\n",
        "\n",
        "**Programming Language** - Python\n",
        "\n",
        "**Target Model** - GPT-4o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS0xHSaZoEJO"
      },
      "source": [
        "# Setup\n",
        "\n",
        "```requirements.txt\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YToTLlRqOqmj"
      },
      "source": [
        "# Prompt\n",
        "## Problem Description\n",
        "\n",
        "You are tasked with creating a Python module that implements a flexible data processing pipeline using functional programming principles. The pipeline will process data from a CSV file by applying a sequence of operations such as filtering, mapping, and reducing.\n",
        "\n",
        "The core of the solution will be a main function that takes a file path and a list of processing steps. It must use higher-order functions to dynamically apply these steps, demonstrating a clean separation between the data and the logic that transforms it.\n",
        "\n",
        "## Input Format\n",
        "\n",
        "The main function will accept two arguments:\n",
        "1.  file_path: A string representing the path to the input CSV file. The CSV will have a header row.\n",
        "2.  operations: A list of tuples, where each tuple defines a processing step.\n",
        "\n",
        "The operation tuples will have the following structure:\n",
        "-   Filter: ('filter', column_name, filter_func)\n",
        "-   Map: ('map', column_name, map_func)\n",
        "-   Reduce: ('reduce', reduce_func, initial_value)\n",
        "\n",
        "## Output Format\n",
        "\n",
        "The function should return the final result after all operations have been applied.\n",
        "-   If the last operation is a filter or map, the output will be a list of dictionaries.\n",
        "-   If the last operation is a reduce, the output will be the single aggregated value.\n",
        "\n",
        "## Example\n",
        "\n",
        "Input CSV (sales.csv):\n",
        "```\n",
        "product,category,quantity,price\n",
        "Laptop,Electronics,10,1200\n",
        "Mouse,Electronics,50,25\n",
        "Keyboard,Electronics,30,75\n",
        "T-Shirt,Apparel,100,15\n",
        "```\n",
        "\n",
        "Input Operations:\n",
        "```\n",
        "operations = [\n",
        "    ('filter', 'category', lambda x: x == 'Electronics'),\n",
        "    ('map', 'total_price', lambda row: int(row['quantity']) * int(row['price'])),\n",
        "    ('reduce', lambda acc, row: acc + row['total_price'], 0)\n",
        "]\n",
        "```\n",
        "\n",
        "Example Execution:\n",
        "process_csv_pipeline('sales.csv', operations)\n",
        "\n",
        "Explanation:\n",
        "1.  Filter: Keeps only the rows where the category is 'Electronics'.\n",
        "2.  Map: Creates a new field total_price for each of these rows.\n",
        "3.  Reduce: Sums the total_price of the filtered and mapped rows.\n",
        "    -   Laptop: 10 * 1200 = 12000\n",
        "    -   Mouse: 50 * 25 = 1250\n",
        "    -   Keyboard: 30 * 75 = 2250\n",
        "    -   Total = 12000 + 1250 + 2250 = 15500\n",
        "\n",
        "Final Output: 15500\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q79gFg5DOtlN"
      },
      "source": [
        "# Requirements\n",
        "### Explicit Requirements\n",
        "\n",
        "1.  Main Function Signature: Implement a main function with the following signature:\n",
        "    def process_csv_pipeline(file_path: str, operations: list) -> any:\n",
        "\n",
        "2.  CSV Reading: The function must read the data from the CSV file specified by file_path. The first row should be treated as the header, and each subsequent row should be loaded as a dictionary.\n",
        "\n",
        "3.  Higher-Order Function for Pipeline: The main function must iterate through the operations list and apply each transformation sequentially. The output of one step serves as the input to the next.\n",
        "\n",
        "4.  Filter Operation: When it encounters a ('filter', ...) tuple, it must apply the provided filter_func to the specified column_name for each row, keeping only the rows for which the function returns True.\n",
        "\n",
        "5.  Map Operation: When it encounters a ('map', ...) tuple, it must apply the map_func to each row and add the result as a new key-value pair in that row's dictionary, with the key being column_name.\n",
        "\n",
        "6.  Reduce Operation: When it encounters a ('reduce', ...) tuple, it must use the reduce_func and initial_value to aggregate the entire dataset into a single value. This should be the final operation in the pipeline if present.\n",
        "\n",
        "### Implicit Requirements\n",
        "\n",
        "-   The solution must be implemented using a functional style. Avoid using imperative loops where standard functional tools like map(), filter(), and functools.reduce() would be more appropriate.\n",
        "-   The functions provided in the operations list (e.g., lambdas) must be treated as first-class citizens.\n",
        "-   The data processing should be lazy where possible, though not a strict requirement.\n",
        "-   The original data from the file should be treated as immutable. Each map or filter operation should produce a new collection of data.\n",
        "\n",
        "### Edge Cases\n",
        "\n",
        "Your implementation must correctly handle the following situations:\n",
        "\n",
        "-   The input file_path does not point to an existing file. A FileNotFoundError should be raised.\n",
        "-   The input CSV file is empty or contains only a header. The pipeline should run and return an empty list or the initial value for a reduce operation.\n",
        "-   An operation tuple in the operations list is malformed (e.g., wrong number of elements or invalid operation name). A ValueError should be raised.\n",
        "-   A function (filter or map) applied to a row causes an exception (e.g., a KeyError if a column is missing, or a TypeError on an unexpected data type). The row that caused the error should be skipped, and the processing should continue with the next row.\n",
        "\n",
        "### Constraints\n",
        "\n",
        "-   You must use only the Python standard library. csv and functools are permitted.\n",
        "-   No input validation is required for the file_path or operations parameters themselves, but the function must handle the runtime errors specified in the edge cases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSg66GeEbDAT"
      },
      "outputs": [],
      "source": [
        "# code\n",
        "\n",
        "import csv\n",
        "from functools import reduce\n",
        "from typing import List, Dict, Any, Callable, Tuple\n",
        "\n",
        "\n",
        "def _safe_apply_filter(row: Dict[str, Any], column: str, func: Callable) -> bool:\n",
        "    \"\"\"\n",
        "    Safely applies a filter function to a single row.\n",
        "    Returns False if the column doesn't exist or if the function raises an exception.\n",
        "    \"\"\"\n",
        "    if column not in row:\n",
        "        return False\n",
        "    try:\n",
        "        return func(row[column])\n",
        "    except Exception:\n",
        "        # Skip the row if any error occurs during filter function execution\n",
        "        return False\n",
        "\n",
        "\n",
        "def _safe_apply_map(row: Dict[str, Any], new_col_name: str, func: Callable) -> Dict[str, Any] | None:\n",
        "    \"\"\"\n",
        "    Safely applies a map function to a single row.\n",
        "    Returns the updated row, or None if the function raises an exception.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        new_value = func(row)\n",
        "        return {**row, new_col_name: new_value}\n",
        "    except Exception:\n",
        "        # Skip the row by returning None if any error occurs\n",
        "        return None\n",
        "\n",
        "\n",
        "def process_csv_pipeline(file_path: str, operations: List[Tuple]) -> Any:\n",
        "    \"\"\"\n",
        "    Processes data from a CSV file using a functional pipeline of operations.\n",
        "\n",
        "    Args:\n",
        "        file_path: The path to the input CSV file.\n",
        "        operations: A list of tuples, each defining a processing step\n",
        "                    ('filter', 'map', or 'reduce').\n",
        "\n",
        "    Returns:\n",
        "        The final processed data, which can be a list of dictionaries or a single\n",
        "        aggregated value if a 'reduce' operation is the final step.\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If the specified file_path does not exist.\n",
        "        ValueError: If an operation tuple is malformed or an unknown operation\n",
        "                    type is provided.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, mode='r', newline='', encoding='utf-8') as csvfile:\n",
        "            reader = csv.DictReader(csvfile)\n",
        "            data: List[Dict[str, Any]] = list(reader)\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
        "\n",
        "    # Process each operation in the pipeline sequentially\n",
        "    for operation in operations:\n",
        "        if not isinstance(operation, tuple) or not operation:\n",
        "            raise ValueError(\n",
        "                \"Invalid operation format: Each operation must be a non-empty tuple.\")\n",
        "\n",
        "        op_type = operation[0]\n",
        "\n",
        "        if op_type == 'filter':\n",
        "            if len(operation) != 3:\n",
        "                raise ValueError(\n",
        "                    \"Filter operation requires 3 elements: ('filter', column_name, filter_func).\")\n",
        "            _, column_name, filter_func = operation\n",
        "            data = [row for row in data if _safe_apply_filter(\n",
        "                row, column_name, filter_func)]\n",
        "\n",
        "        elif op_type == 'map':\n",
        "            if len(operation) != 3:\n",
        "                raise ValueError(\n",
        "                    \"Map operation requires 3 elements: ('map', new_column_name, map_func).\")\n",
        "            _, new_column_name, map_func = operation\n",
        "\n",
        "            # Process map row by row, skipping any that cause errors\n",
        "            mapped_data = []\n",
        "            for row in data:\n",
        "                new_row = _safe_apply_map(row, new_column_name, map_func)\n",
        "                if new_row is not None:\n",
        "                    mapped_data.append(new_row)\n",
        "            data = mapped_data\n",
        "\n",
        "        elif op_type == 'reduce':\n",
        "            if len(operation) != 3:\n",
        "                raise ValueError(\n",
        "                    \"Reduce operation requires 3 elements: ('reduce', reduce_func, initial_value).\")\n",
        "            _, reduce_func, initial_value = operation\n",
        "            # The reduce operation is terminal and returns the final aggregated value\n",
        "            return reduce(reduce_func, data, initial_value)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Unknown operation type: '{op_type}'. Must be 'filter', 'map', or 'reduce'.\")\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KUlcq7ycbHYw"
      },
      "outputs": [],
      "source": [
        "# tests\n",
        "\"\"\"Unit tests for the CSV processing pipeline module.\"\"\"\n",
        "\n",
        "import unittest\n",
        "import tempfile\n",
        "import os\n",
        "import csv\n",
        "from main import process_csv_pipeline\n",
        "\n",
        "\n",
        "class TestProcessCSVPipeline(unittest.TestCase):\n",
        "    \"\"\"Test suite for validating process_csv_pipeline behavior.\"\"\"\n",
        "\n",
        "    def setUp(self):\n",
        "        \"\"\"Create a temporary CSV file for testing.\"\"\"\n",
        "        self.temp_file = tempfile.NamedTemporaryFile(\n",
        "            delete=False, mode='w', newline='', encoding='utf-8'\n",
        "        )\n",
        "        writer = csv.DictWriter(\n",
        "            self.temp_file, fieldnames=['name', 'age', 'salary']\n",
        "        )\n",
        "        writer.writeheader()\n",
        "        writer.writerows([\n",
        "            {'name': 'Alice', 'age': '30', 'salary': '70000'},\n",
        "            {'name': 'Bob', 'age': '25', 'salary': '50000'},\n",
        "            {'name': 'Charlie', 'age': '35', 'salary': '80000'},\n",
        "            {'name': 'Dana', 'age': 'invalid', 'salary': '40000'},\n",
        "        ])\n",
        "        self.temp_file.close()\n",
        "        self.file_path = self.temp_file.name\n",
        "\n",
        "    def tearDown(self):\n",
        "        \"\"\"Remove the temporary CSV file after testing.\"\"\"\n",
        "        os.remove(self.file_path)\n",
        "\n",
        "    def test_filter_valid_numeric_column(self):\n",
        "        \"\"\"Test filtering rows with numeric age > 30.\"\"\"\n",
        "        ops = [('filter', 'age', lambda x: x.isdigit() and int(x) > 30)]\n",
        "        result = process_csv_pipeline(self.file_path, ops)\n",
        "        self.assertEqual(len(result), 1)\n",
        "        self.assertEqual(result[0]['name'], 'Charlie')\n",
        "\n",
        "    def test_filter_invalid_column(self):\n",
        "        \"\"\"Test filtering using a nonexistent column.\"\"\"\n",
        "        ops = [('filter', 'nonexistent', lambda x: True)]\n",
        "        result = process_csv_pipeline(self.file_path, ops)\n",
        "        self.assertEqual(result, [])\n",
        "\n",
        "    def test_map_valid_operation(self):\n",
        "        \"\"\"Test mapping a column to create a new computed column.\"\"\"\n",
        "        ops = [('map', 'double_salary', lambda row: int(row['salary']) * 2)]\n",
        "        result = process_csv_pipeline(self.file_path, ops)\n",
        "        self.assertEqual(result[0]['double_salary'], 140000)\n",
        "        self.assertEqual(result[1]['double_salary'], 100000)\n",
        "\n",
        "    def test_map_raises_exception_skips_row(self):\n",
        "        \"\"\"Test that rows raising exceptions during map are skipped.\"\"\"\n",
        "        ops = [('map', 'parsed_age', lambda row: int(row['age']))]\n",
        "        result = process_csv_pipeline(self.file_path, ops)\n",
        "        for row in result:\n",
        "            self.assertIn('parsed_age', row)\n",
        "            self.assertNotEqual(row['name'], 'Dana')\n",
        "        self.assertEqual(len(result), 3)\n",
        "\n",
        "    def test_reduce_sum_salary(self):\n",
        "        \"\"\"Test reducing mapped numeric salary values to their sum.\"\"\"\n",
        "        ops = [\n",
        "            ('map', 'numeric_salary', lambda row: int(row['salary'])),\n",
        "            ('reduce', lambda acc, row: acc + row['numeric_salary'], 0),\n",
        "        ]\n",
        "        result = process_csv_pipeline(self.file_path, ops)\n",
        "        self.assertEqual(result, 70000 + 50000 + 80000 + 40000)\n",
        "\n",
        "    def test_reduce_returns_initial_on_empty_data(self):\n",
        "        \"\"\"Test reduce on filtered-out data returns initial value.\"\"\"\n",
        "        ops = [\n",
        "            ('filter', 'age', lambda x: False),\n",
        "            ('reduce', lambda acc, row: acc + 1, 0),\n",
        "        ]\n",
        "        result = process_csv_pipeline(self.file_path, ops)\n",
        "        self.assertEqual(result, 0)\n",
        "\n",
        "    def test_invalid_file_path(self):\n",
        "        \"\"\"Test error raised on missing input file.\"\"\"\n",
        "        with self.assertRaises(FileNotFoundError):\n",
        "            process_csv_pipeline(\"nonexistent.csv\", [])\n",
        "\n",
        "    def test_invalid_operation_format(self):\n",
        "        \"\"\"Test malformed operation raises ValueError.\"\"\"\n",
        "        with self.assertRaises(ValueError):\n",
        "            process_csv_pipeline(self.file_path, [None])\n",
        "\n",
        "    def test_unknown_operation_type(self):\n",
        "        \"\"\"Test unknown operation raises ValueError.\"\"\"\n",
        "        with self.assertRaises(ValueError):\n",
        "            process_csv_pipeline(\n",
        "                self.file_path, [('transform', 'col', lambda x: x)]\n",
        "            )\n",
        "\n",
        "    def test_invalid_filter_length(self):\n",
        "        \"\"\"Test ValueError for improperly sized filter tuple.\"\"\"\n",
        "        with self.assertRaises(ValueError):\n",
        "            process_csv_pipeline(self.file_path, [('filter', 'age')])\n",
        "\n",
        "    def test_invalid_map_length(self):\n",
        "        \"\"\"Test ValueError for improperly sized map tuple.\"\"\"\n",
        "        with self.assertRaises(ValueError):\n",
        "            process_csv_pipeline(self.file_path, [('map', 'new_col')])\n",
        "\n",
        "    def test_invalid_reduce_length(self):\n",
        "        \"\"\"Test ValueError for improperly sized reduce tuple.\"\"\"\n",
        "        with self.assertRaises(ValueError):\n",
        "            process_csv_pipeline(\n",
        "                self.file_path, [('reduce', lambda x, y: x)]\n",
        "            )\n",
        "\n",
        "    def test_pipeline_no_operations(self):\n",
        "        \"\"\"Test pipeline returns unmodified data if no ops given.\"\"\"\n",
        "        result = process_csv_pipeline(self.file_path, [])\n",
        "        self.assertEqual(len(result), 4)\n",
        "\n",
        "    def test_chained_filter_and_map(self):\n",
        "        \"\"\"Test combined filtering and mapping operations.\"\"\"\n",
        "        ops = [\n",
        "            ('filter', 'age', lambda x: x.isdigit() and int(x) < 33),\n",
        "            ('map', 'greeting', lambda row: f\"Hello {row['name']}\"),\n",
        "        ]\n",
        "        result = process_csv_pipeline(self.file_path, ops)\n",
        "        self.assertEqual(len(result), 2)\n",
        "        self.assertIn('greeting', result[0])\n",
        "\n",
        "    def test_filter_all_invalid_rows(self):\n",
        "        \"\"\"Test filtering out all rows results in empty output.\"\"\"\n",
        "        ops = [('filter', 'age', lambda x: False)]\n",
        "        result = process_csv_pipeline(self.file_path, ops)\n",
        "        self.assertEqual(result, [])\n",
        "\n",
        "    def test_map_add_constant_column(self):\n",
        "        \"\"\"Test adding a constant value column using map.\"\"\"\n",
        "        ops = [('map', 'country', lambda row: 'India')]\n",
        "        result = process_csv_pipeline(self.file_path, ops)\n",
        "        for row in result:\n",
        "            self.assertEqual(row['country'], 'India')\n",
        "\n",
        "    def test_reduce_to_list_of_names(self):\n",
        "        \"\"\"Test reducing to a list of names.\"\"\"\n",
        "        ops = [('reduce', lambda acc, row: acc + [row['name']], [])]\n",
        "        result = process_csv_pipeline(self.file_path, ops)\n",
        "        self.assertListEqual(result, ['Alice', 'Bob', 'Charlie', 'Dana'])\n",
        "\n",
        "    def test_reduce_on_filtered_rows(self):\n",
        "        \"\"\"Test reduction after filtering based on salary.\"\"\"\n",
        "        ops = [\n",
        "            ('filter', 'salary', lambda x: int(x) > 60000),\n",
        "            ('reduce', lambda acc, row: acc + [row['name']], []),\n",
        "        ]\n",
        "        result = process_csv_pipeline(self.file_path, ops)\n",
        "        self.assertEqual(result, ['Alice', 'Charlie'])\n",
        "\n",
        "    def test_map_add_length_of_name(self):\n",
        "        \"\"\"Test mapping to create name length column.\"\"\"\n",
        "        ops = [('map', 'name_len', lambda row: len(row['name']))]\n",
        "        result = process_csv_pipeline(self.file_path, ops)\n",
        "        for row in result:\n",
        "            self.assertEqual(row['name_len'], len(row['name']))\n",
        "\n",
        "    def test_map_skips_row_on_key_error(self):\n",
        "        \"\"\"Test map skips rows if referenced column is missing.\"\"\"\n",
        "        ops = [('map', 'missing_col', lambda row: row['unknown'])]\n",
        "        result = process_csv_pipeline(self.file_path, ops)\n",
        "        self.assertEqual(result, [])\n",
        "\n",
        "    def test_filter_with_exception_returns_false(self):\n",
        "        \"\"\"Test filter returns False on exception (invalid int).\"\"\"\n",
        "        ops = [('filter', 'age', lambda x: int(x) > 0)]\n",
        "        result = process_csv_pipeline(self.file_path, ops)\n",
        "        self.assertEqual(len(result), 3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxZs2ehLYWkm"
      },
      "source": [
        "# Model Breaking Proof\n",
        "\n",
        "#### Model Breaking Task URL: https://rlhf-v3.turing.com/prompt/2f2bbdf8-98b2-4b9a-ad58-a2d05726dbf3\n",
        "\n",
        "#### Model code:\n",
        "\n",
        "```python\n",
        "import csv\n",
        "from functools import reduce\n",
        "\n",
        "def process_csv_pipeline(file_path: str, operations: list) -> any:\n",
        "    # Step 1: Read the CSV file, handle FileNotFoundError\n",
        "    try:\n",
        "        with open(file_path, newline='') as csvfile:\n",
        "            reader = csv.DictReader(csvfile)\n",
        "            data = list(reader)\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"The file {file_path} does not exist.\")\n",
        "    \n",
        "    # Step 2: Process each operation\n",
        "    for operation in operations:\n",
        "        if not isinstance(operation, tuple) or not operation:\n",
        "            raise ValueError(\"Invalid operation format.\")\n",
        "\n",
        "        op_type = operation[0]\n",
        "\n",
        "        # <Issue>: The error handling for the 'filter' operation is incorrect. The prompt requires that if an exception occurs when applying the `filter_func` to a single row, that specific row should be skipped. This implementation wraps the entire `filter` operation in a single `try...except` block. If any row causes an error, the `except` block is triggered, which then applies a completely different and incorrect fallback logic (`lambda row: column_name in row and True`) to the *entire dataset*, instead of skipping only the problematic row.\n",
        "        if op_type == 'filter':\n",
        "            if len(operation) != 3:\n",
        "                raise ValueError(\"Filter operation requires exactly 3 elements.\")\n",
        "            column_name, filter_func = operation[1], operation[2]\n",
        "            try:\n",
        "                data = list(filter(lambda row: (column_name in row and filter_func(row[column_name])), data))\n",
        "            except Exception as e:\n",
        "                data = list(filter(lambda row: column_name in row and True, data))  # skip error rows\n",
        "\n",
        "        elif op_type == 'map':\n",
        "            if len(operation) != 3:\n",
        "                raise ValueError(\"Map operation requires exactly 3 elements.\")\n",
        "            column_name, map_func = operation[1], operation[2]\n",
        "            try:\n",
        "                data = [{**row, column_name: map_func(row)} for row in data]\n",
        "            except Exception as e:\n",
        "                data = [row for row in data if column_name in row]  # skip error rows\n",
        "\n",
        "        elif op_type == 'reduce':\n",
        "            if len(operation) != 3:\n",
        "                raise ValueError(\"Reduce operation requires exactly 3 elements.\")\n",
        "            reduce_func, initial_value = operation[1], operation[2]\n",
        "            try:\n",
        "                return reduce(reduce_func, data, initial_value)\n",
        "            except Exception as e:\n",
        "                continue  # skip reduce if error occurs\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown operation type: {op_type}\")\n",
        "\n",
        "    # Step 3: Return the transformed data or initial value, depending on last operation\n",
        "    return data\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}